{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Difference with previous versions:``\n",
    "- Using a different approach to Encoding and imputing data, meaning that having more zeros for either the missing numerical values, and nan values in the categorical ones. Since I will be using all (or at least most) the feature in the dataset it could be helpfull to just have zeros rather values that are probably misleading. The columns with a low number of missing values will just imputed using the KNN algorithm.\n",
    "- Using regularizers more extensively, as well as controlling the properties of Layers such as weight and bias-initializers more closely.\n",
    "- Written some new utility functions that can help enhance EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "\n",
    "# The dependent feature\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "The general strategy is to combine both the categorical and numerical values in the training and testing and then process them at the same time. For categorical variables we will be getting dictionaries from the training data and then process them for the combined dataframe. In the case of numerical features, imputation is going to be done using the KNN imputation.\n",
    "\n",
    "1. Encoding categorical features: I'll be using some functions written in utils.py to come up with meaning values for the unique keys in each of the categorical features, then map them in the data given certain conditions.\n",
    "2. Imputing numerical data: The numerical \n",
    "\n",
    "## In-depth analysis of categorical variables:\n",
    "1. Compare the different NaNs for the same categories (and not) in the number of NaNs they have.\n",
    "2. Given that 90% data is not missing for a given feature (column) map their encoded numerical values in the dataframe, otherwise, only impute non-nan values in the feature and then impute the rest of the missing values using any other technique. Dropping the column for values with too many missing might be a general option but in order to use the data for Nerual Networks, it would make sense to just impute the missing values with zeros.\n",
    "\n",
    "## Encoding categorical variables:\n",
    "In order to come up with a meaningful value for any given unique value in a categorical feature column, we will be considering the average SalePrice for each of those unique values and weight them relative to each other. The important thing to note would be that given that more than 90% exists in a column we could just impute the minor missing values with the average of SalePrice for those columns. But if less then 90% of the data existed then there would be a problem since our measures would not make sense and since we are using Neural Networks it would make more sense to impute them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFrames\n",
    "cat_info, num_info = missing_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>1352</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrical</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>730</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>1456</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>1169</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>1408</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test  Train\n",
       "Alley         1352   1369\n",
       "MasVnrType      16      8\n",
       "BsmtQual        44     37\n",
       "BsmtCond        45     37\n",
       "BsmtExposure    44     38\n",
       "BsmtFinType1    42     37\n",
       "BsmtFinType2    42     38\n",
       "Electrical       0      1\n",
       "FireplaceQu    730    690\n",
       "GarageType      76     81\n",
       "GarageFinish    78     81\n",
       "GarageQual      78     81\n",
       "GarageCond      78     81\n",
       "PoolQC        1456   1453\n",
       "Fence         1169   1179\n",
       "MiscFeature   1408   1406"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important to note for categorical features:\n",
    "1. Alley, PoolQc, Fence, MiscFeature are the features with an ecessive number of missing values both in training and testing.\n",
    "2. FireplaceQu is not as bad ass the described functions but it is going to be treated the same way.\n",
    "3. Although for some these values NA means that they just don't have that feature: Alley, MiscFeature, PoolQc\n",
    "\n",
    "Note: To conclude there are 5 features that the np.nan values in them should not be imputed with their given dictionary value but a zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test  Train\n",
       "LotFrontage   259.0    227\n",
       "MasVnrArea      8.0     15\n",
       "GarageYrBlt    81.0     78\n",
       "BsmtFinSF1      0.0      1\n",
       "BsmtFinSF2      0.0      1\n",
       "BsmtUnfSF       0.0      1\n",
       "TotalBsmtSF     0.0      1\n",
       "BsmtFullBath    0.0      2\n",
       "BsmtHalfBath    0.0      2\n",
       "GarageCars      0.0      1\n",
       "GarageArea      0.0      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data:\n",
    "Based on this dataframe, there some features missing in Training that are not missing in the test data. There is no need manually impute anything in the case of numerical values and I am just going to let KNN handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of training data to rebreak the combined data further along the way\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Combine the train and test:\n",
    "# Note: Pass the copies so the actual dataframes won't change and we can still use them\n",
    "feat_cols = combine_train_test(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "2914         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "2915         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "2916          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "2917          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "2918          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "1            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "2            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "3            Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n",
       "4            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "...          ...       ...       ...  ...         ...      ...    ...    ...   \n",
       "2914         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2915         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2916         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2917         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n",
       "2918         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0            NaN       0       2    2008        WD         Normal  \n",
       "1            NaN       0       5    2007        WD         Normal  \n",
       "2            NaN       0       9    2008        WD         Normal  \n",
       "3            NaN       0       2    2006        WD        Abnorml  \n",
       "4            NaN       0      12    2008        WD         Normal  \n",
       "...          ...     ...     ...     ...       ...            ...  \n",
       "2914         NaN       0       6    2006        WD         Normal  \n",
       "2915         NaN       0       4    2006        WD        Abnorml  \n",
       "2916         NaN       0       9    2006        WD        Abnorml  \n",
       "2917        Shed     700       7    2006        WD         Normal  \n",
       "2918         NaN       0      11    2006        WD         Normal  \n",
       "\n",
       "[2919 rows x 79 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring: Alley\n",
      "Ignoring: FireplaceQu\n",
      "Ignoring: PoolQC\n",
      "Ignoring: Fence\n",
      "Ignoring: MiscFeature\n"
     ]
    }
   ],
   "source": [
    "# Get the needed dictionaries to be used for encoding categorical features\n",
    "cat_dicts = get_encoding_dicts(train, data['train_cat_list'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan: 0, 'Grvl': 0.4211, 'Pave': 0.5789}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking one of the values.\n",
    "cat_dicts['Alley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.3491</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSZoning  Street  Alley  LotShape  LandContour  Utilities  LotConfig  \\\n",
       "0       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "1       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1837   \n",
       "2       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1826   \n",
       "3       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1875   \n",
       "4       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1837   \n",
       "...        ...     ...    ...       ...          ...        ...        ...   \n",
       "2914    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2915    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2916    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2917    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2918    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "\n",
       "      LandSlope  Neighborhood  Condition1  ...  GarageType  GarageFinish  \\\n",
       "0        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "1        0.3097        0.0519      0.0875  ...      0.1817        0.2939   \n",
       "2        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "3        0.3097        0.0458      0.1133  ...      0.1201        0.2067   \n",
       "4        0.3097        0.0729      0.1133  ...      0.1817        0.2939   \n",
       "...         ...           ...         ...  ...         ...           ...   \n",
       "2914     0.3097        0.0214      0.1133  ...      0.0925        0.1503   \n",
       "2915     0.3097        0.0214      0.1133  ...      0.0985        0.2067   \n",
       "2916     0.3097        0.0340      0.1133  ...      0.1201        0.2067   \n",
       "2917     0.3097        0.0340      0.1133  ...      0.0925        0.1503   \n",
       "2918     0.3386        0.0340      0.1133  ...      0.1817        0.3491   \n",
       "\n",
       "      GarageQual  GarageCond  PavedDrive  PoolQC  Fence  MiscFeature  \\\n",
       "0         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "1         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "3         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "4         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "...          ...         ...         ...     ...    ...          ...   \n",
       "2914      0.1064      0.1263      0.4298     0.0  0.000        0.000   \n",
       "2915      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2916      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2917      0.1064      0.1263      0.4298     0.0  0.247        0.227   \n",
       "2918      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "\n",
       "      SaleType  SaleCondition  \n",
       "0       0.1035         0.1726  \n",
       "1       0.1035         0.1726  \n",
       "2       0.1035         0.1726  \n",
       "3       0.1035         0.1443  \n",
       "4       0.1035         0.1726  \n",
       "...        ...            ...  \n",
       "2914    0.1035         0.1726  \n",
       "2915    0.1035         0.1443  \n",
       "2916    0.1035         0.1443  \n",
       "2917    0.1035         0.1726  \n",
       "2918    0.1035         0.1726  \n",
       "\n",
       "[2919 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the encoding\n",
    "encoded_feat_cols = encode_categorical(feat_cols.copy(), cat_dicts)\n",
    "encoded_feat_cols[data['train_cat_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: They were some features which did not have any missing values in the training dataset however they did in test set. Hence they are going to be some missing values in the previousley categorical features and from now on they are going to be imputed the same way the numerical features will be imputed, in other words, they will be treated as numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data with KNN:\n",
    "- Both the features of train and test are going to be implemented at the same time together using the KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the missing values with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "missing_features = encoded_feat_cols.columns[encoded_feat_cols.isna().any()].tolist()\n",
    "\n",
    "# The number of neighbors that the function look for is the 1/3 of the whole dataframe\n",
    "num = (train_len + test_len) // 3\n",
    "\n",
    "# Instantiate the Imputer object\n",
    "imputer = KNNImputer(n_neighbors=num, weights=\"distance\")\n",
    "# Fit and transform using the imputer on the missing data and get the imputed combined data\n",
    "imputed_combined = pd.DataFrame()\n",
    "imputed_combined[encoded_feat_cols.columns.to_list()] = pd.DataFrame(imputer.fit_transform(encoded_feat_cols))\n",
    "\n",
    "# Check the imputation:\n",
    "True in imputed_combined.isna().any().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitting and predicting datasets:\n",
    "\n",
    "# features:\n",
    "features = imputed_combined.columns.to_list()\n",
    "\n",
    "train_part = pd.DataFrame()\n",
    "train_part = imputed_combined.iloc[:train_len]\n",
    "# Add the y_feat column (for use further along the way)\n",
    "train_part.loc[:, (y_feat)] = train[y_feat]\n",
    "\n",
    "# X would be the features that will be used for both prediction and training\n",
    "X = train_part[features]\n",
    "y = train[y_feat] # y, the dependent column of the dataset\n",
    "\n",
    "# The dataset used for prediction\n",
    "X_test = imputed_combined[train_len: ].reset_index()\n",
    "X_test.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "# Normalized version of datasets\n",
    "norm_X = normalize(X.copy())\n",
    "norm_y = normalize(y.copy())\n",
    "norm_X_test = normalize(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738629\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681904\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553058\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_part.corr()[y_feat].nlargest(13)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and variance detection using validation data:\n",
    "\n",
    "##### `Note:` One way of validating the accuracy of the neural networks is by using a validation set. In tensorflow's optimizer's they are two options for validation\n",
    "    - Validation_split: This will split a portion of the data being trained to be used as the validation set.\n",
    "    - Validation_data: This will use a given data (from the dataset) to evaluate the predictions.\n",
    "##### Now it would make sense for me to have a number of different randomly selected validation batches to run on the model after being trained to see if there is a variance or bias in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting and undefitting\n",
    "portion = 0.3\n",
    "num = 5\n",
    "devs = []\n",
    "dev_batch_size = int(train_part.shape[0] * portion)\n",
    "\n",
    "for i in range(num):\n",
    "    dev_data = train_part.sample(n=dev_batch_size, random_state=i)\n",
    "    dev_x = dev_data[features]\n",
    "    dev_y = dev_data[y_feat]\n",
    "    devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting parts\n",
    "\n",
    "- Some usefull functions used for hyperparameter tuning\n",
    "- Imports used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2, L1L2\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer,LeakyReLU\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scheduler objects to control the optimizer learning rate:\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay, ExponentialDecay\n",
    "\n",
    "def TimeDecayScheduler(learning_rate=0.001, decay_steps=200, decay_rate=1.2, name=\"\"):\n",
    "    \"\"\" Returns an InverseTimeDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "def ExponentialScheduler(initial_learning_rate, decay_steps, decay_rate, name=\"\"):\n",
    "    \"\"\" Returns an ExponentialDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=initial_learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "# Actual Optimizers: Adam and RMSprop are the main two optimizers that are going to be used for this project since they accept schedulers and happen to be effective.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def AdamOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "        learning_rate: the initial learning rate to be used\n",
    "        scheduler: If this is passed by the user then use it in the optimizer instead of the learning rate\n",
    "\n",
    "        # returns: an Adam optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return Adam(learning_rate)\n",
    "    else:\n",
    "        return Adam(scheduler)\n",
    "    \n",
    "\n",
    "def RMSpropOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "            learning_rate: the initial learning rate to be used\n",
    "            scheduler: If this is passed by the user then use it in the \n",
    "            optimizer instead of the learning rate\n",
    "        \n",
    "        # returns: an RMSprop optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return RMSprop(learning_rate)\n",
    "    else:\n",
    "        return RMSprop(scheduler)\n",
    "\n",
    "# CallBacks:\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def EarlyStopCallBack(patience=100):\n",
    "    \"\"\"\n",
    "        # params: patience of the object for the number of epochs passed with no improvement\n",
    "        # returns: a EarlyStopping callback object \n",
    "    \"\"\"\n",
    "    return EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "\n",
    "# Models: \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Layers \n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2, L1L2  # Regularizer\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError # Error-metric\n",
    "import tensorflow_docs as tfdocs # For logging puposes\n",
    "\n",
    "def Model01(config):\n",
    "    \"\"\"\n",
    "        # params: \n",
    "        config: uses the configuration dictionary to compile and fit the model accordingly\n",
    "        \n",
    "        # returns a history object when the fitting is done\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to try out and improve the model\n",
    "1. Weight-Initializers:\n",
    "    - Use tf.keras.initializers.RandomNormal and tf.keras.initializers.RandomUniform\n",
    "    - Tweak their properties and see how they would work.\n",
    "2. Bias in Dense layers:\n",
    "    - Setup an initiallizer and regularizer for the bias of the layer\n",
    "    - Also use it those for the weights too\n",
    "    - Tweak arguments of earlt-stop call back\n",
    "3. Layers:\n",
    "    - Use LeakyRelu/TreshholdRelu/PRelu as a layer\n",
    "    - Maybe try-out tf.keras.layers.experimental.preprocessing.Normalization*\n",
    "    - Tweak BatchNormalization layer arguments\n",
    "4. Overfitting:\n",
    "    - use tf.keras.layers.GaussianDropout and tf.keras.layers.GaussianNoise ( which could be viewed as a Data augmentation method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A samll neural net to test how does the keras-tuner work\n",
    "def test_model(hp):\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        \n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    time_lr = InverseTimeDecay(\n",
    "      0.15,\n",
    "      decay_steps=20,\n",
    "      decay_rate=0.4\n",
    "    )\n",
    "    \n",
    "    optimizer = Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "        loss=MeanSquaredLogarithmicError(name='MSLE'), \n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the models should have the kernel_initializer and bias_initializer, bias_regularizer\n",
    "def build_model01():\n",
    "    \"\"\"\n",
    "        The architecture of this model consists of three batches of three Dense layers where:\n",
    "            - The layer in between is linear and does not have an activation function, the other\n",
    "                two layers have the relu activation function. The number of layer in between is four times\n",
    "                the number of its sourounding layers.\n",
    "            - The regularazation term for the middle layer is both higher and uses the l2 for its weights.\n",
    "                There is more bias being introduced in the middle layer as well, the weights have a higher range\n",
    "                in the midddle layer.\n",
    "            - Added BatchNormalization layer after each three batches(testing phase).\n",
    "            - The last three layers will be highly biased with high regualarization terms\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        Dense(64, activation='elu', \n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l2(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=25)\n",
    "        ),\n",
    "        Dense(256, \n",
    "              kernel_regularizer=l2(0.01),\n",
    "              bias_regularizer=l1(0.01),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.5),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='elu',\n",
    "              bias_regularizer=l1(0.001), \n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=25)\n",
    "        ),\n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation = 'relu', \n",
    "              bias_regularizer=l1(0.001), \n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1.5),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=1)\n",
    "        ),\n",
    "        Dense(512, \n",
    "              kernel_regularizer=l2(0.01),\n",
    "              bias_regularizer=l1(0.01), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1.5),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation = 'elu',\n",
    "              kernel_regularizer=l2(0.01),\n",
    "              bias_regularizer=l1(0.001), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=1)\n",
    "        ),\n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dense(8,  activation = 'elu',\n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=1.75)\n",
    "        ),\n",
    "        Dense(32,\n",
    "              kernel_regularizer=l2(0.01), \n",
    "              bias_regularizer=l1(0.01), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1.5),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=4)  \n",
    "        ),\n",
    "    \n",
    "        BatchNormalization(),\n",
    "        Dense(8, activation = 'elu',\n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=1.75)\n",
    "        ),\n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dense(128, \n",
    "              activation = 'elu',\n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1.8),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=2.5)\n",
    "        ),\n",
    "        Dense(1024, \n",
    "              kernel_regularizer=l2(0.01), \n",
    "              bias_regularizer=l1(0.01), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.5),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=6)\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dense(128, \n",
    "              activation = 'elu', \n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=1.8),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=2.5),\n",
    "              kernel_regularizer=l2(0.001)\n",
    "        ),\n",
    "        \n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.04, 0.04), \n",
    "              bias_regularizer=l2(0.01), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=5), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=2.5)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.05, 0.05), \n",
    "              bias_regularizer=l2(0.1), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.05), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=4)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.6, 0.6), \n",
    "              bias_regularizer=l2(0.2), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=5), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=2.5)\n",
    "        ),\n",
    "        \n",
    "        Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = TimeDecayScheduler(learning_rate=0.018, decay_steps=500, decay_rate=0.35, name=\"\")\n",
    "    \n",
    "    optimizer = Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "        loss=MeanSquaredLogarithmicError(name='MSLE'), \n",
    "        optimizer=optimizer, \n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=55, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.18\n",
      "1 0.18\n",
      "2 0.17971246006389774\n",
      "3 0.17913921457725052\n",
      "4 0.17828345399805984\n",
      "5 0.17714969594401814\n",
      "6 0.17574374597620848\n",
      "7 0.17407264855012725\n",
      "8 0.172144628708591\n",
      "9 0.16996902518620757\n",
      "10 0.16755621568040968\n",
      "11 0.16491753511851345\n",
      "12 0.16206518781300455\n",
      "13 0.1590121544476104\n",
      "14 0.1557720948742265\n",
      "15 0.15235924772518242\n",
      "16 0.14878832785662346\n",
      "17 0.14507442263711334\n",
      "18 0.1412328880813019\n",
      "19 0.1372792458021986\n",
      "20 0.13322908171797226\n",
      "21 0.12909794740113592\n",
      "22 0.12490126490047979\n",
      "23 0.12065423580030893\n",
      "24 0.1163717552086313\n",
      "25 0.11206833128720271\n",
      "26 0.10775801085307952\n",
      "27 0.10345431149489201\n",
      "28 0.09917016055875386\n",
      "29 0.09491784126986395\n",
      "30 0.09070894616768344\n",
      "31 0.08655433794626281\n",
      "32 0.08246411770794856\n",
      "33 0.07844760055931181\n",
      "34 0.07451329840360164\n",
      "35 0.07066890971510019\n",
      "36 0.0669213160180873\n",
      "37 0.06327658473722324\n",
      "38 0.05973997803740865\n",
      "39 0.0563159672298347\n",
      "40 0.05300825228711851\n",
      "41 0.04981978598413393\n",
      "42 0.046752802162287846\n",
      "43 0.043808847603343186\n",
      "44 0.040988816994145943\n",
      "45 0.03829299046538298\n",
      "46 0.03572107319531994\n",
      "47 0.03327223658282409\n",
      "48 0.030945160512299195\n",
      "49 0.028738076255849922\n",
      "50 0.026648809584430565\n",
      "51 0.02467482368928756\n",
      "52 0.02281326154704841\n",
      "53 0.02106098739572416\n",
      "54 0.019414627024082007\n",
      "55 0.01787060661274117\n",
      "56 0.016425189901416518\n",
      "57 0.015074513492489464\n",
      "58 0.013814620136079054\n",
      "59 0.012641489875621388\n",
      "60 0.01155106896529732\n",
      "61 0.010539296501183687\n",
      "62 0.009602128736501172\n",
      "63 0.00873556107760296\n",
      "64 0.007935647781252688\n",
      "65 0.0071985193951856746\n",
      "66 0.006520398002885574\n",
      "67 0.005897610349932683\n",
      "68 0.005326598943219548\n",
      "69 0.004803931225847356\n",
      "70 0.00432630693970403\n",
      "71 0.0038905637946978684\n",
      "72 0.003493681568514609\n",
      "73 0.003132784763732612\n",
      "74 0.0028051439503336425\n",
      "75 0.002508175921256833\n",
      "76 0.002239442786836458\n",
      "77 0.001996650130916956\n",
      "78 0.0017776443473263499\n",
      "79 0.0015804092703826011\n",
      "80 0.0014030622073709172\n",
      "81 0.0012438494746196073\n",
      "82 0.0011011415320641\n",
      "83 0.0009734278041585042\n",
      "84 0.0008593112677952897\n",
      "85 0.0007575028806375967\n",
      "86 0.0006668159160542224\n",
      "87 0.000586160263760744\n",
      "88 0.0005145367483854846\n",
      "89 0.00045103151155810356\n",
      "90 0.0003948104968120654\n",
      "91 0.0003451140706399173\n",
      "92 0.00030125180747199486\n",
      "93 0.0002625974611854906\n",
      "94 0.00022858414100408305\n",
      "95 0.00019869970532343796\n",
      "96 0.0001724823830932621\n",
      "97 0.0001495166288949914\n",
      "98 0.00012942921476366984\n",
      "99 0.00011188555909722497\n",
      "100 9.658629065713481e-05\n",
      "101 8.32640436699438e-05\n",
      "102 7.168047836599844e-05\n",
      "103 6.162351991574831e-05\n",
      "104 5.2904807619976226e-05\n",
      "105 4.5357345353203215e-05\n",
      "106 3.883334362431782e-05\n",
      "107 3.3202243180846294e-05\n",
      "108 2.834890981971166e-05\n",
      "109 2.4171989955415805e-05\n",
      "110 2.0582416515170136e-05\n",
      "111 1.750205485983855e-05\n",
      "112 1.4862478651357466e-05\n",
      "113 1.2603865884801107e-05\n",
      "114 1.067400566124755e-05\n",
      "115 9.027406682381217e-06\n",
      "116 7.624498887146298e-06\n",
      "117 6.430920113989793e-06\n",
      "118 5.416880149924017e-06\n",
      "119 4.556595011712665e-06\n",
      "120 3.827784788065075e-06\n",
      "121 3.211228849047882e-06\n",
      "122 2.6903726952478904e-06\n",
      "123 2.2509811707227997e-06\n",
      "124 1.8808331974622323e-06\n",
      "125 1.5694536026887788e-06\n",
      "126 1.307878002240649e-06\n",
      "127 1.0884470724372912e-06\n",
      "128 9.046268886613125e-07\n",
      "129 7.50852331226189e-07\n",
      "130 6.22390858111894e-07\n",
      "131 5.15224220291303e-07\n",
      "132 4.259459493149e-07\n",
      "133 3.5167267942115257e-07\n",
      "134 2.8996757867839096e-07\n",
      "135 2.387743566192284e-07\n",
      "136 1.9636049064081282e-07\n",
      "137 1.612684712884468e-07\n",
      "138 1.3227400860272867e-07\n",
      "139 1.0835026916999399e-07\n",
      "140 8.863732752780923e-08\n",
      "141 7.241611726128205e-08\n",
      "142 5.9086257556529085e-08\n",
      "143 4.814721117709345e-08\n",
      "144 3.91823007626086e-08\n",
      "145 3.184517292149594e-08\n",
      "146 2.5848354644071377e-08\n",
      "147 2.0953594880083802e-08\n",
      "148 1.696372642493831e-08\n",
      "149 1.3715820201276122e-08\n",
      "150 1.1075436209040796e-08\n",
      "151 8.931803394387739e-09\n",
      "152 7.193784950376723e-09\n",
      "153 5.7865065559658316e-09\n",
      "154 4.648543184419851e-09\n",
      "155 3.729575725625683e-09\n",
      "156 2.988442087841092e-09\n",
      "157 2.3915189563389023e-09\n",
      "158 1.9113802400406827e-09\n",
      "159 1.525686653927748e-09\n",
      "160 1.2162680595725032e-09\n",
      "161 9.683662894685535e-10\n",
      "162 7.70011362490898e-10\n",
      "163 6.115083882551604e-10\n",
      "164 4.850161708876588e-10\n",
      "165 3.8420165627983117e-10\n",
      "166 3.0395700655049934e-10\n",
      "167 2.4016830479653865e-10\n",
      "168 1.8952675567908668e-10\n",
      "169 1.4937480743938103e-10\n",
      "170 1.1758092525140196e-10\n",
      "171 9.24378343171399e-11\n",
      "172 7.257995784951311e-11\n",
      "173 5.691652905388419e-11\n",
      "174 4.457748202841807e-11\n",
      "175 3.4869745015971584e-11\n",
      "176 2.7241988293727798e-11\n",
      "177 2.1256233063145908e-11\n",
      "178 1.6565019531753358e-11\n",
      "179 1.2893072487354732e-11\n",
      "180 1.0022599881339188e-11\n",
      "181 7.781521647002474e-12\n",
      "182 6.034058349102415e-12\n",
      "183 4.673217432700136e-12\n",
      "184 3.614803088412853e-12\n",
      "185 2.792647627018582e-12\n",
      "186 2.154820699860017e-12\n",
      "187 1.6606201447749821e-12\n",
      "188 1.2781866877886256e-12\n",
      "189 9.8261584239593e-13\n",
      "190 7.544654809551059e-13\n",
      "191 5.785778228183327e-13\n",
      "192 4.4315090595766905e-13\n",
      "193 3.390077309957689e-13\n",
      "194 2.5902179935495787e-13\n",
      "195 1.9766620829896052e-13\n",
      "196 1.5066021974006137e-13\n",
      "197 1.1469261551466303e-13\n",
      "198 8.720545583535815e-14\n",
      "199 6.622528541567296e-14\n",
      "200 5.02315575058199e-14\n",
      "201 3.805421023168174e-14\n",
      "202 2.8794045272156274e-14\n",
      "203 2.1760916922729953e-14\n",
      "204 1.642581289457273e-14\n",
      "205 1.2383755197958934e-14\n",
      "206 9.325116865932931e-15\n",
      "207 7.01347538051514e-15\n",
      "208 5.268536193295628e-15\n",
      "209 3.952983338307044e-15\n",
      "210 2.9623676096425688e-15\n",
      "211 2.2173410251815635e-15\n",
      "212 1.6577011252852597e-15\n",
      "213 1.2378293946275835e-15\n",
      "214 9.232021141315508e-16\n",
      "215 6.877250552231457e-16\n",
      "216 5.117001898981739e-16\n",
      "217 3.802765977245644e-16\n",
      "218 2.822718213513691e-16\n",
      "219 2.092762613815014e-16\n",
      "220 1.5497353479080374e-16\n",
      "221 1.146253955553282e-16\n",
      "222 8.468188205919636e-17\n",
      "223 6.248663079928893e-17\n",
      "224 4.605441538862686e-17\n",
      "225 3.390342711176889e-17\n",
      "226 2.4928990523359478e-17\n",
      "227 1.8308600560634162e-17\n",
      "228 1.343060487135722e-17\n",
      "229 9.840712830713086e-18\n",
      "230 7.201926837465665e-18\n",
      "231 5.2645664016561875e-18\n",
      "232 3.8438714965363514e-18\n",
      "233 2.803290181254632e-18\n",
      "234 2.0420237334314045e-18\n",
      "235 1.4857564998773314e-18\n",
      "236 1.0797648981666654e-18\n",
      "237 7.838014649874169e-19\n",
      "238 5.683015262379763e-19\n",
      "239 4.115741064875263e-19\n",
      "240 2.977243247160925e-19\n",
      "241 2.1511873173128073e-19\n",
      "242 1.552531262494809e-19\n",
      "243 1.1191834360545047e-19\n",
      "244 8.058636492327943e-20\n",
      "245 5.795912321869924e-20\n",
      "246 4.163730116285865e-20\n",
      "247 2.9877512315484107e-20\n",
      "248 2.1414501372910054e-20\n",
      "249 1.5331114957696202e-20\n",
      "250 1.0963325913684354e-20\n",
      "251 7.830947081203111e-21\n",
      "252 5.58714831706843e-21\n",
      "253 3.981719154125164e-21\n",
      "254 2.8343672794171154e-21\n",
      "255 2.0153350962863448e-21\n",
      "256 1.4313459490670063e-21\n",
      "257 1.0154270353767071e-21\n",
      "258 7.195486361796394e-22\n",
      "259 5.093067923128818e-22\n",
      "260 3.6008681583207135e-22\n",
      "261 2.542985987514628e-22\n",
      "262 1.7938670905153981e-22\n",
      "263 1.2639987954589896e-22\n",
      "264 8.89638791848951e-23\n",
      "265 6.254490943819959e-23\n",
      "266 4.3921986965027805e-23\n",
      "267 3.080947458265138e-23\n",
      "268 2.158735606968286e-23\n",
      "269 1.5108731851681732e-23\n",
      "270 1.056259217818913e-23\n",
      "271 7.376111856277326e-24\n",
      "272 5.1451673104613046e-24\n",
      "273 3.584982797144164e-24\n",
      "274 2.4951160893263945e-24\n",
      "275 1.7346468919121207e-24\n",
      "276 1.204615897161195e-24\n",
      "277 8.356103615158123e-25\n",
      "278 5.78998310362952e-25\n",
      "279 4.007463388447896e-25\n",
      "280 2.7706467010839983e-25\n",
      "281 1.9134300421850818e-25\n",
      "282 1.319971055591254e-25\n",
      "283 9.095721165871375e-26\n",
      "284 6.260821287081068e-26\n",
      "285 4.3047451093791724e-26\n",
      "286 2.95655570699119e-26\n",
      "287 2.028372466377051e-26\n",
      "288 1.3900578854009397e-26\n",
      "289 9.515730321747945e-27\n",
      "290 6.506927189379066e-27\n",
      "291 4.444622397116849e-27\n",
      "292 3.0326299106965396e-27\n",
      "293 2.0669505934409346e-27\n",
      "294 1.4072376044668673e-27\n",
      "295 9.570440726787727e-28\n",
      "296 6.501658102437314e-28\n",
      "297 4.412091546170816e-28\n",
      "298 2.9908429678489805e-28\n",
      "299 2.025218694372278e-28\n",
      "300 1.3698719523622012e-28\n",
      "301 9.255891570014873e-29\n",
      "302 6.247227031597512e-29\n",
      "303 4.211992335219466e-29\n",
      "304 2.8367405274915585e-29\n",
      "305 1.9084637563856017e-29\n",
      "306 1.2825697287537645e-29\n",
      "307 8.610161981429675e-30\n",
      "308 5.77398201544372e-30\n",
      "309 3.8678872021997054e-30\n",
      "310 2.588254284127212e-30\n",
      "311 1.7301165000850348e-30\n",
      "312 1.1552594151208832e-30\n",
      "313 7.705839215053917e-31\n",
      "314 5.134487749902663e-31\n",
      "315 3.4175237951961278e-31\n",
      "316 2.272289757444234e-31\n",
      "317 1.5092253968147144e-31\n",
      "318 1.0013438142348156e-31\n",
      "319 6.63669017918091e-32\n",
      "320 4.393995086851767e-32\n",
      "321 2.906081406647994e-32\n",
      "322 1.9199797876902707e-32\n",
      "323 1.2671461112000203e-32\n",
      "324 8.354075100211104e-33\n",
      "325 5.501893506461475e-33\n",
      "326 3.6196667805667594e-33\n",
      "327 2.3788556654618555e-33\n",
      "328 1.5617487299513232e-33\n",
      "329 1.0242318533258941e-33\n",
      "330 6.710114343067964e-34\n",
      "331 4.391436088395264e-34\n",
      "332 2.870970246074309e-34\n",
      "333 1.874980568230348e-34\n",
      "334 1.2232388884592563e-34\n",
      "335 7.97209911665313e-35\n",
      "336 5.190168695737715e-35\n",
      "337 3.3754999321915416e-35\n",
      "338 2.1930223052179973e-35\n",
      "339 1.4233010807489598e-35\n",
      "340 9.227833770415973e-36\n",
      "341 5.976576276176148e-36\n",
      "342 3.866832476822042e-36\n",
      "343 2.4992453960845665e-36\n",
      "344 1.6136656741248494e-36\n",
      "345 1.0408060333622609e-36\n",
      "346 6.706224441767145e-37\n",
      "347 4.3165708301796765e-37\n",
      "348 2.7755728074714995e-37\n",
      "349 1.7828705083963898e-37\n",
      "350 1.1440390839299215e-37\n",
      "351 7.33358387134565e-38\n",
      "352 4.6961986881055653e-38\n",
      "353 3.004221269258934e-38\n",
      "354 1.9198755555080099e-38\n",
      "355 1.225661105406033e-38\n",
      "356 7.816716233456842e-39\n",
      "357 4.98006895607597e-39\n",
      "358 3.169595822349777e-39\n",
      "359 2.01525675378292e-39\n",
      "360 1.2800157226771596e-39\n",
      "361 8.12192717434746e-40\n",
      "362 5.148280409703005e-40\n",
      "363 3.260055983854486e-40\n",
      "364 2.062282378450459e-40\n",
      "365 1.303262372630472e-40\n",
      "366 8.227666493879242e-41\n",
      "367 5.188992491094375e-41\n",
      "368 3.2692745029576454e-41\n",
      "369 2.0577004676218816e-41\n",
      "370 1.2938257467441408e-41\n",
      "371 8.127046147890332e-42\n",
      "372 5.0998030546500576e-42\n",
      "373 3.196967812594068e-42\n",
      "374 2.0021091010734394e-42\n",
      "375 1.2525707589298294e-42\n",
      "376 7.828567243311433e-43\n",
      "377 4.887966560509137e-43\n",
      "378 3.0488813376429243e-43\n",
      "379 1.899851282180287e-43\n",
      "380 1.1826763459787643e-43\n",
      "381 7.354952400365449e-44\n",
      "382 4.569428678159449e-44\n",
      "383 2.836040639374037e-44\n",
      "384 1.758457737707116e-44\n",
      "385 1.0892329891644673e-44\n",
      "386 6.740303150770218e-45\n",
      "387 4.166854074412845e-45\n",
      "388 2.573402960976312e-45\n",
      "389 1.5877362789834107e-45\n",
      "390 9.786342942451988e-46\n",
      "391 6.0260732404261e-46\n",
      "392 3.70698403077393e-46\n",
      "393 2.278136695411707e-46\n",
      "394 1.3986595625071874e-46\n",
      "395 8.578628327448401e-47\n",
      "396 5.256512455544363e-47\n",
      "397 3.217747585421378e-47\n",
      "398 1.9678006270923297e-47\n",
      "399 1.2022242345383246e-47\n",
      "400 7.337794400258329e-48\n",
      "401 4.474264878206298e-48\n",
      "402 2.725551217230932e-48\n",
      "403 1.6586850153547538e-48\n",
      "404 1.0084417651719076e-48\n",
      "405 6.125132198566008e-49\n",
      "406 3.716706431168694e-49\n",
      "407 2.2530955572070163e-49\n",
      "408 1.364520080672854e-49\n",
      "409 8.255808813364315e-50\n",
      "410 4.990213257594485e-50\n",
      "411 3.01341380289522e-50\n",
      "412 1.8179378637157457e-50\n",
      "413 1.095671325768892e-50\n",
      "414 6.597250275583405e-51\n",
      "415 3.9685095497975254e-51\n",
      "416 2.384921604445628e-51\n",
      "417 1.4318693590571735e-51\n",
      "418 8.588467844632758e-52\n",
      "419 5.146493195489428e-52\n",
      "420 3.0809944896368708e-52\n",
      "421 1.8427000536105684e-52\n",
      "422 1.1010397069852824e-52\n",
      "423 6.5725865985272345e-53\n",
      "424 3.9197200611445815e-53\n",
      "425 2.3353908848573534e-53\n",
      "426 1.3901136219389006e-53\n",
      "427 8.266612880226574e-54\n",
      "428 4.9112481465224415e-54\n",
      "429 2.9150333253338326e-54\n",
      "430 1.7285539168250904e-54\n",
      "431 1.024024832242352e-54\n",
      "432 6.060753031737405e-55\n",
      "433 3.583699758595911e-55\n",
      "434 2.117024904652594e-55\n",
      "435 1.2494245187987453e-55\n",
      "436 7.366889851407696e-56\n",
      "437 4.339591100028096e-56\n",
      "438 2.553902483538192e-56\n",
      "439 1.5015889484584854e-56\n",
      "440 8.820423804384901e-57\n",
      "441 5.176305049521655e-57\n",
      "442 3.034888044982209e-57\n",
      "443 1.7776991828621186e-57\n",
      "444 1.0403202146899102e-57\n",
      "445 6.082321180366641e-58\n",
      "446 3.55275769881229e-58\n",
      "447 2.0732712994936333e-58\n",
      "448 1.2087635841264187e-58\n",
      "449 7.040794408937667e-59\n",
      "450 4.09729656013598e-59\n",
      "451 2.3821491628697553e-59\n",
      "452 1.3836832962765772e-59\n",
      "453 8.029731292227119e-60\n",
      "454 4.655456454213311e-60\n",
      "455 2.6966267691226317e-60\n",
      "456 1.5605478987978192e-60\n",
      "457 9.022594234492479e-61\n",
      "458 5.2117572981125685e-61\n",
      "459 3.007708505374289e-61\n",
      "460 1.7341492766226298e-61\n",
      "461 9.989339151052014e-62\n",
      "462 5.7489290694360115e-62\n",
      "463 3.3055019948459125e-62\n",
      "464 1.8988407599068889e-62\n",
      "465 1.0897846418198397e-62\n",
      "466 6.24876514804954e-63\n",
      "467 3.5797233891209553e-63\n",
      "468 2.048834357326554e-63\n",
      "469 1.1715658493404356e-63\n",
      "470 6.693132137456784e-64\n",
      "471 3.820280900374877e-64\n",
      "472 2.1785360973853086e-64\n",
      "473 1.2411896635057593e-64\n",
      "474 7.065059560028229e-65\n",
      "475 4.0178910145747435e-65\n",
      "476 2.282892621917468e-65\n",
      "477 1.2959199715698613e-65\n",
      "478 7.349818350555021e-66\n",
      "479 4.1646749493172146e-66\n",
      "480 2.357719060981213e-66\n",
      "481 1.3335515050798715e-66\n",
      "482 7.535892320749726e-67\n",
      "483 4.2546817529074786e-67\n",
      "484 2.3999784256021423e-67\n",
      "485 1.3525577240769512e-67\n",
      "486 7.615752950883734e-68\n",
      "487 4.284289463818482e-68\n",
      "488 2.4079864342504956e-68\n",
      "489 1.3521936400777714e-68\n",
      "490 7.586364677276545e-69\n",
      "491 4.252446567980126e-69\n",
      "492 2.381522495508583e-69\n",
      "493 1.3325439209425824e-69\n",
      "494 7.449373439974186e-70\n",
      "495 4.160731367277807e-70\n",
      "496 2.3218367004898474e-70\n",
      "497 1.2945119873382289e-70\n",
      "498 7.210962496313664e-71\n",
      "499 4.013224897770294e-71\n",
      "500 2.2315529903082147e-71\n",
      "501 1.2397516612823416e-71\n",
      "502 6.881392436069836e-72\n",
      "503 3.8162114219553217e-72\n",
      "504 2.114478846384819e-72\n",
      "505 1.1705485199207367e-72\n",
      "506 6.474272787172216e-73\n",
      "507 3.577736951355115e-73\n",
      "508 1.9753406312693876e-73\n",
      "509 1.0896627489350107e-73\n",
      "510 6.005636843777616e-74\n",
      "511 3.307068746573577e-74\n",
      "512 1.8194700410285964e-74\n",
      "513 1.000148439439642e-74\n",
      "514 5.492906631368859e-75\n",
      "515 3.0141059215149573e-75\n",
      "516 1.6524703517077617e-75\n",
      "517 9.051656177189755e-76\n",
      "518 4.9538398517894896e-76\n",
      "519 2.7087925698761424e-76\n",
      "520 1.47989104560541e-76\n",
      "521 8.078007890859225e-77\n",
      "522 4.4055453156954755e-77\n",
      "523 2.400580490243829e-77\n",
      "524 1.3069362425107954e-77\n",
      "525 7.109096184240618e-78\n",
      "526 3.863639230565553e-78\n",
      "527 2.0979795995686105e-78\n",
      "528 1.1382267792798451e-78\n",
      "529 6.169919662184762e-79\n",
      "530 3.341594271113931e-79\n",
      "531 1.8082220081785338e-79\n",
      "532 9.776286808923732e-80\n",
      "533 5.281053807759147e-80\n",
      "534 2.8503096976247552e-80\n",
      "535 1.5370522528174909e-80\n",
      "536 8.281531534576998e-81\n",
      "537 4.458188810603466e-81\n",
      "538 2.397907062501864e-81\n",
      "539 1.2886430903384908e-81\n",
      "540 6.919260579566638e-82\n",
      "541 3.712049667149484e-82\n",
      "542 1.9897350274171762e-82\n",
      "543 1.0656250146835776e-82\n",
      "544 5.702188648777705e-83\n",
      "545 3.048646625736583e-83\n",
      "546 1.6285505479362088e-83\n",
      "547 8.692093018446887e-84\n",
      "548 4.635288512397018e-84\n",
      "549 2.469782881711966e-84\n",
      "550 1.3148333058517704e-84\n",
      "551 6.993794180062608e-85\n",
      "552 3.716939934131913e-85\n",
      "553 1.9737361587361477e-85\n",
      "554 1.0471859925382786e-85\n",
      "555 5.551240418459916e-86\n",
      "556 2.940275645370718e-86\n",
      "557 1.5560307183375942e-86\n",
      "558 8.22774280000843e-87\n",
      "559 4.346863271348493e-87\n",
      "560 2.2945857640141963e-87\n",
      "561 1.2102245590792174e-87\n",
      "562 6.377658932753042e-88\n",
      "563 3.3580765231429243e-88\n",
      "564 1.7666648375120606e-88\n",
      "565 9.28650566396163e-89\n",
      "566 4.877366420147915e-89\n",
      "567 2.5594911944520965e-89\n",
      "568 1.342015097762215e-89\n",
      "569 7.030674233875813e-90\n",
      "570 3.6802105495581097e-90\n",
      "571 1.9247963125303921e-90\n",
      "572 1.0058509158290092e-90\n",
      "573 5.251936695013623e-91\n",
      "574 2.7399502791181256e-91\n",
      "575 1.428247643410199e-91\n",
      "576 7.43878980942812e-92\n",
      "577 3.8711437392944004e-92\n",
      "578 2.0128659210141432e-92\n",
      "579 1.045753283984904e-92\n",
      "580 5.428536565536254e-93\n",
      "581 2.8156309987221233e-93\n",
      "582 1.4591785855732396e-93\n",
      "583 7.555812891327877e-94\n",
      "584 3.909257497582718e-94\n",
      "585 2.020914752679238e-94\n",
      "586 1.0438609259706808e-94\n",
      "587 5.3873912364300204e-95\n",
      "588 2.778151421426372e-95\n",
      "589 1.4314465279402164e-95\n",
      "590 7.369473475804244e-96\n",
      "591 3.790881417594776e-96\n",
      "592 1.9484382286157358e-96\n",
      "593 1.0006359021239399e-96\n",
      "594 5.134625934544027e-97\n",
      "595 2.632601484077126e-97\n",
      "596 1.348668793072298e-97\n",
      "597 6.903505288044113e-98\n",
      "598 3.530843539302431e-98\n",
      "599 1.8043967392183314e-98\n",
      "600 9.213627140616479e-99\n",
      "601 4.700830173783918e-99\n",
      "602 2.3964264752161083e-99\n",
      "603 1.2206736324450429e-99\n",
      "604 6.212711891515894e-100\n",
      "605 3.1594344444242747e-100\n",
      "606 1.6054036811098958e-100\n",
      "607 8.150912272085173e-101\n",
      "608 4.135000138030222e-101\n",
      "609 2.0960057471767143e-101\n",
      "610 1.061591241479292e-101\n",
      "611 5.37242531113002e-102\n",
      "612 2.716639012505067e-102\n",
      "613 1.3725944889374832e-102\n",
      "614 6.929495602471139e-103\n",
      "615 3.495508274047185e-103\n",
      "616 1.761848928451202e-103\n",
      "617 8.873131186800976e-104\n",
      "618 4.4651425054352736e-104\n",
      "619 2.24514405945056e-104\n",
      "620 1.1279863642737942e-104\n",
      "621 5.662582149968846e-105\n",
      "622 2.8403802919185626e-105\n",
      "623 1.4236068022847647e-105\n",
      "624 7.129441117211362e-106\n",
      "625 3.5675746183003207e-106\n",
      "626 1.7837873091501603e-106\n",
      "627 8.911807100070746e-107\n",
      "628 4.448785493246179e-107\n",
      "629 2.2190669858570323e-107\n",
      "630 1.1059943111328909e-107\n",
      "631 5.50793979647854e-108\n",
      "632 2.7408139910820763e-108\n",
      "633 1.3627754530042144e-108\n",
      "634 6.77054577207976e-109\n",
      "635 3.36107315929297e-109\n",
      "636 1.6671989877445287e-109\n",
      "637 8.263278091517292e-110\n",
      "638 4.092352462122272e-110\n",
      "639 2.025115034700253e-110\n",
      "640 1.0013424815566915e-110\n",
      "641 4.947344276465867e-111\n",
      "642 2.4424092992031335e-111\n",
      "643 1.2048191097095173e-111\n",
      "644 5.938579996596594e-112\n",
      "645 2.9248325436350444e-112\n",
      "646 1.4393860943085847e-112\n",
      "647 7.078019739912395e-113\n",
      "648 3.4778005797525525e-113\n",
      "649 1.707482609854945e-113\n",
      "650 8.376582662161228e-114\n",
      "651 4.1061679716476606e-114\n",
      "652 2.0112499861126866e-114\n",
      "653 9.843627574944628e-115\n",
      "654 4.813980621549602e-115\n",
      "655 2.352414299037139e-115\n",
      "656 1.148639794451728e-115\n",
      "657 5.604214453804294e-116\n",
      "658 2.732163832782904e-116\n",
      "659 1.3309449692044543e-116\n",
      "660 6.478509390598004e-117\n",
      "661 3.1510259681896906e-117\n",
      "662 1.5314084215540877e-117\n",
      "663 7.4369095840816216e-118\n",
      "664 3.6087488276793576e-118\n",
      "665 1.7497812391773455e-118\n",
      "666 8.477622282835976e-119\n",
      "667 4.1041935916130793e-119\n",
      "668 1.9853877668406923e-119\n",
      "669 9.596808617752765e-120\n",
      "670 4.63524372959465e-120\n",
      "671 2.23708674208236e-120\n",
      "672 1.0788419859579284e-120\n",
      "673 5.198737403421011e-121\n",
      "674 2.5032441272250626e-121\n",
      "675 1.2044092221059769e-121\n",
      "676 5.790428952432581e-122\n",
      "677 2.7817202884476276e-122\n",
      "678 1.335311198371557e-122\n",
      "679 6.404984642994803e-123\n",
      "680 3.069873774441527e-123\n",
      "681 1.4702460605562869e-123\n",
      "682 7.0360167522793215e-124\n",
      "683 3.36458337427282e-124\n",
      "684 1.607694655138006e-124\n",
      "685 7.67615859023112e-125\n",
      "686 3.6622894037362214e-125\n",
      "687 1.7459426981961392e-125\n",
      "688 8.317181298571546e-126\n",
      "689 3.959054311962846e-126\n",
      "690 1.8831118302715206e-126\n",
      "691 8.950151284560459e-127\n",
      "692 4.25064175748502e-127\n",
      "693 2.0171990117146076e-127\n",
      "694 9.565625055551061e-128\n",
      "695 4.532612327308122e-128\n",
      "696 2.1461232610360424e-128\n",
      "697 1.0153876140405198e-128\n",
      "698 4.800433122354954e-129\n",
      "699 2.2677783079908136e-129\n",
      "700 1.070514684663337e-129\n",
      "701 5.049597569166684e-130\n",
      "702 2.3800893519827887e-130\n",
      "703 1.1209915938125419e-130\n",
      "704 5.275751100397882e-131\n",
      "705 2.4810718116995303e-131\n",
      "706 1.1659172047460197e-131\n",
      "707 5.4748178284467496e-132\n",
      "708 2.5688897468312454e-132\n",
      "709 1.2044681858736146e-132\n",
      "710 5.643123059752691e-133\n",
      "711 2.6419115448280387e-133\n",
      "712 1.23592418826162e-133\n",
      "713 5.777506489629862e-134\n",
      "714 2.698760505245638e-134\n",
      "715 1.2596903030459474e-134\n",
      "716 5.875421189579978e-135\n",
      "717 2.73835812340603e-135\n",
      "718 1.2753158175326148e-135\n",
      "719 5.9350140428733e-136\n",
      "720 2.7599581672587886e-136\n",
      "721 1.282508442034753e-136\n",
      "722 5.955184073341163e-137\n",
      "723 2.7631700414537696e-137\n",
      "724 1.2811433797541588e-137\n",
      "725 5.93561610338287e-138\n",
      "726 2.74797041823281e-138\n",
      "727 1.271266847813106e-138\n",
      "728 5.8767883127454985e-139\n",
      "729 2.7147026574027617e-139\n",
      "730 1.2530939149754253e-139\n",
      "731 5.779953482358973e-140\n",
      "732 2.6640641050695857e-140\n",
      "733 1.227000785312079e-140\n",
      "734 5.647094925037182e-141\n",
      "735 2.59708191916721e-141\n",
      "736 1.1935119113819898e-141\n",
      "737 5.48085925506057e-142\n",
      "738 2.515078586206209e-142\n",
      "739 1.1532825505347619e-142\n",
      "740 5.284469164840367e-143\n",
      "741 2.419628738480021e-143\n",
      "742 1.1070775706808296e-143\n",
      "743 5.0616202024544155e-144\n",
      "744 2.3125092299225216e-144\n",
      "745 1.055747457050092e-144\n",
      "746 4.816366136177426e-145\n",
      "747 2.1956446645593663e-145\n",
      "748 1.0002025622081663e-145\n",
      "749 4.552997825055382e-146\n",
      "750 2.0710506846139832e-146\n",
      "751 9.413866748245378e-147\n",
      "752 4.275920579689943e-147\n",
      "753 1.9407773146740846e-147\n",
      "754 8.802509591228613e-148\n",
      "755 3.989534803856332e-148\n",
      "756 1.8068545307320344e-148\n",
      "757 8.177292409178287e-149\n",
      "758 3.698124280561816e-149\n",
      "759 1.6712419923001697e-149\n",
      "760 7.5471549507775e-150\n",
      "761 3.4057558442136727e-150\n",
      "762 1.535784561784665e-150\n",
      "763 6.920442329599247e-151\n",
      "764 3.1161934121034074e-151\n",
      "765 1.4021748614576164e-151\n",
      "766 6.304743082093598e-152\n",
      "767 2.832828487640905e-152\n",
      "768 1.2719237103272743e-152\n",
      "769 5.706764673040534e-153\n",
      "770 2.558628350538259e-153\n",
      "771 1.1463388667286105e-153\n",
      "772 5.1322477915858275e-154\n",
      "773 2.2961022689628794e-154\n",
      "774 1.0265121016464946e-154\n",
      "775 4.5859189673270835e-155\n",
      "776 2.0472852532710194e-155\n",
      "777 9.133142635934241e-156\n",
      "778 4.0714794204414413e-156\n",
      "779 1.8137381594981472e-156\n",
      "780 8.07397684961782e-157\n",
      "781 3.591626712463442e-157\n",
      "782 1.5965623721832513e-157\n",
      "783 7.092050338411742e-158\n",
      "784 3.148104731184189e-158\n",
      "785 1.3964268679844698e-158\n",
      "786 6.189835407732578e-159\n",
      "787 2.741776846089909e-159\n",
      "788 1.213605190372658e-159\n",
      "789 5.368034281549265e-160\n",
      "790 2.37271670860558e-160\n",
      "791 1.0480197476173054e-160\n",
      "792 4.625793377548135e-161\n",
      "793 2.0403111227717604e-161\n",
      "794 8.992908686405854e-162\n",
      "795 3.960935820298561e-162\n",
      "796 1.7433696392159156e-162\n",
      "797 7.66788194588281e-163\n",
      "798 3.3702012771988443e-163\n",
      "799 1.4802359790929568e-163\n",
      "800 6.496822239698721e-164\n",
      "801 2.849483438464351e-164\n",
      "802 1.2488970189622856e-164\n",
      "803 5.469941393492842e-165\n",
      "804 2.3940569824460966e-165\n",
      "805 1.0470858040789433e-165\n",
      "806 4.5764239688765e-166\n",
      "807 1.9987875475526294e-166\n",
      "808 8.723758500142412e-167\n",
      "809 3.804849310948365e-167\n",
      "810 1.6583199577006469e-167\n",
      "811 7.222647899393061e-168\n",
      "812 3.143561933928038e-168\n",
      "813 1.3672416205323758e-168\n",
      "814 5.942461841674095e-169\n",
      "815 2.5809858589619942e-169\n",
      "816 1.1202195568411432e-169\n",
      "817 4.858689958540697e-170\n",
      "818 2.1058815701025905e-170\n",
      "819 9.121108671615517e-171\n",
      "820 3.9478482823820626e-171\n",
      "821 1.7075468349403384e-171\n",
      "822 7.380475600537423e-172\n",
      "823 3.187835003687553e-172\n",
      "824 1.3759646942712162e-172\n",
      "825 5.9349753893686e-173\n",
      "826 2.5581790471416376e-173\n",
      "827 1.1019034489755503e-173\n",
      "828 4.743041705301095e-174\n",
      "829 2.040193438274731e-174\n",
      "830 8.769744834399635e-175\n",
      "831 3.7670725233675405e-175\n",
      "832 1.6170469279565335e-175\n",
      "833 6.936543102078472e-176\n",
      "834 2.973483840054215e-176\n",
      "835 1.2737679232583168e-176\n",
      "836 5.45277364408526e-177\n",
      "837 2.3326375958612504e-177\n",
      "838 9.971945946739272e-178\n",
      "839 4.260058931450475e-178\n",
      "840 1.818672699560483e-178\n",
      "841 7.758842574916735e-179\n",
      "842 3.3078285193198906e-179\n",
      "843 1.409265729089933e-179\n",
      "844 5.999939241697604e-180\n",
      "845 2.5527311273390076e-180\n",
      "846 1.0853448670659044e-180\n",
      "847 4.6114244861739644e-181\n",
      "848 1.957975749904027e-181\n",
      "849 8.307772190699369e-182\n",
      "850 3.522630677874563e-182\n",
      "851 1.4926401177434587e-182\n",
      "852 6.320461203181989e-183\n",
      "853 2.6745350385841187e-183\n",
      "854 1.1309772659777228e-183\n",
      "855 4.779315694632027e-184\n",
      "856 2.0182921007736597e-184\n",
      "857 8.517437967478307e-185\n",
      "858 3.592036929604549e-185\n",
      "859 1.5138388948097395e-185\n",
      "860 6.3756691998388625e-186\n",
      "861 2.683362457844639e-186\n",
      "862 1.1286013029292728e-186\n",
      "863 4.743616774248793e-187\n",
      "864 1.992446561764446e-187\n",
      "865 8.363190739441094e-188\n",
      "866 3.5080498068125385e-188\n",
      "867 1.470510482399622e-188\n",
      "868 6.159980237934073e-189\n",
      "869 2.5786923300125897e-189\n",
      "870 1.0787702183787607e-189\n",
      "871 4.509908939710538e-190\n",
      "872 1.884153133234683e-190\n",
      "873 7.866370796737989e-191\n",
      "874 3.282030539359976e-191\n",
      "875 1.368425008072038e-191\n",
      "876 5.7017708669668244e-192\n",
      "877 2.374155091175393e-192\n",
      "878 9.879140692307729e-193\n",
      "879 4.108092436921045e-193\n",
      "880 1.7071527746513652e-193\n",
      "881 7.089504878120286e-194\n",
      "882 2.9421915994855104e-194\n",
      "883 1.2202188119963132e-194\n",
      "884 5.057272927703553e-195\n",
      "885 2.0946292775445466e-195\n",
      "886 8.669823168644646e-196\n",
      "887 3.586128047917209e-196\n",
      "888 1.4823611309181584e-196\n",
      "889 6.123434942655975e-197\n",
      "890 2.5278380707793818e-197\n",
      "891 1.0428374879452894e-197\n",
      "892 4.299297031436714e-198\n",
      "893 1.771299040638066e-198\n",
      "894 7.292897894590193e-199\n",
      "895 3.0006986070565307e-199\n",
      "896 1.233839887769955e-199\n",
      "897 5.070019262697053e-200\n",
      "898 2.0819724304767795e-200\n",
      "899 8.543878982586915e-201\n",
      "900 3.503887378029411e-201\n",
      "901 1.436019417225168e-201\n",
      "902 5.88146877959194e-202\n",
      "903 2.407280934672536e-202\n",
      "904 9.846535236716852e-203\n",
      "905 4.024908124884259e-203\n",
      "906 1.6441618157206938e-203\n",
      "907 6.711960384228828e-204\n",
      "908 2.738234490954972e-204\n",
      "909 1.1163708785693787e-204\n",
      "910 4.548447191042123e-205\n",
      "911 1.8519736119878349e-205\n",
      "912 7.53569991857029e-206\n",
      "913 3.0642891666274763e-206\n",
      "914 1.2452410462562892e-206\n",
      "915 5.0570217927886984e-207\n",
      "916 2.0523627405798285e-207\n",
      "917 8.323989051670296e-208\n",
      "918 3.373860672693862e-208\n",
      "919 1.366599429963489e-208\n",
      "920 5.531895360927335e-209\n",
      "921 2.2378217479479506e-209\n",
      "922 9.0468214260509e-210\n",
      "923 3.654986031856375e-210\n",
      "924 1.4756888048515726e-210\n",
      "925 5.954199503113188e-211\n",
      "926 2.4008868964166074e-211\n",
      "927 9.67475377343894e-212\n",
      "928 3.896083188401635e-212\n",
      "929 1.5679665117521068e-212\n",
      "930 6.306171620624624e-213\n",
      "931 2.5346348957494466e-213\n",
      "932 1.018089209410928e-213\n",
      "933 4.086742170082402e-214\n",
      "934 1.6394183930048148e-214\n",
      "935 6.572395738473439e-215\n",
      "936 2.6331713695807043e-215\n",
      "937 1.0542806572632544e-215\n",
      "938 4.218472540265902e-216\n",
      "939 1.6868492243545674e-216\n",
      "940 6.74092560883379e-217\n",
      "941 2.692062942824996e-217\n",
      "942 1.074418479735391e-217\n",
      "943 4.2853321623140996e-218\n",
      "944 1.708120281534638e-218\n",
      "945 6.804175754997762e-219\n",
      "946 2.708668692276179e-219\n",
      "947 1.0776053040564046e-219\n",
      "948 4.284372233048682e-220\n",
      "949 1.7023093742246831e-220\n",
      "950 6.75948766766472e-221\n",
      "951 2.6823363760574287e-221\n",
      "952 1.0637438039567848e-221\n",
      "953 4.215852108262463e-222\n",
      "954 1.6697766588492013e-222\n",
      "955 6.609312297534838e-223\n",
      "956 2.6144431556704265e-223\n",
      "957 1.0335401469285366e-223\n",
      "958 4.0832022239591363e-224\n",
      "959 1.6121297472990905e-224\n",
      "960 6.360991742815226e-225\n",
      "961 2.508277501110105e-225\n",
      "962 9.884447907905519e-226\n",
      "963 3.892740984524858e-226\n",
      "964 1.5320926418942294e-226\n",
      "965 6.0261667790049935e-227\n",
      "966 2.368776249608881e-227\n",
      "967 9.305374959180078e-228\n",
      "968 3.6531779833464503e-228\n",
      "969 1.433293307967063e-228\n",
      "970 5.619876521200844e-229\n",
      "971 2.202145972257384e-229\n",
      "972 8.62369193396532e-230\n",
      "973 3.37495770740659e-230\n",
      "974 1.3199928455125899e-230\n",
      "975 5.159446706975414e-231\n",
      "976 2.0154088699122712e-231\n",
      "977 7.867773539632539e-232\n",
      "978 3.0695121487330438e-232\n",
      "979 1.1967842127000326e-232\n",
      "980 4.66328013053317e-233\n",
      "981 1.8159190539459385e-233\n",
      "982 7.066932806452126e-234\n",
      "983 2.748495957705401e-234\n",
      "984 1.0682897845558927e-234\n",
      "985 4.1496651047074765e-235\n",
      "986 1.610894838783958e-235\n",
      "987 6.2495920188701035e-236\n",
      "988 2.4230738286562123e-236\n",
      "989 9.388847755177512e-237\n",
      "990 3.635706224898355e-237\n",
      "991 1.4070070529792397e-237\n",
      "992 5.4417042581189646e-238\n",
      "993 2.1033179723712757e-238\n",
      "994 8.124683144203012e-239\n",
      "995 3.136458903722596e-239\n",
      "996 1.2100535893991498e-239\n",
      "997 4.665536664864087e-240\n",
      "998 1.7977561131566302e-240\n",
      "999 6.922967164035082e-241\n",
      "1000 2.6643192595578367e-241\n",
      "1001 1.0247381767530141e-241\n",
      "1002 3.938876755661954e-242\n",
      "1003 1.5130903333059136e-242\n",
      "1004 5.808854166561401e-243\n",
      "1005 2.228688676550568e-243\n",
      "1006 8.545585416221505e-244\n",
      "1007 3.27467252307691e-244\n",
      "1008 1.2540872101244292e-244\n",
      "1009 4.799782647444998e-245\n",
      "1010 1.8359021754303084e-245\n",
      "1011 7.017974676721362e-246\n",
      "1012 2.6810722328550437e-246\n",
      "1013 1.0236225690497265e-246\n",
      "1014 3.905763770794133e-247\n",
      "1015 1.4893852085090504e-247\n",
      "1016 5.676010703159491e-248\n",
      "1017 2.1617956669559303e-248\n",
      "1018 8.228515784698273e-249\n",
      "1019 3.1301414275328186e-249\n",
      "1020 1.1899868565742163e-249\n",
      "1021 4.521226658716627e-250\n",
      "1022 1.716747668103215e-250\n",
      "1023 6.514676943318211e-251\n",
      "1024 2.4706754184307535e-251\n",
      "1025 9.36429433910989e-252\n",
      "1026 3.547081189056777e-252\n",
      "1027 1.3427775549124684e-252\n",
      "1028 5.08012089479596e-253\n",
      "1029 1.9207958616137177e-253\n",
      "1030 7.25814639364313e-254\n",
      "1031 2.7409918404996713e-254\n",
      "1032 1.0344926934252988e-254\n",
      "1033 3.901979079003088e-255\n",
      "1034 1.4708907867170867e-255\n",
      "1035 5.541330570814824e-256\n",
      "1036 2.0863443414212443e-256\n",
      "1037 7.850482922265366e-257\n",
      "1038 2.9521972481443162e-257\n",
      "1039 1.1095149008359577e-257\n",
      "1040 4.167348635952366e-258\n",
      "1041 1.5643200585406778e-258\n",
      "1042 5.868547638582974e-259\n",
      "1043 2.200265311406334e-259\n",
      "1044 8.244399398255147e-260\n",
      "1045 3.0873275158235274e-260\n",
      "1046 1.1554369445447333e-260\n",
      "1047 4.3216522462026225e-261\n",
      "1048 1.615450151840095e-261\n",
      "1049 6.03500505020956e-262\n",
      "1050 2.2532127576947284e-262\n",
      "1051 8.407510289905702e-263\n",
      "1052 3.135258908825217e-263\n",
      "1053 1.1684775301226955e-263\n",
      "1054 4.352195806476071e-264\n",
      "1055 1.6200847999091986e-264\n",
      "1056 6.027101190138389e-265\n",
      "1057 2.2408912812828628e-265\n",
      "1058 8.326736330569495e-266\n",
      "1059 3.0922223449827298e-266\n",
      "1060 1.1476478418136616e-266\n",
      "1061 4.256854012661949e-267\n",
      "1062 1.5780152775288959e-267\n",
      "1063 5.846233245142619e-268\n",
      "1064 2.164630200363825e-268\n",
      "1065 8.010028864578986e-269\n",
      "1066 2.962288781279211e-269\n",
      "1067 1.0948731450618016e-269\n",
      "1068 4.044300919997789e-270\n",
      "1069 1.4930230803299576e-270\n",
      "1070 5.508497197203209e-271\n",
      "1071 2.0311567836295016e-271\n",
      "1072 7.485100175521453e-272\n",
      "1073 2.7567398996469696e-272\n",
      "1074 1.0147010820255335e-272\n",
      "1075 3.732714398269326e-273\n",
      "1076 1.372321469951958e-273\n",
      "1077 5.042333443386088e-274\n",
      "1078 1.8516206827945386e-274\n",
      "1079 6.795437033156703e-275\n",
      "1080 2.4924578319970304e-275\n",
      "1081 9.136575630487647e-276\n",
      "1082 3.347221435553798e-276\n",
      "1083 1.2255497347516834e-276\n",
      "1084 4.484593584425071e-277\n",
      "1085 1.64006494456739e-277\n",
      "1086 5.994389417278472e-278\n",
      "1087 2.189651306720657e-278\n",
      "1088 7.993762071848193e-279\n",
      "1089 2.9165798569206775e-279\n",
      "1090 1.0635136584454046e-279\n",
      "1091 3.875778638649433e-280\n",
      "1092 1.411632662678261e-280\n",
      "1093 5.138441550226634e-281\n",
      "1094 1.8693399120440313e-281\n",
      "1095 6.7966110821845235e-282\n",
      "1096 2.469698794398446e-282\n",
      "1097 8.968981676345314e-283\n",
      "1098 3.2552924202763185e-283\n",
      "1099 1.180822845428148e-283\n",
      "1100 4.280825280699493e-284\n",
      "1101 1.5510236524273523e-284\n",
      "1102 5.6163950334130656e-285\n",
      "1103 2.032569134848388e-285\n",
      "1104 7.351595539816217e-286\n",
      "1105 2.6574593478225193e-286\n",
      "1106 9.600647932884825e-287\n",
      "1107 3.4664384506372124e-287\n",
      "1108 1.2508799258939131e-287\n",
      "1109 4.511251896616824e-288\n",
      "1110 1.6260279327482787e-288\n",
      "1111 5.8574493254620985e-289\n",
      "1112 2.108816721436527e-289\n",
      "1113 7.587855215301263e-290\n",
      "1114 2.728659096411559e-290\n",
      "1115 9.806854141789673e-291\n",
      "1116 3.522576918746291e-291\n",
      "1117 1.2645666710031198e-291\n",
      "1118 4.53705034085505e-292\n",
      "1119 1.6268826523433197e-292\n",
      "1120 5.830284734601919e-293\n",
      "1121 2.0882108648287676e-293\n",
      "1122 7.47498161808694e-294\n",
      "1123 2.6742206704661344e-294\n",
      "1124 9.561715783989325e-295\n",
      "1125 3.4168509805565055e-295\n",
      "1126 1.2203039216273234e-295\n",
      "1127 4.355739297641789e-296\n",
      "1128 1.553845354466962e-296\n",
      "1129 5.539950636291222e-297\n",
      "1130 1.9740417033534854e-297\n",
      "1131 7.030063046130646e-298\n",
      "1132 2.5021579748471834e-298\n",
      "1133 8.900675778483151e-299\n",
      "1134 3.164347190871427e-299\n",
      "1135 1.1243416681606833e-299\n",
      "1136 3.992690582956972e-300\n",
      "1137 1.4170537276252742e-300\n",
      "1138 5.026439158716211e-301\n",
      "1139 1.7819197244456219e-301\n",
      "1140 6.313491087179782e-302\n",
      "1141 2.2356554841288183e-302\n",
      "1142 7.912144267160314e-303\n",
      "1143 2.79857960779581e-303\n",
      "1144 9.893168862400346e-304\n",
      "1145 3.495325347088873e-304\n",
      "1146 1.2342250519381615e-304\n",
      "1147 4.3556784723961095e-305\n",
      "1148 1.5362861429162349e-305\n",
      "1149 5.415560289467833e-306\n",
      "1150 1.9079623342262656e-306\n",
      "1151 6.718177233191077e-307\n",
      "1152 2.3642234069506885e-307\n",
      "1153 8.315360885448398e-308\n",
      "1154 2.9230036858297236e-308\n",
      "1155 1.026912480968846e-308\n",
      "1156 3.60573202587376e-309\n",
      "1157 1.265346724408253e-309\n",
      "1158 4.4379444599055e-310\n",
      "1159 1.5556451415821e-310\n",
      "1160 5.449989985924e-311\n",
      "1161 1.908259798993e-311\n",
      "1162 6.67784084194e-312\n",
      "1163 2.335562689543e-312\n",
      "1164 8.1640194685e-313\n",
      "1165 2.8521588417e-313\n",
      "1166 9.9586551734e-314\n",
      "1167 3.4752425924e-314\n",
      "1168 1.2120684266e-314\n",
      "1169 4.225001486e-315\n",
      "1170 1.471920807e-315\n",
      "1171 5.12507244e-316\n",
      "1172 1.7835024e-316\n",
      "1173 6.203055e-317\n",
      "1174 2.1562344e-317\n",
      "1175 7.49109e-318\n",
      "1176 2.601073e-318\n",
      "1177 9.0265e-319\n",
      "1178 3.1307e-319\n",
      "1179 1.0852e-319\n",
      "1180 3.76e-320\n",
      "1181 1.302e-320\n",
      "1182 4.506e-321\n",
      "1183 1.556e-321\n",
      "1184 5.4e-322\n",
      "1185 1.9e-322\n",
      "1186 6.4e-323\n",
      "1187 2e-323\n",
      "1188 5e-324\n",
      "1189 0.0\n",
      "1190 0.0\n",
      "1191 0.0\n",
      "1192 0.0\n",
      "1193 0.0\n",
      "1194 0.0\n",
      "1195 0.0\n",
      "1196 0.0\n",
      "1197 0.0\n",
      "1198 0.0\n",
      "1199 0.0\n",
      "1200 0.0\n",
      "1201 0.0\n",
      "1202 0.0\n",
      "1203 0.0\n",
      "1204 0.0\n",
      "1205 0.0\n",
      "1206 0.0\n",
      "1207 0.0\n",
      "1208 0.0\n",
      "1209 0.0\n",
      "1210 0.0\n",
      "1211 0.0\n",
      "1212 0.0\n",
      "1213 0.0\n",
      "1214 0.0\n",
      "1215 0.0\n",
      "1216 0.0\n",
      "1217 0.0\n",
      "1218 0.0\n",
      "1219 0.0\n",
      "1220 0.0\n",
      "1221 0.0\n",
      "1222 0.0\n",
      "1223 0.0\n",
      "1224 0.0\n",
      "1225 0.0\n",
      "1226 0.0\n",
      "1227 0.0\n",
      "1228 0.0\n",
      "1229 0.0\n",
      "1230 0.0\n",
      "1231 0.0\n",
      "1232 0.0\n",
      "1233 0.0\n",
      "1234 0.0\n",
      "1235 0.0\n",
      "1236 0.0\n",
      "1237 0.0\n",
      "1238 0.0\n",
      "1239 0.0\n",
      "1240 0.0\n",
      "1241 0.0\n",
      "1242 0.0\n",
      "1243 0.0\n",
      "1244 0.0\n",
      "1245 0.0\n",
      "1246 0.0\n",
      "1247 0.0\n",
      "1248 0.0\n",
      "1249 0.0\n",
      "1250 0.0\n",
      "1251 0.0\n",
      "1252 0.0\n",
      "1253 0.0\n",
      "1254 0.0\n",
      "1255 0.0\n",
      "1256 0.0\n",
      "1257 0.0\n",
      "1258 0.0\n",
      "1259 0.0\n",
      "1260 0.0\n",
      "1261 0.0\n",
      "1262 0.0\n",
      "1263 0.0\n",
      "1264 0.0\n",
      "1265 0.0\n",
      "1266 0.0\n",
      "1267 0.0\n",
      "1268 0.0\n",
      "1269 0.0\n",
      "1270 0.0\n",
      "1271 0.0\n",
      "1272 0.0\n",
      "1273 0.0\n",
      "1274 0.0\n",
      "1275 0.0\n",
      "1276 0.0\n",
      "1277 0.0\n",
      "1278 0.0\n",
      "1279 0.0\n",
      "1280 0.0\n",
      "1281 0.0\n",
      "1282 0.0\n",
      "1283 0.0\n",
      "1284 0.0\n",
      "1285 0.0\n",
      "1286 0.0\n",
      "1287 0.0\n",
      "1288 0.0\n",
      "1289 0.0\n",
      "1290 0.0\n",
      "1291 0.0\n",
      "1292 0.0\n",
      "1293 0.0\n",
      "1294 0.0\n",
      "1295 0.0\n",
      "1296 0.0\n",
      "1297 0.0\n",
      "1298 0.0\n",
      "1299 0.0\n",
      "1300 0.0\n",
      "1301 0.0\n",
      "1302 0.0\n",
      "1303 0.0\n",
      "1304 0.0\n",
      "1305 0.0\n",
      "1306 0.0\n",
      "1307 0.0\n",
      "1308 0.0\n",
      "1309 0.0\n",
      "1310 0.0\n",
      "1311 0.0\n",
      "1312 0.0\n",
      "1313 0.0\n",
      "1314 0.0\n",
      "1315 0.0\n",
      "1316 0.0\n",
      "1317 0.0\n",
      "1318 0.0\n",
      "1319 0.0\n",
      "1320 0.0\n",
      "1321 0.0\n",
      "1322 0.0\n",
      "1323 0.0\n",
      "1324 0.0\n",
      "1325 0.0\n",
      "1326 0.0\n",
      "1327 0.0\n",
      "1328 0.0\n",
      "1329 0.0\n",
      "1330 0.0\n",
      "1331 0.0\n",
      "1332 0.0\n",
      "1333 0.0\n",
      "1334 0.0\n",
      "1335 0.0\n",
      "1336 0.0\n",
      "1337 0.0\n",
      "1338 0.0\n",
      "1339 0.0\n",
      "1340 0.0\n",
      "1341 0.0\n",
      "1342 0.0\n",
      "1343 0.0\n",
      "1344 0.0\n",
      "1345 0.0\n",
      "1346 0.0\n",
      "1347 0.0\n",
      "1348 0.0\n",
      "1349 0.0\n",
      "1350 0.0\n",
      "1351 0.0\n",
      "1352 0.0\n",
      "1353 0.0\n",
      "1354 0.0\n",
      "1355 0.0\n",
      "1356 0.0\n",
      "1357 0.0\n",
      "1358 0.0\n",
      "1359 0.0\n",
      "1360 0.0\n",
      "1361 0.0\n",
      "1362 0.0\n",
      "1363 0.0\n",
      "1364 0.0\n",
      "1365 0.0\n",
      "1366 0.0\n",
      "1367 0.0\n",
      "1368 0.0\n",
      "1369 0.0\n",
      "1370 0.0\n",
      "1371 0.0\n",
      "1372 0.0\n",
      "1373 0.0\n",
      "1374 0.0\n",
      "1375 0.0\n",
      "1376 0.0\n",
      "1377 0.0\n",
      "1378 0.0\n",
      "1379 0.0\n",
      "1380 0.0\n",
      "1381 0.0\n",
      "1382 0.0\n",
      "1383 0.0\n",
      "1384 0.0\n",
      "1385 0.0\n",
      "1386 0.0\n",
      "1387 0.0\n",
      "1388 0.0\n",
      "1389 0.0\n",
      "1390 0.0\n",
      "1391 0.0\n",
      "1392 0.0\n",
      "1393 0.0\n",
      "1394 0.0\n",
      "1395 0.0\n",
      "1396 0.0\n",
      "1397 0.0\n",
      "1398 0.0\n",
      "1399 0.0\n",
      "1400 0.0\n",
      "1401 0.0\n",
      "1402 0.0\n",
      "1403 0.0\n",
      "1404 0.0\n",
      "1405 0.0\n",
      "1406 0.0\n",
      "1407 0.0\n",
      "1408 0.0\n",
      "1409 0.0\n",
      "1410 0.0\n",
      "1411 0.0\n",
      "1412 0.0\n",
      "1413 0.0\n",
      "1414 0.0\n",
      "1415 0.0\n",
      "1416 0.0\n",
      "1417 0.0\n",
      "1418 0.0\n",
      "1419 0.0\n",
      "1420 0.0\n",
      "1421 0.0\n",
      "1422 0.0\n",
      "1423 0.0\n",
      "1424 0.0\n",
      "1425 0.0\n",
      "1426 0.0\n",
      "1427 0.0\n",
      "1428 0.0\n",
      "1429 0.0\n",
      "1430 0.0\n",
      "1431 0.0\n",
      "1432 0.0\n",
      "1433 0.0\n",
      "1434 0.0\n",
      "1435 0.0\n",
      "1436 0.0\n",
      "1437 0.0\n",
      "1438 0.0\n",
      "1439 0.0\n",
      "1440 0.0\n",
      "1441 0.0\n",
      "1442 0.0\n",
      "1443 0.0\n",
      "1444 0.0\n",
      "1445 0.0\n",
      "1446 0.0\n",
      "1447 0.0\n",
      "1448 0.0\n",
      "1449 0.0\n",
      "1450 0.0\n",
      "1451 0.0\n",
      "1452 0.0\n",
      "1453 0.0\n",
      "1454 0.0\n",
      "1455 0.0\n",
      "1456 0.0\n",
      "1457 0.0\n",
      "1458 0.0\n",
      "1459 0.0\n",
      "1460 0.0\n",
      "1461 0.0\n",
      "1462 0.0\n",
      "1463 0.0\n",
      "1464 0.0\n",
      "1465 0.0\n",
      "1466 0.0\n",
      "1467 0.0\n",
      "1468 0.0\n",
      "1469 0.0\n",
      "1470 0.0\n",
      "1471 0.0\n",
      "1472 0.0\n",
      "1473 0.0\n",
      "1474 0.0\n",
      "1475 0.0\n",
      "1476 0.0\n",
      "1477 0.0\n",
      "1478 0.0\n",
      "1479 0.0\n",
      "1480 0.0\n",
      "1481 0.0\n",
      "1482 0.0\n",
      "1483 0.0\n",
      "1484 0.0\n",
      "1485 0.0\n",
      "1486 0.0\n",
      "1487 0.0\n",
      "1488 0.0\n",
      "1489 0.0\n",
      "1490 0.0\n",
      "1491 0.0\n",
      "1492 0.0\n",
      "1493 0.0\n",
      "1494 0.0\n",
      "1495 0.0\n",
      "1496 0.0\n",
      "1497 0.0\n",
      "1498 0.0\n",
      "1499 0.0\n",
      "1500 0.0\n",
      "1501 0.0\n",
      "1502 0.0\n",
      "1503 0.0\n",
      "1504 0.0\n",
      "1505 0.0\n",
      "1506 0.0\n",
      "1507 0.0\n",
      "1508 0.0\n",
      "1509 0.0\n",
      "1510 0.0\n",
      "1511 0.0\n",
      "1512 0.0\n",
      "1513 0.0\n",
      "1514 0.0\n",
      "1515 0.0\n",
      "1516 0.0\n",
      "1517 0.0\n",
      "1518 0.0\n",
      "1519 0.0\n",
      "1520 0.0\n",
      "1521 0.0\n",
      "1522 0.0\n",
      "1523 0.0\n",
      "1524 0.0\n",
      "1525 0.0\n",
      "1526 0.0\n",
      "1527 0.0\n",
      "1528 0.0\n",
      "1529 0.0\n",
      "1530 0.0\n",
      "1531 0.0\n",
      "1532 0.0\n",
      "1533 0.0\n",
      "1534 0.0\n",
      "1535 0.0\n",
      "1536 0.0\n",
      "1537 0.0\n",
      "1538 0.0\n",
      "1539 0.0\n",
      "1540 0.0\n",
      "1541 0.0\n",
      "1542 0.0\n",
      "1543 0.0\n",
      "1544 0.0\n",
      "1545 0.0\n",
      "1546 0.0\n",
      "1547 0.0\n",
      "1548 0.0\n",
      "1549 0.0\n",
      "1550 0.0\n",
      "1551 0.0\n",
      "1552 0.0\n",
      "1553 0.0\n",
      "1554 0.0\n",
      "1555 0.0\n",
      "1556 0.0\n",
      "1557 0.0\n",
      "1558 0.0\n",
      "1559 0.0\n",
      "1560 0.0\n",
      "1561 0.0\n",
      "1562 0.0\n",
      "1563 0.0\n",
      "1564 0.0\n",
      "1565 0.0\n",
      "1566 0.0\n",
      "1567 0.0\n",
      "1568 0.0\n",
      "1569 0.0\n",
      "1570 0.0\n",
      "1571 0.0\n",
      "1572 0.0\n",
      "1573 0.0\n",
      "1574 0.0\n",
      "1575 0.0\n",
      "1576 0.0\n",
      "1577 0.0\n",
      "1578 0.0\n",
      "1579 0.0\n",
      "1580 0.0\n",
      "1581 0.0\n",
      "1582 0.0\n",
      "1583 0.0\n",
      "1584 0.0\n",
      "1585 0.0\n",
      "1586 0.0\n",
      "1587 0.0\n",
      "1588 0.0\n",
      "1589 0.0\n",
      "1590 0.0\n",
      "1591 0.0\n",
      "1592 0.0\n",
      "1593 0.0\n",
      "1594 0.0\n",
      "1595 0.0\n",
      "1596 0.0\n",
      "1597 0.0\n",
      "1598 0.0\n",
      "1599 0.0\n",
      "1600 0.0\n",
      "1601 0.0\n",
      "1602 0.0\n",
      "1603 0.0\n",
      "1604 0.0\n",
      "1605 0.0\n",
      "1606 0.0\n",
      "1607 0.0\n",
      "1608 0.0\n",
      "1609 0.0\n",
      "1610 0.0\n",
      "1611 0.0\n",
      "1612 0.0\n",
      "1613 0.0\n",
      "1614 0.0\n",
      "1615 0.0\n",
      "1616 0.0\n",
      "1617 0.0\n",
      "1618 0.0\n",
      "1619 0.0\n",
      "1620 0.0\n",
      "1621 0.0\n",
      "1622 0.0\n",
      "1623 0.0\n",
      "1624 0.0\n",
      "1625 0.0\n",
      "1626 0.0\n",
      "1627 0.0\n",
      "1628 0.0\n",
      "1629 0.0\n",
      "1630 0.0\n",
      "1631 0.0\n",
      "1632 0.0\n",
      "1633 0.0\n",
      "1634 0.0\n",
      "1635 0.0\n",
      "1636 0.0\n",
      "1637 0.0\n",
      "1638 0.0\n",
      "1639 0.0\n",
      "1640 0.0\n",
      "1641 0.0\n",
      "1642 0.0\n",
      "1643 0.0\n",
      "1644 0.0\n",
      "1645 0.0\n",
      "1646 0.0\n",
      "1647 0.0\n",
      "1648 0.0\n",
      "1649 0.0\n",
      "1650 0.0\n",
      "1651 0.0\n",
      "1652 0.0\n",
      "1653 0.0\n",
      "1654 0.0\n",
      "1655 0.0\n",
      "1656 0.0\n",
      "1657 0.0\n",
      "1658 0.0\n",
      "1659 0.0\n",
      "1660 0.0\n",
      "1661 0.0\n",
      "1662 0.0\n",
      "1663 0.0\n",
      "1664 0.0\n",
      "1665 0.0\n",
      "1666 0.0\n",
      "1667 0.0\n",
      "1668 0.0\n",
      "1669 0.0\n",
      "1670 0.0\n",
      "1671 0.0\n",
      "1672 0.0\n",
      "1673 0.0\n",
      "1674 0.0\n",
      "1675 0.0\n",
      "1676 0.0\n",
      "1677 0.0\n",
      "1678 0.0\n",
      "1679 0.0\n",
      "1680 0.0\n",
      "1681 0.0\n",
      "1682 0.0\n",
      "1683 0.0\n",
      "1684 0.0\n",
      "1685 0.0\n",
      "1686 0.0\n",
      "1687 0.0\n",
      "1688 0.0\n",
      "1689 0.0\n",
      "1690 0.0\n",
      "1691 0.0\n",
      "1692 0.0\n",
      "1693 0.0\n",
      "1694 0.0\n",
      "1695 0.0\n",
      "1696 0.0\n",
      "1697 0.0\n",
      "1698 0.0\n",
      "1699 0.0\n",
      "1700 0.0\n",
      "1701 0.0\n",
      "1702 0.0\n",
      "1703 0.0\n",
      "1704 0.0\n",
      "1705 0.0\n",
      "1706 0.0\n",
      "1707 0.0\n",
      "1708 0.0\n",
      "1709 0.0\n",
      "1710 0.0\n",
      "1711 0.0\n",
      "1712 0.0\n",
      "1713 0.0\n",
      "1714 0.0\n",
      "1715 0.0\n",
      "1716 0.0\n",
      "1717 0.0\n",
      "1718 0.0\n",
      "1719 0.0\n",
      "1720 0.0\n",
      "1721 0.0\n",
      "1722 0.0\n",
      "1723 0.0\n",
      "1724 0.0\n",
      "1725 0.0\n",
      "1726 0.0\n",
      "1727 0.0\n",
      "1728 0.0\n",
      "1729 0.0\n",
      "1730 0.0\n",
      "1731 0.0\n",
      "1732 0.0\n",
      "1733 0.0\n",
      "1734 0.0\n",
      "1735 0.0\n",
      "1736 0.0\n",
      "1737 0.0\n",
      "1738 0.0\n",
      "1739 0.0\n",
      "1740 0.0\n",
      "1741 0.0\n",
      "1742 0.0\n",
      "1743 0.0\n",
      "1744 0.0\n",
      "1745 0.0\n",
      "1746 0.0\n",
      "1747 0.0\n",
      "1748 0.0\n",
      "1749 0.0\n",
      "1750 0.0\n",
      "1751 0.0\n",
      "1752 0.0\n",
      "1753 0.0\n",
      "1754 0.0\n",
      "1755 0.0\n",
      "1756 0.0\n",
      "1757 0.0\n",
      "1758 0.0\n",
      "1759 0.0\n",
      "1760 0.0\n",
      "1761 0.0\n",
      "1762 0.0\n",
      "1763 0.0\n",
      "1764 0.0\n",
      "1765 0.0\n",
      "1766 0.0\n",
      "1767 0.0\n",
      "1768 0.0\n",
      "1769 0.0\n",
      "1770 0.0\n",
      "1771 0.0\n",
      "1772 0.0\n",
      "1773 0.0\n",
      "1774 0.0\n",
      "1775 0.0\n",
      "1776 0.0\n",
      "1777 0.0\n",
      "1778 0.0\n",
      "1779 0.0\n",
      "1780 0.0\n",
      "1781 0.0\n",
      "1782 0.0\n",
      "1783 0.0\n",
      "1784 0.0\n",
      "1785 0.0\n",
      "1786 0.0\n",
      "1787 0.0\n",
      "1788 0.0\n",
      "1789 0.0\n",
      "1790 0.0\n",
      "1791 0.0\n",
      "1792 0.0\n",
      "1793 0.0\n",
      "1794 0.0\n",
      "1795 0.0\n",
      "1796 0.0\n",
      "1797 0.0\n",
      "1798 0.0\n",
      "1799 0.0\n",
      "1800 0.0\n",
      "1801 0.0\n",
      "1802 0.0\n",
      "1803 0.0\n",
      "1804 0.0\n",
      "1805 0.0\n",
      "1806 0.0\n",
      "1807 0.0\n",
      "1808 0.0\n",
      "1809 0.0\n",
      "1810 0.0\n",
      "1811 0.0\n",
      "1812 0.0\n",
      "1813 0.0\n",
      "1814 0.0\n",
      "1815 0.0\n",
      "1816 0.0\n",
      "1817 0.0\n",
      "1818 0.0\n",
      "1819 0.0\n",
      "1820 0.0\n",
      "1821 0.0\n",
      "1822 0.0\n",
      "1823 0.0\n",
      "1824 0.0\n",
      "1825 0.0\n",
      "1826 0.0\n",
      "1827 0.0\n",
      "1828 0.0\n",
      "1829 0.0\n",
      "1830 0.0\n",
      "1831 0.0\n",
      "1832 0.0\n",
      "1833 0.0\n",
      "1834 0.0\n",
      "1835 0.0\n",
      "1836 0.0\n",
      "1837 0.0\n",
      "1838 0.0\n",
      "1839 0.0\n",
      "1840 0.0\n",
      "1841 0.0\n",
      "1842 0.0\n",
      "1843 0.0\n",
      "1844 0.0\n",
      "1845 0.0\n",
      "1846 0.0\n",
      "1847 0.0\n",
      "1848 0.0\n",
      "1849 0.0\n",
      "1850 0.0\n",
      "1851 0.0\n",
      "1852 0.0\n",
      "1853 0.0\n",
      "1854 0.0\n",
      "1855 0.0\n",
      "1856 0.0\n",
      "1857 0.0\n",
      "1858 0.0\n",
      "1859 0.0\n",
      "1860 0.0\n",
      "1861 0.0\n",
      "1862 0.0\n",
      "1863 0.0\n",
      "1864 0.0\n",
      "1865 0.0\n",
      "1866 0.0\n",
      "1867 0.0\n",
      "1868 0.0\n",
      "1869 0.0\n",
      "1870 0.0\n",
      "1871 0.0\n",
      "1872 0.0\n",
      "1873 0.0\n",
      "1874 0.0\n",
      "1875 0.0\n",
      "1876 0.0\n",
      "1877 0.0\n",
      "1878 0.0\n",
      "1879 0.0\n",
      "1880 0.0\n",
      "1881 0.0\n",
      "1882 0.0\n",
      "1883 0.0\n",
      "1884 0.0\n",
      "1885 0.0\n",
      "1886 0.0\n",
      "1887 0.0\n",
      "1888 0.0\n",
      "1889 0.0\n",
      "1890 0.0\n",
      "1891 0.0\n",
      "1892 0.0\n",
      "1893 0.0\n",
      "1894 0.0\n",
      "1895 0.0\n",
      "1896 0.0\n",
      "1897 0.0\n",
      "1898 0.0\n",
      "1899 0.0\n",
      "1900 0.0\n",
      "1901 0.0\n",
      "1902 0.0\n",
      "1903 0.0\n",
      "1904 0.0\n",
      "1905 0.0\n",
      "1906 0.0\n",
      "1907 0.0\n",
      "1908 0.0\n",
      "1909 0.0\n",
      "1910 0.0\n",
      "1911 0.0\n",
      "1912 0.0\n",
      "1913 0.0\n",
      "1914 0.0\n",
      "1915 0.0\n",
      "1916 0.0\n",
      "1917 0.0\n",
      "1918 0.0\n",
      "1919 0.0\n",
      "1920 0.0\n",
      "1921 0.0\n",
      "1922 0.0\n",
      "1923 0.0\n",
      "1924 0.0\n",
      "1925 0.0\n",
      "1926 0.0\n",
      "1927 0.0\n",
      "1928 0.0\n",
      "1929 0.0\n",
      "1930 0.0\n",
      "1931 0.0\n",
      "1932 0.0\n",
      "1933 0.0\n",
      "1934 0.0\n",
      "1935 0.0\n",
      "1936 0.0\n",
      "1937 0.0\n",
      "1938 0.0\n",
      "1939 0.0\n",
      "1940 0.0\n",
      "1941 0.0\n",
      "1942 0.0\n",
      "1943 0.0\n",
      "1944 0.0\n",
      "1945 0.0\n",
      "1946 0.0\n",
      "1947 0.0\n",
      "1948 0.0\n",
      "1949 0.0\n",
      "1950 0.0\n",
      "1951 0.0\n",
      "1952 0.0\n",
      "1953 0.0\n",
      "1954 0.0\n",
      "1955 0.0\n",
      "1956 0.0\n",
      "1957 0.0\n",
      "1958 0.0\n",
      "1959 0.0\n",
      "1960 0.0\n",
      "1961 0.0\n",
      "1962 0.0\n",
      "1963 0.0\n",
      "1964 0.0\n",
      "1965 0.0\n",
      "1966 0.0\n",
      "1967 0.0\n",
      "1968 0.0\n",
      "1969 0.0\n",
      "1970 0.0\n",
      "1971 0.0\n",
      "1972 0.0\n",
      "1973 0.0\n",
      "1974 0.0\n",
      "1975 0.0\n",
      "1976 0.0\n",
      "1977 0.0\n",
      "1978 0.0\n",
      "1979 0.0\n",
      "1980 0.0\n",
      "1981 0.0\n",
      "1982 0.0\n",
      "1983 0.0\n",
      "1984 0.0\n",
      "1985 0.0\n",
      "1986 0.0\n",
      "1987 0.0\n",
      "1988 0.0\n",
      "1989 0.0\n",
      "1990 0.0\n",
      "1991 0.0\n",
      "1992 0.0\n",
      "1993 0.0\n",
      "1994 0.0\n",
      "1995 0.0\n",
      "1996 0.0\n",
      "1997 0.0\n",
      "1998 0.0\n",
      "1999 0.0\n"
     ]
    }
   ],
   "source": [
    "decay_rate = 0.8\n",
    "lr = 0.18\n",
    "step = 500\n",
    "for i in range(2000):\n",
    "    print(i , lr)\n",
    "    lr = lr / (1 + decay_rate * (i / step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different hyperparameters\n",
    "- Run number of different fitting and see their results.\n",
    "- Normalized X works way betetr!! The training time is way faster and the loss and val_loss decrease close to each other.\n",
    "- Using BatchNormalizations before each layer would be very helpful.\n",
    "\n",
    "Note: A lower loss value would not necessary mean that the model's prediction would improve\n",
    "##### Important Note: when we increase the amount of regularazation, the loss will be increase naturally. Hence, if one were to compare different model epochs, this should be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:56154.5586,  val_loss:50657.1211,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:471.4744,  val_loss:466.2067,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:52.5536,  val_loss:51.9563,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:5.1245,  val_loss:5.1230,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:1.6325,  val_loss:58.6813,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:1.0239,  val_loss:1.0482,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.8372,  val_loss:0.8500,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.7522,  val_loss:0.7706,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.6980,  val_loss:0.7050,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.6591,  val_loss:0.6636,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.6245,  val_loss:0.6411,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.6021,  val_loss:0.5934,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.5777,  val_loss:0.6266,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.5619,  val_loss:0.5602,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.5474,  val_loss:0.8209,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.5334,  val_loss:0.5370,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.5195,  val_loss:0.5224,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.5134,  val_loss:0.5169,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.5043,  val_loss:0.5077,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.4966,  val_loss:0.4985,  \n",
      ".........\n",
      "MAEs:\n",
      "b011: 14.421\n",
      "b012: 14.863\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n",
      "###############################################\n",
      "Lograithmic Error:\n",
      "MSLE:\n",
      "b011: 0.010690432775512192\n",
      "b012: 0.010733275210683272\n",
      "-----------------------------------\n",
      "base-differences: 0.0020225966675850296\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Model01\n",
    "    prev-err: 11.125\n",
    "    \n",
    "    more regularazation at the last layers\n",
    "\"\"\"\n",
    "history = model.fit(norm_X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.20,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:48199.0391,  val_loss:42734.3555,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:8.4621,  val_loss:8.3970,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:1.1681,  val_loss:1.1689,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.3892,  val_loss:0.3936,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.3198,  val_loss:0.3128,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.2920,  val_loss:0.2993,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.2756,  val_loss:0.2716,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.2738,  val_loss:0.2798,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.2805,  val_loss:0.3075,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.2500,  val_loss:0.2483,  \n",
      "....................................................................................................\n",
      "b011: 10.872\n",
      "b012: 11.765\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Model01\n",
    "    prev-err: 11.125\n",
    "    \n",
    "    0.018 # fixed\n",
    "    decay_steps=500\n",
    "    decay_rate=0.35\n",
    "    \n",
    "    error:10.872, 0.13119\n",
    "\"\"\"\n",
    "history = model.fit(norm_X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.20,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:23061.1875,  val_loss:7358.2192,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.7990,  val_loss:0.8313,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.4338,  val_loss:0.4407,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.3621,  val_loss:0.3572,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.3286,  val_loss:0.3325,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.3178,  val_loss:0.3448,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.2939,  val_loss:0.3007,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.2838,  val_loss:0.2810,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.2761,  val_loss:0.2729,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.2699,  val_loss:0.2697,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.2633,  val_loss:0.2620,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.2623,  val_loss:0.2601,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.2604,  val_loss:0.2567,  \n",
      "..........................................\n",
      "b011: 10.473\n",
      "b012: 11.416\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Model01\n",
    "    prev-err: 10.676\n",
    "    added initializers and regularizers to all of the layers\n",
    "    changed decay_rate from 0.6 to 0.5\n",
    "    using batchNormalization\n",
    "\"\"\"\n",
    "history = model.fit(norm_X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.20,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0067319995507884095\n",
      "11134.41652298235\n"
     ]
    }
   ],
   "source": [
    "b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']\n",
    "pred_y = quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error as MSLE,mean_absolute_error as MAE\n",
    "print(MSLE(b011, pred_y))\n",
    "print(MAE(b011, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(y_pred):\n",
    "    \"\"\" Prints out the data validation with respect to the highest submissions. \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error as MAE\n",
    "    from sklearn.metrics import mean_squared_log_error as MSLE\n",
    "    # Import the base_validation submititions\n",
    "    b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "    b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']\n",
    "    \n",
    "    # Print out the differences\n",
    "    print('MAEs:')\n",
    "    print('b011:', int(MAE(b011, y_pred)) / 1000)\n",
    "    print('b012:', int(MAE(b012, y_pred)) / 1000)\n",
    "    print('-----------------------------------')\n",
    "    print('base-differences:', int(MAE(b011, b012)) / 1000)\n",
    "    print('###############################################')\n",
    "    print('Lograithmic Error:')\n",
    "    print('MSLE:')\n",
    "    print('b011:', MSLE(b011, y_pred))\n",
    "    print('b012:', MSLE(b012, y_pred))\n",
    "    print('-----------------------------------')\n",
    "    print('base-differences:', MSLE(b011, b012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.2575,  \n",
      ".......\n",
      "b011: 11.134\n",
      "b012: 12.177\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n"
     ]
    }
   ],
   "source": [
    "train_stop = EarlyStopping(monitor='loss', patience=5, mode='min', restore_best_weights=True)\n",
    "# Fit on whole model then predict\n",
    "history = model.fit(norm_X, y, epochs=4000,\n",
    "          verbose=0,\n",
    "          callbacks=[train_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "predictions = quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])\n",
    "# See how accurate it is\n",
    "validate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
