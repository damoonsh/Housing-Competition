{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Difference with previous versions:``\n",
    "- Using a different approach to Encoding and imputing data, meaning that having more zeros for either the missing numerical values, and nan values in the categorical ones. Since I will be using all (or at least most) the feature in the dataset it could be helpfull to just have zeros rather values that are probably misleading. The columns with a low number of missing values will just imputed using the KNN algorithm.\n",
    "- Using regularizers more extensively, as well as controlling the properties of Layers such as weight and bias-initializers more closely.\n",
    "- Written some new utility functions that can help enhance EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "\n",
    "# The dependent feature\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "The general strategy is to combine both the categorical and numerical values in the training and testing and then process them at the same time. For categorical variables we will be getting dictionaries from the training data and then process them for the combined dataframe. In the case of numerical features, imputation is going to be done using the KNN imputation.\n",
    "\n",
    "1. Encoding categorical features: I'll be using some functions written in utils.py to come up with meaning values for the unique keys in each of the categorical features, then map them in the data given certain conditions.\n",
    "2. Imputing numerical data: The numerical \n",
    "\n",
    "## In-depth analysis of categorical variables:\n",
    "1. Compare the different NaNs for the same categories (and not) in the number of NaNs they have.\n",
    "2. Given that 90% data is not missing for a given feature (column) map their encoded numerical values in the dataframe, otherwise, only impute non-nan values in the feature and then impute the rest of the missing values using any other technique. Dropping the column for values with too many missing might be a general option but in order to use the data for Nerual Networks, it would make sense to just impute the missing values with zeros.\n",
    "\n",
    "## Encoding categorical variables:\n",
    "In order to come up with a meaningful value for any given unique value in a categorical feature column, we will be considering the average SalePrice for each of those unique values and weight them relative to each other. The important thing to note would be that given that more than 90% exists in a column we could just impute the minor missing values with the average of SalePrice for those columns. But if less then 90% of the data existed then there would be a problem since our measures would not make sense and since we are using Neural Networks it would make more sense to impute them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFrames\n",
    "cat_info, num_info = missing_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>1352</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrical</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>730</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>1456</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>1169</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>1408</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test  Train\n",
       "Alley         1352   1369\n",
       "MasVnrType      16      8\n",
       "BsmtQual        44     37\n",
       "BsmtCond        45     37\n",
       "BsmtExposure    44     38\n",
       "BsmtFinType1    42     37\n",
       "BsmtFinType2    42     38\n",
       "Electrical       0      1\n",
       "FireplaceQu    730    690\n",
       "GarageType      76     81\n",
       "GarageFinish    78     81\n",
       "GarageQual      78     81\n",
       "GarageCond      78     81\n",
       "PoolQC        1456   1453\n",
       "Fence         1169   1179\n",
       "MiscFeature   1408   1406"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important to note for categorical features:\n",
    "1. Alley, PoolQc, Fence, MiscFeature are the features with an ecessive number of missing values both in training and testing.\n",
    "2. FireplaceQu is not as bad ass the described functions but it is going to be treated the same way.\n",
    "3. Although for some these values NA means that they just don't have that feature: Alley, MiscFeature, PoolQc\n",
    "\n",
    "Note: To conclude there are 5 features that the np.nan values in them should not be imputed with their given dictionary value but a zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test  Train\n",
       "LotFrontage   259.0    227\n",
       "MasVnrArea      8.0     15\n",
       "GarageYrBlt    81.0     78\n",
       "BsmtFinSF1      0.0      1\n",
       "BsmtFinSF2      0.0      1\n",
       "BsmtUnfSF       0.0      1\n",
       "TotalBsmtSF     0.0      1\n",
       "BsmtFullBath    0.0      2\n",
       "BsmtHalfBath    0.0      2\n",
       "GarageCars      0.0      1\n",
       "GarageArea      0.0      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data:\n",
    "Based on this dataframe, there some features missing in Training that are not missing in the test data. There is no need manually impute anything in the case of numerical values and I am just going to let KNN handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of training data to rebreak the combined data further along the way\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Combine the train and test:\n",
    "# Note: Pass the copies so the actual dataframes won't change and we can still use them\n",
    "feat_cols = combine_train_test(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "2914         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "2915         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "2916          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "2917          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "2918          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "1            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "2            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "3            Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n",
       "4            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "...          ...       ...       ...  ...         ...      ...    ...    ...   \n",
       "2914         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2915         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2916         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2917         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n",
       "2918         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0            NaN       0       2    2008        WD         Normal  \n",
       "1            NaN       0       5    2007        WD         Normal  \n",
       "2            NaN       0       9    2008        WD         Normal  \n",
       "3            NaN       0       2    2006        WD        Abnorml  \n",
       "4            NaN       0      12    2008        WD         Normal  \n",
       "...          ...     ...     ...     ...       ...            ...  \n",
       "2914         NaN       0       6    2006        WD         Normal  \n",
       "2915         NaN       0       4    2006        WD        Abnorml  \n",
       "2916         NaN       0       9    2006        WD        Abnorml  \n",
       "2917        Shed     700       7    2006        WD         Normal  \n",
       "2918         NaN       0      11    2006        WD         Normal  \n",
       "\n",
       "[2919 rows x 79 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring: Alley\n",
      "Ignoring: FireplaceQu\n",
      "Ignoring: PoolQC\n",
      "Ignoring: Fence\n",
      "Ignoring: MiscFeature\n"
     ]
    }
   ],
   "source": [
    "# Get the needed dictionaries to be used for encoding categorical features\n",
    "cat_dicts = get_encoding_dicts(train, data['train_cat_list'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pave': 0.5818, 'Grvl': 0.4182}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking one of the values.\n",
    "cat_dicts['Street']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.3491</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSZoning  Street  Alley  LotShape  LandContour  Utilities  LotConfig  \\\n",
       "0       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "1       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1837   \n",
       "2       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1826   \n",
       "3       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1875   \n",
       "4       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1837   \n",
       "...        ...     ...    ...       ...          ...        ...        ...   \n",
       "2914    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2915    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2916    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2917    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2918    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "\n",
       "      LandSlope  Neighborhood  Condition1  ...  GarageType  GarageFinish  \\\n",
       "0        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "1        0.3097        0.0519      0.0875  ...      0.1817        0.2939   \n",
       "2        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "3        0.3097        0.0458      0.1133  ...      0.1201        0.2067   \n",
       "4        0.3097        0.0729      0.1133  ...      0.1817        0.2939   \n",
       "...         ...           ...         ...  ...         ...           ...   \n",
       "2914     0.3097        0.0214      0.1133  ...      0.0925        0.1503   \n",
       "2915     0.3097        0.0214      0.1133  ...      0.0985        0.2067   \n",
       "2916     0.3097        0.0340      0.1133  ...      0.1201        0.2067   \n",
       "2917     0.3097        0.0340      0.1133  ...      0.0925        0.1503   \n",
       "2918     0.3386        0.0340      0.1133  ...      0.1817        0.3491   \n",
       "\n",
       "      GarageQual  GarageCond  PavedDrive  PoolQC  Fence  MiscFeature  \\\n",
       "0         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "1         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "3         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "4         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "...          ...         ...         ...     ...    ...          ...   \n",
       "2914      0.1064      0.1263      0.4298     0.0  0.000        0.000   \n",
       "2915      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2916      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2917      0.1064      0.1263      0.4298     0.0  0.247        0.227   \n",
       "2918      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "\n",
       "      SaleType  SaleCondition  \n",
       "0       0.1035         0.1726  \n",
       "1       0.1035         0.1726  \n",
       "2       0.1035         0.1726  \n",
       "3       0.1035         0.1443  \n",
       "4       0.1035         0.1726  \n",
       "...        ...            ...  \n",
       "2914    0.1035         0.1726  \n",
       "2915    0.1035         0.1443  \n",
       "2916    0.1035         0.1443  \n",
       "2917    0.1035         0.1726  \n",
       "2918    0.1035         0.1726  \n",
       "\n",
       "[2919 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the encoding\n",
    "encoded_feat_cols = encode_categorical(feat_cols.copy(), cat_dicts)\n",
    "encoded_feat_cols[data['train_cat_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: They were some features which did not have any missing values in the training dataset however they did in test set. Hence they are going to be some missing values in the previousley categorical features and from now on they are going to be imputed the same way the numerical features will be imputed, in other words, they will be treated as numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data with KNN:\n",
    "- Both the features of train and test are going to be implemented at the same time together using the KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the missing values with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "missing_features = encoded_feat_cols.columns[encoded_feat_cols.isna().any()].tolist()\n",
    "\n",
    "# The number of neighbors that the function look for is the 1/3 of the whole dataframe\n",
    "num = (train_len + test_len) // 3\n",
    "\n",
    "# Instantiate the Imputer object\n",
    "imputer = KNNImputer(n_neighbors=num, weights=\"distance\")\n",
    "# Fit and transform using the imputer on the missing data and get the imputed combined data\n",
    "imputed_combined = pd.DataFrame()\n",
    "imputed_combined[encoded_feat_cols.columns.to_list()] = pd.DataFrame(imputer.fit_transform(encoded_feat_cols))\n",
    "\n",
    "# Check the imputation:\n",
    "True in imputed_combined.isna().any().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitting and predicting datasets:\n",
    "\n",
    "# features:\n",
    "features = imputed_combined.columns.to_list()\n",
    "\n",
    "train_part = pd.DataFrame()\n",
    "train_part = imputed_combined.iloc[:train_len]\n",
    "# Add the y_feat column (for use further along the way)\n",
    "train_part.loc[:, (y_feat)] = train[y_feat]\n",
    "\n",
    "# X would be the features that will be used for both prediction and training\n",
    "X = train_part[features]\n",
    "y = train[y_feat] # y, the dependent column of the dataset\n",
    "\n",
    "# The dataset used for prediction\n",
    "X_test = imputed_combined[train_len: ].reset_index()\n",
    "X_test.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "# Normalized version of datasets\n",
    "norm_X = normalize(X.copy())\n",
    "norm_y = normalize(y.copy())\n",
    "norm_X_test = normalize(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738629\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681904\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553058\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_part.corr()[y_feat].nlargest(13)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and variance detection using validation data:\n",
    "\n",
    "##### `Note:` One way of validating the accuracy of the neural networks is by using a validation set. In tensorflow's optimizer's they are two options for validation\n",
    "    - Validation_split: This will split a portion of the data being trained to be used as the validation set.\n",
    "    - Validation_data: This will use a given data (from the dataset) to evaluate the predictions.\n",
    "##### Now it would make sense for me to have a number of different randomly selected validation batches to run on the model after being trained to see if there is a variance or bias in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting and undefitting\n",
    "portion = 0.3\n",
    "num = 5\n",
    "devs = []\n",
    "dev_batch_size = int(train_part.shape[0] * portion)\n",
    "\n",
    "for i in range(num):\n",
    "    dev_data = train_part.sample(n=dev_batch_size, random_state=i)\n",
    "    dev_x = dev_data[features]\n",
    "    dev_y = dev_data[y_feat]\n",
    "    devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting parts\n",
    "\n",
    "- Some usefull functions used for hyperparameter tuning\n",
    "- Imports used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2, L1L2\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer,LeakyReLU\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scheduler objects to control the optimizer learning rate:\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay, ExponentialDecay\n",
    "\n",
    "def TimeDecayScheduler(learning_rate=0.001, decay_steps=200, decay_rate=1.2, name=\"\"):\n",
    "    \"\"\" Returns an InverseTimeDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "def ExponentialScheduler(initial_learning_rate, decay_steps, decay_rate, name=\"\"):\n",
    "    \"\"\" Returns an ExponentialDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=initial_learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "# Actual Optimizers: Adam and RMSprop are the main two optimizers that are going to be used for this project since they accept schedulers and happen to be effective.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def AdamOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "        learning_rate: the initial learning rate to be used\n",
    "        scheduler: If this is passed by the user then use it in the optimizer instead of the learning rate\n",
    "\n",
    "        # returns: an Adam optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return Adam(learning_rate)\n",
    "    else:\n",
    "        return Adam(scheduler)\n",
    "    \n",
    "\n",
    "def RMSpropOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "            learning_rate: the initial learning rate to be used\n",
    "            scheduler: If this is passed by the user then use it in the \n",
    "            optimizer instead of the learning rate\n",
    "        \n",
    "        # returns: an RMSprop optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return RMSprop(learning_rate)\n",
    "    else:\n",
    "        return RMSprop(scheduler)\n",
    "\n",
    "# CallBacks:\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def EarlyStopCallBack(patience=100):\n",
    "    \"\"\"\n",
    "        # params: patience of the object for the number of epochs passed with no improvement\n",
    "        # returns: a EarlyStopping callback object \n",
    "    \"\"\"\n",
    "    return EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "\n",
    "# Models: \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Layers \n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2, L1L2  # Regularizer\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError # Error-metric\n",
    "import tensorflow_docs as tfdocs # For logging puposes\n",
    "\n",
    "def Model01(config):\n",
    "    \"\"\"\n",
    "        # params: \n",
    "        config: uses the configuration dictionary to compile and fit the model accordingly\n",
    "        \n",
    "        # returns a history object when the fitting is done\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to try out and improve the model\n",
    "1. Weight-Initializers:\n",
    "    - Use tf.keras.initializers.RandomNormal and tf.keras.initializers.RandomUniform\n",
    "    - Tweak their properties and see how they would work.\n",
    "2. Bias in Dense layers:\n",
    "    - Setup an initiallizer and regularizer for the bias of the layer\n",
    "    - Also use it those for the weights too\n",
    "    - Tweak arguments of earlt-stop call back\n",
    "3. Layers:\n",
    "    - Use LeakyRelu/TreshholdRelu/PRelu as a layer\n",
    "    - Maybe try-out tf.keras.layers.experimental.preprocessing.Normalization*\n",
    "    - Tweak BatchNormalization layer arguments\n",
    "4. Overfitting:\n",
    "    - use tf.keras.layers.GaussianDropout and tf.keras.layers.GaussianNoise ( which could be viewed as a Data augmentation method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model01():\n",
    "    model = keras.Sequential([\n",
    "        InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        Dense(64, activation='elu', \n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=2.5)\n",
    "        ),\n",
    "        Dense(256, \n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.5),\n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        Dense(64, activation='elu', \n",
    "              bias_regularizer=l1(0.001),\n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=1.5),\n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.5)\n",
    "        ),\n",
    "        \n",
    "        Dense(128, activation = 'relu', \n",
    "              bias_regularizer=l1(0.001), \n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=1.5),\n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.5)\n",
    "        ),\n",
    "        Dense(512, kernel_regularizer=l2(0.001)),\n",
    "        Dense(128, activation = 'elu',\n",
    "              bias_regularizer=l1(0.001), \n",
    "        ),\n",
    "        Dense(8, kernel_regularizer=l1(0.01), \n",
    "              bias_regularizer=l1(0.001), \n",
    "        ),\n",
    "        \n",
    "        Dense(32, \n",
    "              kernel_regularizer=l1(0.01), \n",
    "              bias_regularizer=l1(0.001), \n",
    "              bias_initializer='ones',\n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.65)  \n",
    "        ),\n",
    "        Dense(8, \n",
    "              kernel_regularizer=l1(0.01), \n",
    "        ),\n",
    "        \n",
    "        Dense(128, activation = 'elu'),\n",
    "        Dense(1024, \n",
    "              kernel_regularizer=l2(0.001), \n",
    "              bias_regularizer=l1(0.001), \n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.005) ,\n",
    "             kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=1.5)\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation = 'elu', bias_regularizer=l1(0.001)),\n",
    "        \n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.001, 0.001), \n",
    "              bias_regularizer=l2(0.01), \n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.05), \n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=2.5)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.01, 0.01), \n",
    "              bias_regularizer=l2(0.01), \n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.005), \n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=1.5)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.1, 0.1), \n",
    "              bias_regularizer=l2(0.01), \n",
    "              bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.005), \n",
    "              kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.8)\n",
    "        ),\n",
    "        \n",
    "        Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = InverseTimeDecay(\n",
    "      0.018,\n",
    "      decay_steps=90,\n",
    "      decay_rate=0.60\n",
    "    )\n",
    "    \n",
    "    exp_lr = ExponentialDecay(\n",
    "        initial_learning_rate = 0.025, \n",
    "        decay_steps=50,\n",
    "        decay_rate=0.70,\n",
    "    )\n",
    "    \n",
    "    optimizer = Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "        loss=MeanSquaredLogarithmicError(name='MSLE'), \n",
    "        optimizer=optimizer, \n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1500\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=35, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different hyperparameters\n",
    "- Run number of different fitting and see their results.\n",
    "- Normalized X works way betetr!! The training time is way faster and the loss and val_loss decrease close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:617.5388,  val_loss:538.3251,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:8.4957,  val_loss:8.4497,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:1.8730,  val_loss:1.8808,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.5553,  val_loss:0.5650,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.2285,  val_loss:0.2375,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.1333,  val_loss:0.1437,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.1019,  val_loss:0.1106,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0875,  val_loss:0.0974,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0808,  val_loss:0.0887,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0751,  val_loss:0.0836,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.0714,  val_loss:0.0798,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.0683,  val_loss:0.0770,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.0657,  val_loss:0.0753,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.0641,  val_loss:0.0728,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0619,  val_loss:0.0710,  \n",
      "....................................................................................................\n",
      "b011: 10.676\n",
      "b012: 10.954\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Model01\n",
    "    prev-err: 10.676\n",
    "    trying out normalized data (just x)\n",
    "    increasing regularazation terms\n",
    "    10 times the initial-rate\n",
    "\"\"\"\n",
    "history = model.fit(norm_X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.20,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:601.4512,  val_loss:514.6172,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:149.7161,  val_loss:149.9133,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:1.0519,  val_loss:1.0579,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.3575,  val_loss:0.3645,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.1690,  val_loss:0.1771,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0993,  val_loss:0.1076,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0827,  val_loss:0.0905,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0755,  val_loss:0.0833,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0707,  val_loss:0.0788,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0677,  val_loss:0.0757,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.0649,  val_loss:0.0734,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.0627,  val_loss:0.0708,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.0599,  val_loss:0.0691,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.0592,  val_loss:0.0672,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0578,  val_loss:0.0664,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.0571,  val_loss:0.0651,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.0557,  val_loss:0.0646,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.0555,  val_loss:0.0633,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.0547,  val_loss:0.0629,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.0539,  val_loss:0.0619,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.0533,  val_loss:0.0614,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.0527,  val_loss:0.0609,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.0521,  val_loss:0.0605,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.0522,  val_loss:0.0604,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.0515,  val_loss:0.0597,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.0513,  val_loss:0.0593,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.0510,  val_loss:0.0590,  \n",
      ".....................................................................\n",
      "b011: 11.649\n",
      "b012: 12.532\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Model01\n",
    "    prev-err: 10.676\n",
    "    trying out normalized data (just x)\n",
    "    increasing regularazation term\n",
    "    epochs: 3000, early_stop: 35\n",
    "\"\"\"\n",
    "history = model.fit(norm_X, y, epochs=3000,\n",
    "          verbose=0, validation_split=0.20,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAEs:\n",
      "b011: 11.649\n",
      "b012: 12.532\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n",
      "MSLE:\n",
      "b011: 0\n",
      "b012: 0\n",
      "-----------------------------------\n",
      "base-differences: 0\n"
     ]
    }
   ],
   "source": [
    "pred_y = quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])\n",
    "\n",
    "validate(quantize(pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(y_pred):\n",
    "    \"\"\" Prints out the data validation with respect to the highest submissions. \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error as MAE\n",
    "    from sklearn.metrics import mean_squared_log_error as MSLE\n",
    "    # Import the base_validation submititions\n",
    "    b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "    b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']\n",
    "    \n",
    "    # Print out the differences\n",
    "    print('MAEs:')\n",
    "    print('b011:', int(MAE(b011, y_pred)) / 1000)\n",
    "    print('b012:', int(MAE(b012, y_pred)) / 1000)\n",
    "    print('-----------------------------------')\n",
    "    print('base-differences:', int(MAE(b011, b012)) / 1000)\n",
    "    # \n",
    "    print('MSLE:')\n",
    "    print('b011:', int(MSLE(b011, y_pred)))\n",
    "    print('b012:', int(MSLE(b012, y_pred)))\n",
    "    print('-----------------------------------')\n",
    "    print('base-differences:', int(MSLE(b011, b012)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:4.6275,  \n",
      "................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-f32dca786989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Fit on whole model then predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(norm_X, y, epochs=4000,\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           callbacks=[train_stop, tfdocs.modeling.EpochDots()])\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_stop = EarlyStopping(monitor='loss', patience=5, mode='min', restore_best_weights=True)\n",
    "# Fit on whole model then predict\n",
    "history = model.fit(norm_X, y, epochs=4000,\n",
    "          verbose=0,\n",
    "          callbacks=[train_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "predictions = quantize(pd.DataFrame(model.predict(X_test, verbose=0))[0])\n",
    "# See how accurate it is\n",
    "validate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
