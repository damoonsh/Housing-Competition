{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Check all the unique categorical features in both training and testing datasets to see which features are missing in test dataset, This part might not be necessary but could be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "train_num = data['train_num']\n",
    "y_feat = 'SalePrice'\n",
    "\n",
    "# Generates a dictionary of values corresponding to the \n",
    "# categorical features within the dataset\n",
    "cat_dics = {}\n",
    "cat_feats = train.select_dtypes('object').columns.to_list()\n",
    "\n",
    "for feat in cat_feats:\n",
    "    cat_dics[feat] = rank_categorical_values(train, feat)[0]\n",
    "\n",
    "# There might be some missing values in the categorical features in the\n",
    "# testing data, which will be treated as numerical and imputed with that\n",
    "# respect.\n",
    "## Note: mappings should be done after combining datasets\n",
    "\n",
    "# Get the length of dataset so I can rebreak them after combining\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Get the column for the dependent data into a seprate variable\n",
    "dep_col = train[y_feat]\n",
    "\n",
    "# Drop the dependent column in train\n",
    "train.drop([y_feat], axis=1 , inplace = True)\n",
    "feat_cols = train.append(test) # Combine datasets\n",
    "feat_cols.reset_index(inplace=True) # Reset Indexes\n",
    "feat_cols.drop(['index', 'Id'], inplace=True, axis=1) # Drop Id and index columns\n",
    "\n",
    "# In order to impute and decode data, first it is needed \n",
    "# to break it into categorical and numerical datasets\n",
    "feat_cols_cat = feat_cols.select_dtypes('object').columns.to_list()\n",
    "feat_cols_num = feat_cols.select_dtypes(['float64', 'int64']).columns.to_list()\n",
    "\n",
    "# Decode the categorical features in the combined dataset\n",
    "for feat in cat_feats:\n",
    "    feat_cols[feat] = impute_rank_weight(feat_cols[feat].copy(), cat_dics[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CollgCr': 0.04304425976649378,\n",
       " 'Veenker': 0.05191703153946601,\n",
       " 'Crawfor': 0.04579673165007603,\n",
       " 'NoRidge': 0.07290421209470918,\n",
       " 'Mitchel': 0.03397825609535837,\n",
       " 'Somerst': 0.049004977454523396,\n",
       " 'NWAmes': 0.04110569276735894,\n",
       " 'OldTown': 0.027880390973622476,\n",
       " 'BrkSide': 0.027143022046897163,\n",
       " 'Sawyer': 0.029743319483390717,\n",
       " 'NridgHt': 0.06876761896720276,\n",
       " 'NAmes': 0.031711944403308295,\n",
       " 'SawyerW': 0.04056335615506725,\n",
       " 'IDOTRR': 0.021770198380387268,\n",
       " 'MeadowV': 0.021433761682225477,\n",
       " 'Edwards': 0.027879173157315654,\n",
       " 'Timber': 0.052672549788386036,\n",
       " 'Gilbert': 0.041932902480024487,\n",
       " 'StoneBr': 0.06751267852111145,\n",
       " 'ClearCr': 0.04621870422721212,\n",
       " 'NPkVill': 0.031026457909772254,\n",
       " 'Blmngtn': 0.04237132883976904,\n",
       " 'BrDale': 0.02272037253329444,\n",
       " 'SWISU': 0.0310040439665444,\n",
       " 'Blueste': 0.029897015116482902}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dics['Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing categorical variables with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "missings = feat_cols.columns[feat_cols.isna().any()].tolist()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=300, weights=\"distance\")\n",
    "feat_cols[missings] = pd.DataFrame(imputer.fit_transform(feat_cols[missings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmn/.local/lib/python3.8/site-packages/pandas/core/indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/dmn/.local/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Now rebreak the data into train and test\n",
    "imp_train = feat_cols.iloc[:train_len]\n",
    "imp_train.loc[:, (y_feat)] = dep_col\n",
    "imp_test = feat_cols[train_len:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_train.corr()[y_feat].nlargest(15)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SalePrice',\n",
       " 'OverallQual',\n",
       " 'Neighborhood',\n",
       " 'GrLivArea',\n",
       " 'ExterQual',\n",
       " 'BsmtQual',\n",
       " 'KitchenQual',\n",
       " 'GarageCars',\n",
       " 'GarageArea',\n",
       " 'TotalBsmtSF',\n",
       " '1stFlrSF',\n",
       " 'FullBath',\n",
       " 'GarageFinish',\n",
       " 'FireplaceQu',\n",
       " 'TotRmsAbvGrd',\n",
       " 'YearBuilt',\n",
       " 'YearRemodAdd',\n",
       " 'Foundation',\n",
       " 'GarageYrBlt',\n",
       " 'GarageType',\n",
       " 'MasVnrArea',\n",
       " 'Fireplaces',\n",
       " 'BsmtFinType1',\n",
       " 'HeatingQC',\n",
       " 'MasVnrType',\n",
       " 'Exterior2nd',\n",
       " 'Exterior1st',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinSF1',\n",
       " 'SaleType',\n",
       " 'SaleCondition',\n",
       " 'LotFrontage',\n",
       " 'MSZoning',\n",
       " 'WoodDeckSF',\n",
       " '2ndFlrSF',\n",
       " 'OpenPorchSF',\n",
       " 'HouseStyle',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'HalfBath',\n",
       " 'LotShape',\n",
       " 'LotArea',\n",
       " 'CentralAir',\n",
       " 'Electrical',\n",
       " 'RoofStyle',\n",
       " 'PavedDrive',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtCond',\n",
       " 'BsmtUnfSF',\n",
       " 'Fence',\n",
       " 'BldgType',\n",
       " 'Condition1',\n",
       " 'RoofMatl',\n",
       " 'BsmtFinType2',\n",
       " 'BedroomAbvGr',\n",
       " 'LandContour',\n",
       " 'ExterCond',\n",
       " 'PoolQC',\n",
       " 'LotConfig',\n",
       " 'Alley',\n",
       " 'Functional',\n",
       " 'Heating',\n",
       " 'ScreenPorch',\n",
       " 'Condition2',\n",
       " 'PoolArea',\n",
       " 'MiscFeature',\n",
       " 'LandSlope',\n",
       " 'MoSold',\n",
       " '3SsnPorch',\n",
       " 'Street',\n",
       " 'Utilities',\n",
       " 'BsmtFinSF2',\n",
       " 'BsmtHalfBath',\n",
       " 'MiscVal',\n",
       " 'LowQualFinSF',\n",
       " 'YrSold',\n",
       " 'OverallCond',\n",
       " 'MSSubClass',\n",
       " 'EnclosedPorch',\n",
       " 'KitchenAbvGr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "imp_train.corr()[y_feat].nlargest(100).keys().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the x and y splits:\n",
    "# Finding the features\n",
    "features = imp_train.corr()[y_feat].nlargest(100)[1:].keys().to_list()\n",
    "\n",
    "# Training datasets\n",
    "X = imp_train[features]\n",
    "y = imp_train[y_feat]\n",
    "\n",
    "# It makes more sense to use batchnormalization in NN instead\n",
    "# of feeding normalized data into the model.\n",
    "norm_X = normalize(X)\n",
    "norm_y = normalize(y)\n",
    "\n",
    "# Testing datasets\n",
    "X_test = imp_test[features]\n",
    "norm_X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting\n",
    "devs = []\n",
    "dev_batch_size = int(imp_train.shape[0] * 0.3)\n",
    "\n",
    "for i in range(10):\n",
    "    dev_data = imp_train.sample(n=438, random_state=i)\n",
    "    dev_x = dev_data[features]\n",
    "    dev_y = dev_data[y_feat]\n",
    "    devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if the imputation worked\n",
    "True in X.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    1.0\n",
       "11    3.0\n",
       "12    1.0\n",
       "13    3.0\n",
       "14    1.0\n",
       "15    2.0\n",
       "16    2.0\n",
       "17    2.0\n",
       "18    2.0\n",
       "19    1.0\n",
       "Name: GarageCars, dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['GarageCars'][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax does not make sense, drop out and batchnormalization works\n",
    "# For metrics, mse and msle should be considered\n",
    "# The only place to use the BatchNormalization layer is the \n",
    "def build_model05():\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=[len(X.keys())]),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(64),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(256),\n",
    "        \n",
    "        layers.Dense(2048, kernel_regularizer=regularizers.l2(0.1)),\n",
    "        layers.Dropout(0.70),\n",
    "        layers.Dense(256, activation = 'elu', kernel_regularizer=regularizers.l1_l2(0.001, 0.001)),\n",
    "        layers.Dense(256, activation = 'elu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        \n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'relu'),\n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "      0.0025,\n",
    "      decay_steps=1460 // 5,\n",
    "      decay_rate=1.2,\n",
    "      staircase=False\n",
    "    )\n",
    "    \n",
    "    exp_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate = 0.0025, \n",
    "        decay_steps=1460 // 5, \n",
    "        decay_rate=0.5, \n",
    "        staircase=False, name=None\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(exp_lr)\n",
    "        \n",
    "    model.compile(\n",
    "                loss=losses.MeanSquaredLogarithmicError(name='MSLE'), \n",
    "                optimizer=optimizer, \n",
    "                metrics=[metrics.MeanSquaredLogarithmicError(name='msle')]\n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model05()\n",
    "\n",
    "def validate():\n",
    "    # Check to see if there have been an overfit or underfit\n",
    "    for i in range(10):\n",
    "        model.evaluate(devs[i][0], devs[i][1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2500\n",
    "batch_size = 1460 // 20\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:190.1047,  msle:124.5566,  val_loss:181.6721,  val_msle:117.6487,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:13.6205,  msle:11.2187,  val_loss:14.4337,  val_msle:12.0475,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:5.3139,  msle:4.4443,  val_loss:5.2251,  val_msle:4.3574,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:2.9869,  msle:2.3947,  val_loss:3.0826,  val_msle:2.4919,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:1.8248,  msle:1.3620,  val_loss:1.8477,  val_msle:1.3857,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:1.3172,  msle:0.8993,  val_loss:1.2426,  val_msle:0.8252,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:1.0059,  msle:0.6456,  val_loss:0.9500,  val_msle:0.5901,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.7802,  msle:0.4412,  val_loss:0.9043,  val_msle:0.5616,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.6627,  msle:0.3559,  val_loss:0.6244,  val_msle:0.3177,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.4872,  msle:0.2066,  val_loss:0.5885,  val_msle:0.3082,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.4699,  msle:0.2118,  val_loss:0.4539,  val_msle:0.1959,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.3990,  msle:0.1545,  val_loss:0.4742,  val_msle:0.2298,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.3958,  msle:0.1723,  val_loss:0.3771,  val_msle:0.1534,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.3847,  msle:0.1723,  val_loss:0.3386,  val_msle:0.1263,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.3384,  msle:0.1424,  val_loss:0.3301,  val_msle:0.1343,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.3660,  msle:0.1784,  val_loss:0.2877,  val_msle:0.0999,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.2875,  msle:0.1188,  val_loss:0.2983,  val_msle:0.1294,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.2556,  msle:0.0984,  val_loss:0.2684,  val_msle:0.1110,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.2449,  msle:0.0958,  val_loss:0.2682,  val_msle:0.1193,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.2203,  msle:0.0831,  val_loss:0.2099,  val_msle:0.0728,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.2200,  msle:0.0883,  val_loss:0.1989,  val_msle:0.0673,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.2152,  msle:0.0927,  val_loss:0.2106,  val_msle:0.0884,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.2006,  msle:0.0847,  val_loss:0.1950,  val_msle:0.0793,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.1696,  msle:0.0606,  val_loss:0.1772,  val_msle:0.0683,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.1858,  msle:0.0801,  val_loss:0.1797,  val_msle:0.0740,  \n",
      "........................------------------------------------------------------------------------\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1651 - msle: 0.0605\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1788 - msle: 0.0743\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.1738 - msle: 0.0692\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1651 - msle: 0.0606\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1665 - msle: 0.0619\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1618 - msle: 0.0572\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1738 - msle: 0.0692\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1691 - msle: 0.0645\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1614 - msle: 0.0569\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1671 - msle: 0.0625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0], steps_per_epoch=3,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print('------------------------------------------------------------------------')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:190.9156,  msle:125.4110,  val_loss:129.1139,  val_msle:64.8590,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:5.4127,  msle:0.6525,  val_loss:6.0034,  val_msle:1.2683,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:2.6480,  msle:0.3259,  val_loss:2.9844,  val_msle:0.6668,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:1.8969,  msle:0.2542,  val_loss:2.0800,  val_msle:0.4403,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:1.3533,  msle:0.1859,  val_loss:1.4236,  val_msle:0.2594,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:1.0871,  msle:0.1900,  val_loss:1.0133,  val_msle:0.1182,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.8649,  msle:0.1438,  val_loss:0.8379,  val_msle:0.1176,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.7276,  msle:0.1288,  val_loss:0.7052,  val_msle:0.1068,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.6860,  msle:0.1637,  val_loss:0.6028,  val_msle:0.0758,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.5768,  msle:0.1080,  val_loss:0.6696,  val_msle:0.2042,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.5572,  msle:0.1345,  val_loss:0.4761,  val_msle:0.0530,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.5111,  msle:0.1268,  val_loss:0.5749,  val_msle:0.1899,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.4646,  msle:0.1151,  val_loss:0.4014,  val_msle:0.0533,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.4231,  msle:0.0947,  val_loss:0.3756,  val_msle:0.0472,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.4031,  msle:0.1005,  val_loss:0.3478,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.3695,  msle:0.0890,  val_loss:0.3235,  val_msle:0.0433,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.3410,  msle:0.0783,  val_loss:0.3351,  val_msle:0.0725,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.3499,  msle:0.1051,  val_loss:0.3173,  val_msle:0.0725,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.3204,  msle:0.0922,  val_loss:0.2748,  val_msle:0.0461,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.2973,  msle:0.0840,  val_loss:0.3025,  val_msle:0.0889,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.2957,  msle:0.0927,  val_loss:0.2903,  val_msle:0.0873,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.2679,  msle:0.0771,  val_loss:0.2344,  val_msle:0.0438,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.2576,  msle:0.0792,  val_loss:0.2173,  val_msle:0.0387,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.2567,  msle:0.0879,  val_loss:0.2059,  val_msle:0.0372,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.2393,  msle:0.0823,  val_loss:0.1919,  val_msle:0.0350,  \n",
      "....................................................................................................------------------------------------------------------------------------\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1874 - msle: 0.0409\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.2020 - msle: 0.0555\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1907 - msle: 0.0443\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1878 - msle: 0.0414\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 0.1880 - msle: 0.0416\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1904 - msle: 0.0439\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1894 - msle: 0.0429\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1892 - msle: 0.0428\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1825 - msle: 0.0360\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.1926 - msle: 0.0461\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0], steps_per_epoch=3,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print('------------------------------------------------------------------------')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 15\n",
    "den = (0.12008 ** exp + 0.11978 ** exp)\n",
    "\n",
    "w012 = 1 - 0.12008 ** exp / den\n",
    "w011 = 1 - 0.11978 ** exp / den\n",
    "\n",
    "pred_y = b012 * w012 + b011 * w011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.2186,  msle:0.0720,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.1973,  msle:0.0642,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.1753,  msle:0.0534,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.1611,  msle:0.0499,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.1471,  msle:0.0454,  \n",
      "...................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[123193, 146664, 175908, 174839, 156534]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit it to all of the data\n",
    "model.fit(X, y, \n",
    "          epochs=500, steps_per_epoch=5, \n",
    "          verbose=0, callbacks=[tfdocs.modeling.EpochDots()]\n",
    "         )\n",
    "\n",
    "pred_y = pd.DataFrame(model.predict(X_test, batch_size=20, steps=73, verbose=0))[0]\n",
    "# It would make sense to convert all of the data to int \n",
    "# instead of float since there no floats in trainig.\n",
    "modified = quantize(pred_y)\n",
    "\n",
    "modified[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b011: 21.68\n",
      "b012: 22.369\n",
      "-----------------------------------\n",
      "b: 5.76\n"
     ]
    }
   ],
   "source": [
    "# val_loss of 0.0197 is close\n",
    "print('b011:', int(MAE(b011, modified)) / 1000)\n",
    "print('b012:', int(MAE(b012, modified)) / 1000)\n",
    "print('-----------------------------------')\n",
    "print('b:', int(MAE(b011, b012)) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': modified})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
