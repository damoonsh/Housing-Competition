{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "# The dependent feature\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "The general strategy is to combine both the categorical and numerical values in the training and testing and then process them at the same time. For categorical variables we will be getting dictionaries from the training data and then process them for the combined dataframe. In the case of numerical features, imputation is going to be done using the KNN imputation.\n",
    "\n",
    "1. Encoding categorical features: I'll be using some functions written in utils.py to come up with meaning values for the unique keys in each of the categorical features, then map them in the data given certain conditions.\n",
    "2. Imputing numerical data: The numerical \n",
    "\n",
    "## In-depth analysis of categorical variables:\n",
    "1. Compare the different NaNs for the same categories (and not) in the number of NaNs they have.\n",
    "2. Given that 90% data is not missing for a given feature (column) map their encoded numerical values in the dataframe, otherwise, only impute non-nan values in the feature and then impute the rest of the missing values using any other technique. Dropping the column for values with too many missing might be a general option but in order to use the data for Nerual Networks, it would make sense to just impute the missing values with zeros.\n",
    "\n",
    "## Encoding categorical variables:\n",
    "In order to come up with a meaningful value for any given unique value in a categorical feature column, we will be considering the average SalePrice for each of those unique values and weight them relative to each other. The important thing to note would be that given that more than 90% exists in a column we could just impute the minor missing values with the average of SalePrice for those columns. But if less then 90% of the data existed then there would be a problem since our measures would not make sense and since we are using Neural Networks it would make more sense to impute them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_info(data):\n",
    "    \"\"\" retuns two dataframes (train, test) defining their relative missing values. \"\"\"\n",
    "    # test data:\n",
    "    cat_dict = {\n",
    "        \"Test\": dict(data['test'][data['test_cat_missing']].isna().sum()), \n",
    "        \"Train\": dict(data['train'][data['train_cat_missing']].isna().sum())\n",
    "    }\n",
    "\n",
    "    # train data:\n",
    "    num_dict = {\n",
    "        \"Test\": dict(data['train'][data['train_num_missing']].isna().sum()), \n",
    "        \"Train\": dict(data['test'][data['test_num_missing']].isna().sum())\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(cat_dict).fillna(0), pd.DataFrame(num_dict).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFrames\n",
    "cat_info, num_info = missing_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              Test  Train\nAlley         1352   1369\nMasVnrType      16      8\nBsmtQual        44     37\nBsmtCond        45     37\nBsmtExposure    44     38\nBsmtFinType1    42     37\nBsmtFinType2    42     38\nElectrical       0      1\nFireplaceQu    730    690\nGarageType      76     81\nGarageFinish    78     81\nGarageQual      78     81\nGarageCond      78     81\nPoolQC        1456   1453\nFence         1169   1179\nMiscFeature   1408   1406",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Alley</th>\n      <td>1352</td>\n      <td>1369</td>\n    </tr>\n    <tr>\n      <th>MasVnrType</th>\n      <td>16</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>BsmtQual</th>\n      <td>44</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>BsmtCond</th>\n      <td>45</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>BsmtExposure</th>\n      <td>44</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>BsmtFinType1</th>\n      <td>42</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>BsmtFinType2</th>\n      <td>42</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>Electrical</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>FireplaceQu</th>\n      <td>730</td>\n      <td>690</td>\n    </tr>\n    <tr>\n      <th>GarageType</th>\n      <td>76</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>GarageFinish</th>\n      <td>78</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>GarageQual</th>\n      <td>78</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>GarageCond</th>\n      <td>78</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>PoolQC</th>\n      <td>1456</td>\n      <td>1453</td>\n    </tr>\n    <tr>\n      <th>Fence</th>\n      <td>1169</td>\n      <td>1179</td>\n    </tr>\n    <tr>\n      <th>MiscFeature</th>\n      <td>1408</td>\n      <td>1406</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    # Put this in a markdown cell\n",
    "    1. Alley, PoolQc, Fence, MiscFeature are the features with an ecessive number of missing values both in training and testing.\n",
    "    2. FireplaceQu is not as bad ass the described functions but it is going to be treated the same way.\n",
    "\n",
    "    Note: To conclude there are 5 features that the np.nan values in them should not be imputed with their given dictionary value but a zero.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               Test  Train\nLotFrontage   259.0    227\nMasVnrArea      8.0     15\nGarageYrBlt    81.0     78\nBsmtFinSF1      0.0      1\nBsmtFinSF2      0.0      1\nBsmtUnfSF       0.0      1\nTotalBsmtSF     0.0      1\nBsmtFullBath    0.0      2\nBsmtHalfBath    0.0      2\nGarageCars      0.0      1\nGarageArea      0.0      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LotFrontage</th>\n      <td>259.0</td>\n      <td>227</td>\n    </tr>\n    <tr>\n      <th>MasVnrArea</th>\n      <td>8.0</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>GarageYrBlt</th>\n      <td>81.0</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>BsmtFinSF1</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>BsmtFinSF2</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>BsmtUnfSF</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>TotalBsmtSF</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>BsmtFullBath</th>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>BsmtHalfBath</th>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>GarageCars</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>GarageArea</th>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "num_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    #  Put this in Mark down cell:\n",
    "    Based on this dataframe, there some features missing in Training that are not missing in the test data. There is no need manually impute anything in the case of numerical values and I am just going to let KNN handle it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_train_test(train, test, y_feat='SalePrice'):\n",
    "    train.drop([y_feat], axis=1 , inplace = True) # Drop the dependent column in traininig data\n",
    "    feat_cols = train.append(test) # Combine datasets\n",
    "    feat_cols.reset_index(inplace=True) # Reset Indexes\n",
    "    feat_cols.drop(['index', 'Id'], inplace=True, axis=1) # Drop Id and index columns\n",
    "\n",
    "    return feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of training data to rebreak the combined data further along the way\n",
    "train_len = train.shape[0]\n",
    "\n",
    "# Combine the train and test \n",
    "feat_cols = combine_train_test(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0             60       RL         65.0     8450   Pave   NaN      Reg   \n1             20       RL         80.0     9600   Pave   NaN      Reg   \n2             60       RL         68.0    11250   Pave   NaN      IR1   \n3             70       RL         60.0     9550   Pave   NaN      IR1   \n4             60       RL         84.0    14260   Pave   NaN      IR1   \n...          ...      ...          ...      ...    ...   ...      ...   \n2914         160       RM         21.0     1936   Pave   NaN      Reg   \n2915         160       RM         21.0     1894   Pave   NaN      Reg   \n2916          20       RL        160.0    20000   Pave   NaN      Reg   \n2917          85       RL         62.0    10441   Pave   NaN      Reg   \n2918          60       RL         74.0     9627   Pave   NaN      Reg   \n\n     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n0            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n1            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n2            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n3            Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n4            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n...          ...       ...       ...  ...         ...      ...    ...    ...   \n2914         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n2915         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n2916         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n2917         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n2918         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n\n     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n0            NaN       0       2    2008        WD         Normal  \n1            NaN       0       5    2007        WD         Normal  \n2            NaN       0       9    2008        WD         Normal  \n3            NaN       0       2    2006        WD        Abnorml  \n4            NaN       0      12    2008        WD         Normal  \n...          ...     ...     ...     ...       ...            ...  \n2914         NaN       0       6    2006        WD         Normal  \n2915         NaN       0       4    2006        WD        Abnorml  \n2916         NaN       0       9    2006        WD        Abnorml  \n2917        Shed     700       7    2006        WD         Normal  \n2918         NaN       0      11    2006        WD         Normal  \n\n[2919 rows x 79 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2914</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1936</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2915</th>\n      <td>160</td>\n      <td>RM</td>\n      <td>21.0</td>\n      <td>1894</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>2916</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>160.0</td>\n      <td>20000</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>2917</th>\n      <td>85</td>\n      <td>RL</td>\n      <td>62.0</td>\n      <td>10441</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>Shed</td>\n      <td>700</td>\n      <td>7</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2918</th>\n      <td>60</td>\n      <td>RL</td>\n      <td>74.0</td>\n      <td>9627</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>11</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>2919 rows Ã— 79 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "missings = feat_cols.columns[feat_cols.isna().any()].tolist()\n",
    "# The number of neighbors that the function look for is the 1/3 of the whole dataframe\n",
    "num = (train_len + test_len) // 3\n",
    "\n",
    "# Imputer object\n",
    "imputer = KNNImputer(n_neighbors=num, weights=\"distance\")\n",
    "# Get the new imputed data\n",
    "feat_cols[missings] = pd.DataFrame(imputer.fit_transform(feat_cols[missings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rebreak the data into train and test\n",
    "imp_train = feat_cols[: train_len]\n",
    "# test-data\n",
    "imp_test = feat_cols[train_len:].reset_index()\n",
    "imp_test.drop(['index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dep_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "YearBuilt       0.522897\n",
       "YearRemodAdd    0.507101\n",
       "Foundation      0.506328\n",
       "GarageYrBlt     0.506210\n",
       "GarageType      0.499204\n",
       "MasVnrArea      0.477596\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "imp_train.corr()[y_feat].nlargest(21)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check nans in X: False\n",
      "Check nans in y: False\n"
     ]
    }
   ],
   "source": [
    "# Breaking the x and y splits:\n",
    "# Finding the features\n",
    "# features = imp_train.corr()[y_feat].nlargest(21)[1:].keys().to_list()\n",
    "\n",
    "# Training datasets\n",
    "X = imp_train\n",
    "y = dep_col\n",
    "\n",
    "# It makes more sense to use batchnormalization in NN instead\n",
    "# of feeding normalized data into the model.\n",
    "norm_X = normalize(X)\n",
    "norm_y = normalize(y)\n",
    "\n",
    "# Testing datasets\n",
    "X_test = imp_test\n",
    "norm_X_test = normalize(X_test)\n",
    "\n",
    "# Check to see if the imputation worked\n",
    "print('Check nans in X:', True in X.isna().any())\n",
    "print('Check nans in y:', True in dict(y.isna()).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting\n",
    "# devs = []\n",
    "# dev_batch_size = int(imp_train.shape[0] * 0.3)\n",
    "\n",
    "# for i in range(10):\n",
    "#     dev_data = imp_train.sample(n=438, random_state=i)\n",
    "#     dev_x = dev_data[features]\n",
    "#     dev_y = dev_data[y_feat]\n",
    "#     devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax does not make sense, drop out and batchnormalization works\n",
    "# For metrics, mse and msle should be considered\n",
    "# The only place to use the BatchNormalization layer is at the beginning\n",
    "\n",
    "#\n",
    "def build_model05():\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        \n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        \n",
    "        layers.Dense(1024),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'relu'),\n",
    "        \n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'relu'),\n",
    "        \n",
    "        Dense(4),\n",
    "        Dense(4, kernel_regularizer=regularizers.l1_l2(0.001, 0.01)),\n",
    "        Dense(4),\n",
    "        \n",
    "        layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "      0.0025,\n",
    "      decay_steps=1460 // 5,\n",
    "      decay_rate=1.2,\n",
    "      staircase=False\n",
    "    )\n",
    "    \n",
    "    exp_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = 0.25, \n",
    "        decay_steps=1460 // 20, \n",
    "        decay_rate=0.02,\n",
    "        staircase=False, name=None\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "                loss=losses.MeanSquaredLogarithmicError(name='MSLE'), \n",
    "                optimizer=optimizer, \n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model05()\n",
    "\n",
    "def validate():\n",
    "    # Check to see if there have been an overfit or underfit\n",
    "    for i in range(10):\n",
    "        model.evaluate(devs[i][0], devs[i][1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3500\n",
    "batch_size = 1460 // 20\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.6405,  val_loss:0.6394,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.2473,  val_loss:0.2510,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.1323,  val_loss:0.1372,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.0959,  val_loss:0.1013,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.0832,  val_loss:0.0889,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0780,  val_loss:0.0834,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0743,  val_loss:0.0800,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0719,  val_loss:0.0778,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0707,  val_loss:0.0762,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0675,  val_loss:0.0742,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.0669,  val_loss:0.0739,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.0653,  val_loss:0.0719,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.0647,  val_loss:0.0710,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.0641,  val_loss:0.0701,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0633,  val_loss:0.0695,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.0629,  val_loss:0.0693,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.0609,  val_loss:0.0683,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.0608,  val_loss:0.0679,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.0611,  val_loss:0.0676,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.0605,  val_loss:0.0670,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.0596,  val_loss:0.0667,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.0590,  val_loss:0.0663,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.0592,  val_loss:0.0662,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.0584,  val_loss:0.0656,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.0582,  val_loss:0.0655,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.0583,  val_loss:0.0652,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.0570,  val_loss:0.0649,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, loss:0.0582,  val_loss:0.0646,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, loss:0.0569,  val_loss:0.0644,  \n",
      "....................................................................................................\n",
      "Epoch: 2900, loss:0.0569,  val_loss:0.0646,  \n",
      "....................................................................................................\n",
      "Epoch: 3000, loss:0.0562,  val_loss:0.0641,  \n",
      "....................................................................................................\n",
      "Epoch: 3100, loss:0.0563,  val_loss:0.0639,  \n",
      "....................................................................................................\n",
      "Epoch: 3200, loss:0.0570,  val_loss:0.0637,  \n",
      "....................................................................................................\n",
      "Epoch: 3300, loss:0.0564,  val_loss:0.0635,  \n",
      "....................................................................................................\n",
      "Epoch: 3400, loss:0.0555,  val_loss:0.0633,  \n",
      "....................................................................................................------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.33,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "# print('------------------------------------------------------------------------')\n",
    "# validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 15\n",
    "den = (0.12008 ** exp + 0.11978 ** exp)\n",
    "\n",
    "w012 = 1 - 0.12008 ** exp / den\n",
    "w011 = 1 - 0.11978 ** exp / den\n",
    "\n",
    "pred_y = b012 * w012 + b011 * w011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[142684, 179834, 193854, 190913, 173967]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A very low patience rate for the \n",
    "# train_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# # Fit it to all of the data\n",
    "# model.fit(X, y, \n",
    "#           epochs=1500, steps_per_epoch=5, validation_split=0.3,\n",
    "#           verbose=0, callbacks=[tfdocs.modeling.EpochDots(), train_stop]\n",
    "#          )\n",
    "\n",
    "pred_y = pd.DataFrame(model.predict(X_test, batch_size=20, steps=73, verbose=0))[0]\n",
    "# It would make sense to convert all of the data to int \n",
    "# instead of float since there no floats in trainig.\n",
    "modified = quantize(pred_y)\n",
    "\n",
    "modified[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b011: 20.887\n",
      "b012: 21.302\n",
      "-----------------------------------\n",
      "b: 5.76\n"
     ]
    }
   ],
   "source": [
    "# val_loss of 0.0197 is close\n",
    "print('b011:', int(MAE(b011, modified)) / 1000)\n",
    "print('b012:', int(MAE(b012, modified)) / 1000)\n",
    "print('-----------------------------------')\n",
    "print('b:', int(MAE(b011, b012)) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': modified})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}