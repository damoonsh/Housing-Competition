{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "\n",
    "# The dependent feature\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Preprocessing:\n",
    "The general strategy is to combine both the categorical and numerical values in the training and testing and then process them at the same time. For categorical variables we will be getting dictionaries from the training data and then process them for the combined dataframe. In the case of numerical features, imputation is going to be done using the KNN imputation.\n",
    "\n",
    "1. Encoding categorical features: I'll be using some functions written in utils.py to come up with meaning values for the unique keys in each of the categorical features, then map them in the data given certain conditions.\n",
    "2. Imputing numerical data: The numerical \n",
    "\n",
    "## In-depth analysis of categorical variables:\n",
    "1. Compare the different NaNs for the same categories (and not) in the number of NaNs they have.\n",
    "2. Given that 90% data is not missing for a given feature (column) map their encoded numerical values in the dataframe, otherwise, only impute non-nan values in the feature and then impute the rest of the missing values using any other technique. Dropping the column for values with too many missing might be a general option but in order to use the data for Nerual Networks, it would make sense to just impute the missing values with zeros.\n",
    "\n",
    "## Encoding categorical variables:\n",
    "In order to come up with a meaningful value for any given unique value in a categorical feature column, we will be considering the average SalePrice for each of those unique values and weight them relative to each other. The important thing to note would be that given that more than 90% exists in a column we could just impute the minor missing values with the average of SalePrice for those columns. But if less then 90% of the data existed then there would be a problem since our measures would not make sense and since we are using Neural Networks it would make more sense to impute them with zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def missing_info(data):\n",
    "#     \"\"\" retuns two dataframes (train, test) defining their relative missing values. \"\"\"\n",
    "#     # test data:\n",
    "#     cat_dict = {\n",
    "#         \"Test\": dict(data['test'][data['test_cat_missing']].isna().sum()), \n",
    "#         \"Train\": dict(data['train'][data['train_cat_missing']].isna().sum())\n",
    "#     }\n",
    "\n",
    "#     # train data:\n",
    "#     num_dict = {\n",
    "#         \"Test\": dict(data['train'][data['train_num_missing']].isna().sum()), \n",
    "#         \"Train\": dict(data['test'][data['test_num_missing']].isna().sum())\n",
    "#     }\n",
    "\n",
    "#     return pd.DataFrame(cat_dict).fillna(0), pd.DataFrame(num_dict).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e649e0d5d7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcat_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_info' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the DataFrames\n",
    "cat_info, num_info = missing_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e97cd9b9b0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cat_info' is not defined"
     ]
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important to note for categorical features:\n",
    "1. Alley, PoolQc, Fence, MiscFeature are the features with an ecessive number of missing values both in training and testing.\n",
    "2. FireplaceQu is not as bad ass the described functions but it is going to be treated the same way.\n",
    "3. Although for some these values NA means that they just don't have that feature: Alley, MiscFeature, PoolQc\n",
    "\n",
    "Note: To conclude there are 5 features that the np.nan values in them should not be imputed with their given dictionary value but a zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test  Train\n",
       "LotFrontage   259.0    227\n",
       "MasVnrArea      8.0     15\n",
       "GarageYrBlt    81.0     78\n",
       "BsmtFinSF1      0.0      1\n",
       "BsmtFinSF2      0.0      1\n",
       "BsmtUnfSF       0.0      1\n",
       "TotalBsmtSF     0.0      1\n",
       "BsmtFullBath    0.0      2\n",
       "BsmtHalfBath    0.0      2\n",
       "GarageCars      0.0      1\n",
       "GarageArea      0.0      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data:\n",
    "Based on this dataframe, there some features missing in Training that are not missing in the test data. There is no need manually impute anything in the case of numerical values and I am just going to let KNN handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_train_test(train, test, y_feat='SalePrice'):\n",
    "#     \"\"\" Returns a combined version of the train and test datasets. \"\"\"\n",
    "#     train.drop([y_feat], axis=1 , inplace = True) # Drop the dependent column in traininig data\n",
    "#     feat_cols = train.append(test) # Combine datasets\n",
    "#     feat_cols.reset_index(inplace=True) # Reset Indexes\n",
    "#     feat_cols.drop(['index', 'Id'], inplace=True, axis=1) # Drop Id and index columns\n",
    "\n",
    "#     return feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of training data to rebreak the combined data further along the way\n",
    "train_len = train.shape[0]\n",
    "\n",
    "# Combine the train and test:\n",
    "# Note: Pass the copies so the actual dataframes won't change and we can still use them\n",
    "feat_cols = combine_train_test(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "2914         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "2915         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "2916          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "2917          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "2918          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "1            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "2            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "3            Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n",
       "4            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "...          ...       ...       ...  ...         ...      ...    ...    ...   \n",
       "2914         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2915         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2916         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2917         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n",
       "2918         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0            NaN       0       2    2008        WD         Normal  \n",
       "1            NaN       0       5    2007        WD         Normal  \n",
       "2            NaN       0       9    2008        WD         Normal  \n",
       "3            NaN       0       2    2006        WD        Abnorml  \n",
       "4            NaN       0      12    2008        WD         Normal  \n",
       "...          ...     ...     ...     ...       ...            ...  \n",
       "2914         NaN       0       6    2006        WD         Normal  \n",
       "2915         NaN       0       4    2006        WD        Abnorml  \n",
       "2916         NaN       0       9    2006        WD        Abnorml  \n",
       "2917        Shed     700       7    2006        WD         Normal  \n",
       "2918         NaN       0      11    2006        WD         Normal  \n",
       "\n",
       "[2919 rows x 79 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_categorical_feature(df, category, y_feature='SalePrice', outlier=False,  include_nan=True):\n",
    "#     \"\"\"\n",
    "#         Given that there categorical variables, we \n",
    "#         want to have them ranked based on their value\n",
    "        \n",
    "#         # Arguments:\n",
    "#             df: Dataframe\n",
    "#             category: the category (feature) to be imputed\n",
    "#             y_feature: the independent feature that we base our\n",
    "#                 ranking on\n",
    "#             Type: \n",
    "#                 'average' would average the values, \n",
    "#                 'norm' returns the normalized version of means\n",
    "#             outlier: given that it is set to True, the outliers in the \n",
    "#                 y_feature of the dataframe would not be considered\n",
    "            \n",
    "#         # Returns:\n",
    "#             imputed column values with the encoding dictionary\n",
    "#             True if the data needed raking False if not\n",
    "#     \"\"\"\n",
    "#     vals_list = list(df[category].unique())\n",
    "    \n",
    "#     unique_categories = stringify_keys(vals_list)\n",
    "#     haveNan = False # Check to see if there is na/nan in unique vales\n",
    "    \n",
    "#     # Deleting NaNs since they are going to be considered seperately\n",
    "#     if 'nan' in unique_categories:\n",
    "#         haveNan = True\n",
    "#         i = unique_categories.index('nan')\n",
    "#         unique_categories.pop(i)\n",
    "    \n",
    "    \n",
    "#     # Dictionary containing mean values of different values in column\n",
    "#     means = {}\n",
    "    \n",
    "#     AVG = 0 # Sum of all averages\n",
    "    \n",
    "#     if not include_nan:\n",
    "#         haveNan = False\n",
    "#         means[np.nan] = 0\n",
    "    \n",
    "    \n",
    "#     # Going through unique values\n",
    "#     for cat in unique_categories:\n",
    "#         cat_avg = df.loc[df[category] == cat][y_feature].mean()\n",
    "#         means[cat] = cat_avg\n",
    "#         AVG += cat_avg\n",
    "        \n",
    "#     # Now considering the nan's or the values that were not in any of the unique\n",
    "#     if haveNan:\n",
    "#         na_avg = df.loc[~df[category].isin(unique_categories)][y_feature].mean()\n",
    "#         means[np.nan] = na_avg\n",
    "#         AVG += na_avg\n",
    "#         unique_categories.append('nan')\n",
    "    \n",
    "#     for cat in unique_categories:\n",
    "#         if cat == 'nan':\n",
    "#             means[np.nan] = round(means[np.nan] / AVG, 4)\n",
    "#         else:\n",
    "#             means[cat] = round(means[cat] / AVG, 4)\n",
    "    \n",
    "#     # IF the Type was not softmax return averages\n",
    "#     return means\n",
    "\n",
    "\n",
    "# def get_encoding_dicts(df, features):\n",
    "#     \"\"\" Returns the dictionary containing the encoded values for each unique\n",
    "#     value inside the categorical columns. \"\"\"\n",
    "#     cat_dicts = {}\n",
    "#     len_df = df.shape[0]\n",
    "    \n",
    "#     for feature in features:\n",
    "#         if df[feature].isna().sum() / len_df < 0.1:\n",
    "#             cat_dicts[feature] = encode_categorical_feature(df, feature)\n",
    "#         else:\n",
    "#             cat_dicts[feature] = encode_categorical_feature(df, feature, include_nan=False)\n",
    "    \n",
    "#     return cat_dicts\n",
    "\n",
    "# Get the needed dictionaries to be used for encoding categorical features\n",
    "cat_dicts = get_encoding_dicts(train, data['train_cat_list'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implement: Don't impute ones with more than 10% missing data.\n",
    "# def encode_categorical(df, cat_dicts):\n",
    "#     \"\"\" Encodes the dataframe's categorical features by mapping them \n",
    "# \tto their relative dictionary values. \"\"\"\n",
    "    \n",
    "#     for feature in cat_dicts.keys():\n",
    "#         df[feature] = df[feature].map(cat_dicts[feature])\n",
    "\t\t\n",
    "#     return df\n",
    "\n",
    "# Do the encoding\n",
    "encoded_feat_cols = encode_categorical(feat_cols.copy(), cat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38729798, 0.25802482, 0.3546772 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_feat_cols['Alley'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data with KNN:\n",
    "- Both the features of train and test are going to be implemented at the same time together using the KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "missings = feat_cols.columns[feat_cols.isna().any()].tolist()\n",
    "# The number of neighbors that the function look for is the 1/3 of the whole dataframe\n",
    "num = (train_len + test_len) // 3\n",
    "\n",
    "# Imputer object\n",
    "imputer = KNNImputer(n_neighbors=num, weights=\"distance\")\n",
    "# Get the new imputed data\n",
    "feat_cols[missings] = pd.DataFrame(imputer.fit_transform(feat_cols[missings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rebreak the data into train and test\n",
    "imp_train = feat_cols[: train_len]\n",
    "# test-data\n",
    "imp_test = feat_cols[train_len:].reset_index()\n",
    "imp_test.drop(['index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dep_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "YearBuilt       0.522897\n",
       "YearRemodAdd    0.507101\n",
       "Foundation      0.506328\n",
       "GarageYrBlt     0.506210\n",
       "GarageType      0.499204\n",
       "MasVnrArea      0.477596\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "imp_train.corr()[y_feat].nlargest(21)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check nans in X: False\n",
      "Check nans in y: False\n"
     ]
    }
   ],
   "source": [
    "# Breaking the x and y splits:\n",
    "# Finding the features\n",
    "# features = imp_train.corr()[y_feat].nlargest(21)[1:].keys().to_list()\n",
    "\n",
    "# Training datasets\n",
    "X = imp_train\n",
    "y = dep_col\n",
    "\n",
    "# It makes more sense to use batchnormalization in NN instead\n",
    "# of feeding normalized data into the model.\n",
    "norm_X = normalize(X)\n",
    "norm_y = normalize(y)\n",
    "\n",
    "# Testing datasets\n",
    "X_test = imp_test\n",
    "norm_X_test = normalize(X_test)\n",
    "\n",
    "# Check to see if the imputation worked\n",
    "print('Check nans in X:', True in X.isna().any())\n",
    "print('Check nans in y:', True in dict(y.isna()).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting\n",
    "# devs = []\n",
    "# dev_batch_size = int(imp_train.shape[0] * 0.3)\n",
    "\n",
    "# for i in range(10):\n",
    "#     dev_data = imp_train.sample(n=438, random_state=i)\n",
    "#     dev_x = dev_data[features]\n",
    "#     dev_y = dev_data[y_feat]\n",
    "#     devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "\n",
    "# Scheduler objects to control the optimizer learning rate:\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay, ExponentialDecay\n",
    "\n",
    "def TimeDecayScheduler(learning_rate=0.001, decay_steps=200, decay_rate=1.2, name=\"\")\n",
    "    \"\"\" Returns an InverseTimeDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "def ExponentialScheduler(initial_learning_rate, decay_steps, decay_rate, name=\"\"):\n",
    "    \"\"\" Returns an ExponentialDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=initial_learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "# Actual Optimizers: Adam and RMSprop are the main two optimizers that are going to be used for this project since they accept schedulers and happen to be effective.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def AdamOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "        learning_rate: the initial learning rate to be used\n",
    "        scheduler: If this is passed by the user then use it in the optimizer instead of the learning rate\n",
    "\n",
    "        # returns: an Adam optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return Adam(learning_rate)\n",
    "    else:\n",
    "        return Adam(scheduler)\n",
    "    \n",
    "\n",
    "def RMSpropOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "            learning_rate: the initial learning rate to be used\n",
    "            scheduler: If this is passed by the user then use it in the \n",
    "            optimizer instead of the learning rate\n",
    "        \n",
    "        # returns: an RMSprop optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return RMSprop(learning_rate)\n",
    "    else:\n",
    "        return RMSprop(scheduler)\n",
    "\n",
    "# CallBacks:\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def EarlyStopCallBack(patience=100):\n",
    "    \"\"\"\n",
    "        # params: patience of the object for the number of epochs passed with no improvement\n",
    "        # returns: a EarlyStopping callback object \n",
    "    \"\"\"\n",
    "    return EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "\n",
    "# Models: \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Layers \n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2, L1L2  # Regularizer\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError # Error-metric\n",
    "import tensorflow_docs as tfdocs # For logging puposes\n",
    "\n",
    "def Model01(config):\n",
    "    \"\"\"\n",
    "        # params: \n",
    "        config: uses the configuration dictionary to compile and fit the model accordingly\n",
    "        \n",
    "        # returns a history object when the fitting is done\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to try out and improve the model\n",
    "1. Weight-Initializers:\n",
    "    - Use tf.keras.initializers.RandomNormal and tf.keras.initializers.RandomUniform\n",
    "    - Tweak their properties and see how they would work.\n",
    "2. Bias in Dense layers:\n",
    "    - Setup an initiallizer and regularizer for the bias of the layer\n",
    "    - Also use it those for the weights too\n",
    "3. Layers:\n",
    "    - Use LeakyRelu/TreshholdRelu as a layer\n",
    "    - Maybe try-out tf.keras.layers.experimental.preprocessing.Normalization*\n",
    "    - Tweak BatchNormalization layer arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model01():\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        \n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        \n",
    "        layers.Dense(1024),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'relu'),\n",
    "        \n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'relu'),\n",
    "        \n",
    "        Dense(4),\n",
    "        Dense(4, kernel_regularizer=regularizers.l1_l2(0.001, 0.01)),\n",
    "        Dense(4),\n",
    "        \n",
    "        layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "      0.0025,\n",
    "      decay_steps=1460 // 5,\n",
    "      decay_rate=1.2,\n",
    "      staircase=False\n",
    "    )\n",
    "    \n",
    "    exp_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = 0.25, \n",
    "        decay_steps=1460 // 20, \n",
    "        decay_rate=0.02,\n",
    "        staircase=False, name=None\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "                loss=losses.MeanSquaredLogarithmicError(name='MSLE'), \n",
    "                optimizer=optimizer, \n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model05()\n",
    "\n",
    "def validate():\n",
    "    # Check to see if there have been an overfit or underfit\n",
    "    for i in range(10):\n",
    "        model.evaluate(devs[i][0], devs[i][1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3500\n",
    "batch_size = 1460 // 20\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.6405,  val_loss:0.6394,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.2473,  val_loss:0.2510,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.1323,  val_loss:0.1372,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.0959,  val_loss:0.1013,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.0832,  val_loss:0.0889,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0780,  val_loss:0.0834,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0743,  val_loss:0.0800,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0719,  val_loss:0.0778,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0707,  val_loss:0.0762,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0675,  val_loss:0.0742,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.0669,  val_loss:0.0739,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.0653,  val_loss:0.0719,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.0647,  val_loss:0.0710,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.0641,  val_loss:0.0701,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0633,  val_loss:0.0695,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.0629,  val_loss:0.0693,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.0609,  val_loss:0.0683,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.0608,  val_loss:0.0679,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.0611,  val_loss:0.0676,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.0605,  val_loss:0.0670,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.0596,  val_loss:0.0667,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.0590,  val_loss:0.0663,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.0592,  val_loss:0.0662,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.0584,  val_loss:0.0656,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.0582,  val_loss:0.0655,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.0583,  val_loss:0.0652,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.0570,  val_loss:0.0649,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, loss:0.0582,  val_loss:0.0646,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, loss:0.0569,  val_loss:0.0644,  \n",
      "....................................................................................................\n",
      "Epoch: 2900, loss:0.0569,  val_loss:0.0646,  \n",
      "....................................................................................................\n",
      "Epoch: 3000, loss:0.0562,  val_loss:0.0641,  \n",
      "....................................................................................................\n",
      "Epoch: 3100, loss:0.0563,  val_loss:0.0639,  \n",
      "....................................................................................................\n",
      "Epoch: 3200, loss:0.0570,  val_loss:0.0637,  \n",
      "....................................................................................................\n",
      "Epoch: 3300, loss:0.0564,  val_loss:0.0635,  \n",
      "....................................................................................................\n",
      "Epoch: 3400, loss:0.0555,  val_loss:0.0633,  \n",
      "....................................................................................................------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.33,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "# print('------------------------------------------------------------------------')\n",
    "# validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[142684, 179834, 193854, 190913, 173967]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = quantize(pd.DataFrame(model.predict(X_test, batch_size=20, steps=73, verbose=0))[0])\n",
    "\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(y_pred):\n",
    "    \"\"\" Prints out the data validation with respect to the highest submissions. \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error as MAE\n",
    "    # Import the base_validation submititions\n",
    "    b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "    b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']\n",
    "    \n",
    "    # Print out the differences\n",
    "    print('b011:', int(MAE(b011, y_pred)) / 1000)\n",
    "    print('b012:', int(MAE(b012, y_pred)) / 1000)\n",
    "    print('-----------------------------------')\n",
    "    print('base-differences:', int(MAE(b011, b012)) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': modified})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
