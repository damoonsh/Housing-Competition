{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Check all the unique categorical features in both training and testing datasets to see which features are missing in test dataset, This part might not be necessary but could be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "train_num = data['train_num']\n",
    "y_feat = 'SalePrice'\n",
    "\n",
    "# Generates a dictionary of values corresponding to the \n",
    "# categorical features within the dataset\n",
    "cat_dics = {}\n",
    "cat_feats = train.select_dtypes('object').columns.to_list()\n",
    "\n",
    "for feat in cat_feats:\n",
    "    cat_dics[feat] = rank_categorical_values(train, feat)[0]\n",
    "\n",
    "# There might be some missing values in the categorical features in the\n",
    "# testing data, which will be treated as numerical and imputed with that\n",
    "# respect.\n",
    "## Note: mappings should be done after combining datasets\n",
    "\n",
    "# Get the length of dataset so I can rebreak them after combining\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Get the column for the dependent data into a seprate variable\n",
    "dep_col = train[y_feat]\n",
    "\n",
    "# Drop the dependent column in train\n",
    "train.drop([y_feat], axis=1 , inplace = True)\n",
    "feat_cols = train.append(test) # Combine datasets\n",
    "feat_cols.reset_index(inplace=True) # Reset Indexes\n",
    "feat_cols.drop(['index', 'Id'], inplace=True, axis=1) # Drop Id and index columns\n",
    "\n",
    "# In order to impute and decode data, first it is needed \n",
    "# to break it into categorical and numerical datasets\n",
    "feat_cols_cat = feat_cols.select_dtypes('object').columns.to_list()\n",
    "feat_cols_num = feat_cols.select_dtypes(['float64', 'int64']).columns.to_list()\n",
    "\n",
    "# Decode the categorical features in the combined dataset\n",
    "for feat in cat_feats:\n",
    "    feat_cols[feat] = impute_rank_weight(feat_cols[feat].copy(), cat_dics[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CollgCr': 0.04304425976649378,\n",
       " 'Veenker': 0.05191703153946601,\n",
       " 'Crawfor': 0.04579673165007603,\n",
       " 'NoRidge': 0.07290421209470918,\n",
       " 'Mitchel': 0.03397825609535837,\n",
       " 'Somerst': 0.049004977454523396,\n",
       " 'NWAmes': 0.04110569276735894,\n",
       " 'OldTown': 0.027880390973622476,\n",
       " 'BrkSide': 0.027143022046897163,\n",
       " 'Sawyer': 0.029743319483390717,\n",
       " 'NridgHt': 0.06876761896720276,\n",
       " 'NAmes': 0.031711944403308295,\n",
       " 'SawyerW': 0.04056335615506725,\n",
       " 'IDOTRR': 0.021770198380387268,\n",
       " 'MeadowV': 0.021433761682225477,\n",
       " 'Edwards': 0.027879173157315654,\n",
       " 'Timber': 0.052672549788386036,\n",
       " 'Gilbert': 0.041932902480024487,\n",
       " 'StoneBr': 0.06751267852111145,\n",
       " 'ClearCr': 0.04621870422721212,\n",
       " 'NPkVill': 0.031026457909772254,\n",
       " 'Blmngtn': 0.04237132883976904,\n",
       " 'BrDale': 0.02272037253329444,\n",
       " 'SWISU': 0.0310040439665444,\n",
       " 'Blueste': 0.029897015116482902}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dics['Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing categorical variables with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "missings = feat_cols.columns[feat_cols.isna().any()].tolist()\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=300, weights=\"distance\")\n",
    "feat_cols[missings] = pd.DataFrame(imputer.fit_transform(feat_cols[missings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rebreak the data into train and test\n",
    "imp_train = feat_cols.iloc[:train_len]\n",
    "imp_train.loc[:, (y_feat)] = dep_col\n",
    "imp_test = feat_cols[train_len:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_train.corr()[y_feat].nlargest(15)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "YearBuilt       0.522897\n",
       "YearRemodAdd    0.507101\n",
       "Foundation      0.506328\n",
       "GarageYrBlt     0.506210\n",
       "GarageType      0.499204\n",
       "MasVnrArea      0.477596\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "imp_train.corr()[y_feat].nlargest(21)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the x and y splits:\n",
    "# Finding the features\n",
    "features = imp_train.corr()[y_feat].nlargest(20)[1:].keys().to_list()\n",
    "\n",
    "# Training datasets\n",
    "X = imp_train[features]\n",
    "y = imp_train[y_feat]\n",
    "\n",
    "# It makes more sense to use batchnormalization in NN instead\n",
    "# of feeding normalized data into the model.\n",
    "norm_X = normalize(X)\n",
    "norm_y = normalize(y)\n",
    "\n",
    "# Testing datasets\n",
    "X_test = imp_test[features]\n",
    "norm_X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting\n",
    "devs = []\n",
    "dev_batch_size = int(imp_train.shape[0] * 0.3)\n",
    "\n",
    "for i in range(10):\n",
    "    dev_data = imp_train.sample(n=438, random_state=i)\n",
    "    dev_x = dev_data[features]\n",
    "    dev_y = dev_data[y_feat]\n",
    "    devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if the imputation worked\n",
    "True in X.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax does not make sense, drop out and batchnormalization works\n",
    "# For metrics, mse and msle should be considered\n",
    "# The only place to use the BatchNormalization layer is at the beginning\n",
    "\n",
    "#\n",
    "def build_model05():\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(64),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        \n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
    "        layers.Dense(256, activation='elu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        \n",
    "        layers.Dense(1024),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'elu'),\n",
    "        layers.Dense(16, activation = 'relu'),\n",
    "        \n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'elu'),\n",
    "        Dense(8, activation = 'relu'),\n",
    "        \n",
    "        Dense(4),\n",
    "        Dense(4, kernel_regularizer=regularizers.l1_l2(0.001, 0.01)),\n",
    "        Dense(4),\n",
    "        \n",
    "        layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "      0.0025,\n",
    "      decay_steps=1460 // 5,\n",
    "      decay_rate=1.2,\n",
    "      staircase=False\n",
    "    )\n",
    "    \n",
    "    exp_lr = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate = 0.25, \n",
    "        decay_steps=1460 // 20, \n",
    "        decay_rate=0.02, \n",
    "        staircase=False, name=None\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "                loss=losses.MeanSquaredLogarithmicError(name='MSLE'), \n",
    "                optimizer=optimizer, \n",
    "                metrics=[metrics.MeanSquaredLogarithmicError(name='msle')]\n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = build_model05()\n",
    "\n",
    "def validate():\n",
    "    # Check to see if there have been an overfit or underfit\n",
    "    for i in range(10):\n",
    "        model.evaluate(devs[i][0], devs[i][1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3500\n",
    "batch_size = 1460 // 20\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:77.6320,  msle:71.9987,  val_loss:15.2908,  val_msle:9.9288,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:1.4919,  msle:0.0613,  val_loss:1.4780,  val_msle:0.0511,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:1.1085,  msle:0.0532,  val_loss:1.1007,  val_msle:0.0471,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.9043,  msle:0.0521,  val_loss:0.8976,  val_msle:0.0465,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.7663,  msle:0.0487,  val_loss:0.7624,  val_msle:0.0455,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.6749,  msle:0.0549,  val_loss:0.6652,  val_msle:0.0458,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.5987,  msle:0.0543,  val_loss:0.5891,  val_msle:0.0452,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.5310,  msle:0.0475,  val_loss:0.5280,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.4739,  msle:0.0411,  val_loss:0.4782,  val_msle:0.0457,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.4317,  msle:0.0417,  val_loss:0.4348,  val_msle:0.0451,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.3971,  msle:0.0454,  val_loss:0.3963,  val_msle:0.0446,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.3528,  msle:0.0347,  val_loss:0.3624,  val_msle:0.0445,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.3266,  msle:0.0378,  val_loss:0.3331,  val_msle:0.0445,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.3072,  msle:0.0449,  val_loss:0.3074,  val_msle:0.0454,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.2933,  msle:0.0548,  val_loss:0.2827,  val_msle:0.0441,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.2565,  msle:0.0394,  val_loss:0.2611,  val_msle:0.0441,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.2465,  msle:0.0485,  val_loss:0.2424,  val_msle:0.0446,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.2301,  msle:0.0496,  val_loss:0.2245,  val_msle:0.0440,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.2076,  msle:0.0429,  val_loss:0.2090,  val_msle:0.0444,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.1960,  msle:0.0454,  val_loss:0.1946,  val_msle:0.0442,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.1762,  msle:0.0387,  val_loss:0.1817,  val_msle:0.0444,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.1776,  msle:0.0520,  val_loss:0.1707,  val_msle:0.0452,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.1612,  msle:0.0461,  val_loss:0.1590,  val_msle:0.0441,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.1624,  msle:0.0566,  val_loss:0.1497,  val_msle:0.0440,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.1449,  msle:0.0477,  val_loss:0.1413,  val_msle:0.0442,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.1338,  msle:0.0441,  val_loss:0.1341,  val_msle:0.0445,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.1172,  msle:0.0340,  val_loss:0.1273,  val_msle:0.0442,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, loss:0.1177,  msle:0.0405,  val_loss:0.1225,  val_msle:0.0453,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, loss:0.1263,  msle:0.0543,  val_loss:0.1160,  val_msle:0.0440,  \n",
      "....................................................................................................\n",
      "Epoch: 2900, loss:0.1109,  msle:0.0435,  val_loss:0.1113,  val_msle:0.0440,  \n",
      "....................................................................................................\n",
      "Epoch: 3000, loss:0.1011,  msle:0.0379,  val_loss:0.1084,  val_msle:0.0452,  \n",
      "....................................................................................................\n",
      "Epoch: 3100, loss:0.1046,  msle:0.0450,  val_loss:0.1035,  val_msle:0.0439,  \n",
      "....................................................................................................\n",
      "Epoch: 3200, loss:0.0953,  msle:0.0388,  val_loss:0.1004,  val_msle:0.0439,  \n",
      "....................................................................................................\n",
      "Epoch: 3300, loss:0.1006,  msle:0.0468,  val_loss:0.0975,  val_msle:0.0438,  \n",
      "....................................................................................................\n",
      "Epoch: 3400, loss:0.0939,  msle:0.0427,  val_loss:0.0954,  val_msle:0.0442,  \n",
      "....................................................................................................------------------------------------------------------------------------\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0930 - msle: 0.0439\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0999 - msle: 0.0507\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0977 - msle: 0.0485\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - msle: 0.0438\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0930 - msle: 0.0439\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.0884 - msle: 0.0393\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.0938 - msle: 0.0446\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0937 - msle: 0.0446\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0874 - msle: 0.0382\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.0935 - msle: 0.0444\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0], steps_per_epoch=3,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print('------------------------------------------------------------------------')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:90.0606,  msle:84.4350,  val_loss:32.7803,  val_msle:27.4311,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:1.6448,  msle:0.0422,  val_loss:1.6497,  val_msle:0.0502,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:1.3061,  msle:0.0523,  val_loss:1.3012,  val_msle:0.0491,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:1.0975,  msle:0.0513,  val_loss:1.0924,  val_msle:0.0473,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.9486,  msle:0.0477,  val_loss:0.9472,  val_msle:0.0470,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.8541,  msle:0.0650,  val_loss:0.8361,  val_msle:0.0476,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.7581,  msle:0.0582,  val_loss:0.7458,  val_msle:0.0465,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.6805,  msle:0.0552,  val_loss:0.6715,  val_msle:0.0468,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.6018,  msle:0.0402,  val_loss:0.6072,  val_msle:0.0460,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.5473,  msle:0.0409,  val_loss:0.5519,  val_msle:0.0459,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.4934,  msle:0.0356,  val_loss:0.5033,  val_msle:0.0458,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.4617,  msle:0.0470,  val_loss:0.4601,  val_msle:0.0457,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.4153,  msle:0.0389,  val_loss:0.4217,  val_msle:0.0456,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.4016,  msle:0.0594,  val_loss:0.3889,  val_msle:0.0471,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.3693,  msle:0.0588,  val_loss:0.3564,  val_msle:0.0461,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.3324,  msle:0.0501,  val_loss:0.3278,  val_msle:0.0458,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.3084,  msle:0.0516,  val_loss:0.3026,  val_msle:0.0460,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.2721,  msle:0.0384,  val_loss:0.2787,  val_msle:0.0452,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.2483,  msle:0.0357,  val_loss:0.2582,  val_msle:0.0457,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.2459,  msle:0.0519,  val_loss:0.2399,  val_msle:0.0460,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.2172,  msle:0.0402,  val_loss:0.2232,  val_msle:0.0464,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.2073,  msle:0.0454,  val_loss:0.2068,  val_msle:0.0450,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.1912,  msle:0.0429,  val_loss:0.1931,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.1774,  msle:0.0410,  val_loss:0.1812,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.1740,  msle:0.0484,  val_loss:0.1706,  val_msle:0.0451,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.1636,  msle:0.0475,  val_loss:0.1610,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.1559,  msle:0.0483,  val_loss:0.1523,  val_msle:0.0447,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, loss:0.1417,  msle:0.0416,  val_loss:0.1449,  val_msle:0.0449,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, loss:0.1315,  msle:0.0380,  val_loss:0.1383,  val_msle:0.0447,  \n",
      "....................................................................................................\n",
      "Epoch: 2900, loss:0.1317,  msle:0.0441,  val_loss:0.1322,  val_msle:0.0446,  \n",
      "....................................................................................................\n",
      "Epoch: 3000, loss:0.1198,  msle:0.0373,  val_loss:0.1273,  val_msle:0.0448,  \n",
      "....................................................................................................\n",
      "Epoch: 3100, loss:0.1164,  msle:0.0387,  val_loss:0.1222,  val_msle:0.0445,  \n",
      "....................................................................................................\n",
      "Epoch: 3200, loss:0.1189,  msle:0.0453,  val_loss:0.1181,  val_msle:0.0446,  \n",
      "....................................................................................................\n",
      "Epoch: 3300, loss:0.1075,  msle:0.0379,  val_loss:0.1141,  val_msle:0.0445,  \n",
      "....................................................................................................\n",
      "Epoch: 3400, loss:0.1240,  msle:0.0577,  val_loss:0.1106,  val_msle:0.0444,  \n",
      "....................................................................................................------------------------------------------------------------------------\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1076 - msle: 0.0444\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1145 - msle: 0.0512\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1129 - msle: 0.0497\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - msle: 0.0449\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - msle: 0.0449\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - msle: 0.0392\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1085 - msle: 0.0453\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1080 - msle: 0.0448\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1023 - msle: 0.0390\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.1078 - msle: 0.0446\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0], steps_per_epoch=3,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print('------------------------------------------------------------------------')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:2731.9441,  msle:310.2028,  val_loss:4506.7349,  val_msle:144.8269,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:172.0736,  msle:144.8881,  val_loss:171.5040,  val_msle:144.8269,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:157.5065,  msle:143.7339,  val_loss:158.3683,  val_msle:144.8269,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:155.9819,  msle:143.8297,  val_loss:156.8117,  val_msle:144.8269,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:156.9399,  msle:145.2515,  val_loss:156.3703,  val_msle:144.8269,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:156.2864,  msle:144.9460,  val_loss:156.0669,  val_msle:144.8269,  \n",
      "............................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-edff3e2fe9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0], steps_per_epoch=3,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print('------------------------------------------------------------------------')\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b012 = load_bench_data(file_name='012008.csv', root='./submissions/')['SalePrice']\n",
    "b011 = load_bench_data(file_name='011978.csv', root='./submissions/')['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 15\n",
    "den = (0.12008 ** exp + 0.11978 ** exp)\n",
    "\n",
    "w012 = 1 - 0.12008 ** exp / den\n",
    "w011 = 1 - 0.11978 ** exp / den\n",
    "\n",
    "pred_y = b012 * w012 + b011 * w011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.1061,  msle:0.0429,  val_loss:0.1105,  val_msle:0.0474,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.1013,  msle:0.0427,  val_loss:0.1067,  val_msle:0.0481,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.0977,  msle:0.0432,  val_loss:0.1025,  val_msle:0.0480,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.0939,  msle:0.0430,  val_loss:0.0989,  val_msle:0.0481,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.0908,  msle:0.0432,  val_loss:0.0957,  val_msle:0.0481,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0880,  msle:0.0433,  val_loss:0.0928,  val_msle:0.0481,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0854,  msle:0.0430,  val_loss:0.0902,  val_msle:0.0478,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0834,  msle:0.0431,  val_loss:0.0882,  val_msle:0.0479,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0806,  msle:0.0421,  val_loss:0.0864,  val_msle:0.0479,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0794,  msle:0.0425,  val_loss:0.0847,  val_msle:0.0478,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.0781,  msle:0.0425,  val_loss:0.0832,  val_msle:0.0477,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.0763,  msle:0.0420,  val_loss:0.0821,  val_msle:0.0477,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.0762,  msle:0.0429,  val_loss:0.0810,  val_msle:0.0477,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.0746,  msle:0.0423,  val_loss:0.0799,  val_msle:0.0476,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0734,  msle:0.0419,  val_loss:0.0791,  val_msle:0.0475,  \n",
      "...................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[138841, 154922, 179706, 176166, 170645]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A very low patience rate for the \n",
    "train_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Fit it to all of the data\n",
    "model.fit(X, y, \n",
    "          epochs=1500, steps_per_epoch=5, validation_split=0.3,\n",
    "          verbose=0, callbacks=[tfdocs.modeling.EpochDots(), train_stop]\n",
    "         )\n",
    "\n",
    "pred_y = pd.DataFrame(model.predict(X_test, batch_size=20, steps=73, verbose=0))[0]\n",
    "# It would make sense to convert all of the data to int \n",
    "# instead of float since there no floats in trainig.\n",
    "modified = quantize(pred_y)\n",
    "\n",
    "modified[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b011: 24.429\n",
      "b012: 24.389\n",
      "-----------------------------------\n",
      "b: 5.76\n"
     ]
    }
   ],
   "source": [
    "# val_loss of 0.0197 is close\n",
    "print('b011:', int(MAE(b011, modified)) / 1000)\n",
    "print('b012:', int(MAE(b012, modified)) / 1000)\n",
    "print('-----------------------------------')\n",
    "print('b:', int(MAE(b011, b012)) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': modified})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
