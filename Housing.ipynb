{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another more concrete way of imputing data could be to seperate the\n",
    "# independent feature columns from both train and test set and then \n",
    "# impute them all at once, then break it again into test and train\n",
    "# and then run the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "train_num = data['train_num']\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Check all the unique categorical features in both training and testing datasets to see which features are missing in test dataset, This part might not be necessary but could be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a dictionary of values corresponding to the \n",
    "# categorical features within the dataset\n",
    "cat_dics = {}\n",
    "cat_feats = train.select_dtypes('object').columns.to_list()\n",
    "\n",
    "for feat in cat_feats:\n",
    "    cat_dics[feat] = rank_categorical_values(train, feat)[0]\n",
    "\n",
    "# There might be some missing values in the categorical features in the\n",
    "# testing data, which will be treated as numerical and imputed with that\n",
    "# respect.\n",
    "## Note: mappings should be done after combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of dataset so I can rebreak them after combining\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Get the column for the dependent data into a seprate variable\n",
    "dep_col = train[y_feature]\n",
    "\n",
    "# Drop the dependent column in train\n",
    "train.drop([y_feat], axis=1 , inplace = True)\n",
    "feat_cols = train.append(test) # Combine datasets\n",
    "feat_cols.reset_index(inplace=True) # Reset Indexes\n",
    "feat_cols.drop(['index', 'Id'], inplace=True, axis=1) # Drop Id and index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to impute and decode data, first it is needed \n",
    "# to break it into categorical and numerical datasets\n",
    "feat_cols_cat = feat_cols.select_dtypes('object').columns.to_list()\n",
    "feat_cols_num = feat_cols.select_dtypes(['float64', 'int64']).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the categorical features in the combined dataset\n",
    "for feat in cat_feats:\n",
    "    feat_cols[feat] = impute_rank_weight(feat_cols[feat].copy(), cat_dics[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "missings = feat_cols.columns[feat_cols.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing categorical variables with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=300, weights=\"distance\")\n",
    "feat_cols[missings] = pd.DataFrame(imputer.fit_transform(feat_cols[missings]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-205-da9d8535fe27>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  imp_train[y_feature] = dep_col\n"
     ]
    }
   ],
   "source": [
    "# Now rebreak the data into train and test\n",
    "imp_train = feat_cols[:train_len]\n",
    "imp_train[y_feature] = dep_col\n",
    "imp_test = feat_cols[train_len:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738630\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681905\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553059\n",
       "FireplaceQu     0.542181\n",
       "TotRmsAbvGrd    0.533723\n",
       "YearBuilt       0.522897\n",
       "YearRemodAdd    0.507101\n",
       "Foundation      0.506328\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_train.corr()[y_feature].nlargest(18)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the x and y splits:\n",
    "# Finding the features\n",
    "features = imp_train.corr()[y_feature].nlargest(18)[1:].keys().to_list()\n",
    "\n",
    "# Training datasets\n",
    "X = imp_train[features]\n",
    "y = imp_train[y_feature]\n",
    "# It makes more sense to use batchnormalization in NN instead\n",
    "# of feeding normalized data into the model.\n",
    "norm_X = normalize(X)\n",
    "norm_y = normalize(y)\n",
    "\n",
    "# Testing datasets\n",
    "X_test = imp_test[features]\n",
    "norm_X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks of data used to check for overfitting\n",
    "devs = []\n",
    "dev_batch_size = int(imp_train.shape[0] * 0.3)\n",
    "\n",
    "for i in range(10):\n",
    "    dev_data = imp_train.sample(n=438, random_state=i)\n",
    "    dev_x = dev_data[features]\n",
    "    dev_y = dev_data[y_feature]\n",
    "    devs.append((dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if the imputation worked\n",
    "True in X.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651256</td>\n",
       "      <td>0.290473</td>\n",
       "      <td>0.370207</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.401816</td>\n",
       "      <td>0.581115</td>\n",
       "      <td>0.311618</td>\n",
       "      <td>0.350880</td>\n",
       "      <td>-0.459145</td>\n",
       "      <td>-0.793162</td>\n",
       "      <td>0.789470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.071812</td>\n",
       "      <td>0.985904</td>\n",
       "      <td>-0.482347</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>0.401816</td>\n",
       "      <td>-0.763002</td>\n",
       "      <td>0.311618</td>\n",
       "      <td>-0.060710</td>\n",
       "      <td>0.466305</td>\n",
       "      <td>0.257052</td>\n",
       "      <td>0.789470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.651256</td>\n",
       "      <td>0.290473</td>\n",
       "      <td>0.514836</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.401816</td>\n",
       "      <td>0.581115</td>\n",
       "      <td>0.311618</td>\n",
       "      <td>0.631510</td>\n",
       "      <td>-0.313261</td>\n",
       "      <td>-0.627611</td>\n",
       "      <td>0.789470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.651256</td>\n",
       "      <td>0.506207</td>\n",
       "      <td>0.383528</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.741365</td>\n",
       "      <td>0.581115</td>\n",
       "      <td>1.649742</td>\n",
       "      <td>0.790533</td>\n",
       "      <td>-0.687089</td>\n",
       "      <td>-0.521555</td>\n",
       "      <td>-1.025689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.374324</td>\n",
       "      <td>2.630839</td>\n",
       "      <td>1.298881</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.401816</td>\n",
       "      <td>0.581115</td>\n",
       "      <td>1.649742</td>\n",
       "      <td>1.697903</td>\n",
       "      <td>0.199611</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.789470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OverallQual  Neighborhood  GrLivArea  ExterQual  BsmtQual  KitchenQual  \\\n",
       "0     0.651256      0.290473   0.370207   0.923900  0.401816     0.581115   \n",
       "1    -0.071812      0.985904  -0.482347  -0.666429  0.401816    -0.763002   \n",
       "2     0.651256      0.290473   0.514836   0.923900  0.401816     0.581115   \n",
       "3     0.651256      0.506207   0.383528  -0.666429 -0.741365     0.581115   \n",
       "4     1.374324      2.630839   1.298881   0.923900  0.401816     0.581115   \n",
       "\n",
       "   GarageCars  GarageArea  TotalBsmtSF  1stFlrSF  FullBath  \n",
       "0    0.311618    0.350880    -0.459145 -0.793162  0.789470  \n",
       "1    0.311618   -0.060710     0.466305  0.257052  0.789470  \n",
       "2    0.311618    0.631510    -0.313261 -0.627611  0.789470  \n",
       "3    1.649742    0.790533    -0.687089 -0.521555 -1.025689  \n",
       "4    1.649742    1.697903     0.199611 -0.045596  0.789470  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax does not make sense, drop out and batchnormalization works\n",
    "def build_model04():\n",
    "  model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=[len(X.keys())]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "  model.compile(loss='msle', optimizer=optimizer,)\n",
    "  return model\n",
    "\n",
    "model = build_model04()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "batch_size = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:123.1985,  val_loss:81.0794,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.2243,  val_loss:0.2128,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.0778,  val_loss:0.0730,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.0411,  val_loss:0.0384,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.0298,  val_loss:0.0280,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0276,  val_loss:0.0250,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0255,  val_loss:0.0246,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0250,  val_loss:0.0244,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0262,  val_loss:0.0246,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0246,  val_loss:0.0242,  \n",
      "...................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f86bc4fd850>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0],\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:124.6541,  val_loss:40.6012,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.2236,  val_loss:0.2366,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.0742,  val_loss:0.0702,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.0450,  val_loss:0.0416,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.0326,  val_loss:0.0305,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.0264,  val_loss:0.0251,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.0243,  val_loss:0.0231,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.0244,  val_loss:0.0224,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.0227,  val_loss:0.0226,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.0235,  val_loss:0.0221,  \n",
      "...................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f86b4371940>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=batch_size, epochs=EPOCHS,\n",
    "          verbose=0, validation_data=devs[0],\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0220\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0211\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0203\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0180\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.evaluate(devs[i][0], devs[i][1], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/3-015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def f(x, std, mean):\n",
    "    exponent = ((x - mean) / std) ** 2 * (-1) * 0.5\n",
    "    hyp = std * np.sqrt(2 * np.pi)\n",
    "    \n",
    "    return np.exp(exponent) / hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = pd.DataFrame(model.predict(X_test, batch_size=20, steps=73, verbose=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would make sense to convert all of the data to int \n",
    "# instead of float since there no floats in trainig.\n",
    "modified = [] \n",
    "for num in list(pd.DataFrame(pred_y)[0].values):\n",
    "    if num - int(num) >= 0.5:\n",
    "        modified.append(int(num) + 1)\n",
    "    else:\n",
    "        modified.append(int(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120429,\n",
       " 143637,\n",
       " 178952,\n",
       " 190009,\n",
       " 215563,\n",
       " 180544,\n",
       " 172236,\n",
       " 179009,\n",
       " 178711,\n",
       " 122787]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': modified})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
