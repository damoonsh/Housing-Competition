{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Difference with previous versions:``\n",
    "- Using a different approach to Encoding and imputing data, meaning that having more zeros for either the missing numerical values, and nan values in the categorical ones. Since I will be using all (or at least most) the feature in the dataset it could be helpfull to just have zeros rather values that are probably misleading. The columns with a low number of missing values will just imputed using the KNN algorithm.\n",
    "- Using regularizers more extensively, as well as controlling the properties of Layers such as weight and bias-initializers more closely.\n",
    "- Written some new utility functions that can help enhance EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "data = retrieve_data()\n",
    "train = data['train'].copy()\n",
    "test = data['test'].copy()\n",
    "\n",
    "# The dependent feature\n",
    "y_feat = 'SalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing:\n",
    "The general strategy is to combine both the categorical and numerical values in the training and testing and then process them at the same time. For categorical variables we will be getting dictionaries based on the training data and then process them for the combined dataframe. After converting the categorical features to numerical there will be some missing values. And imputation is done using the KNN imputation.\n",
    "\n",
    "1. Encoding categorical features: The goal is to find a number that could represent the unique values in each of the categorical columns. By having those values each of the string values will be replaced with a numerical one. The encode_categorical_feature function in ./utils.py gets the dataset and the column that we are trying to encode, then it returns the average of the all prices with a given value in the column divided by the sum of all averages for different values in the column: Given that x1,x2,x3,..,xn are unique values in column C, the average SalePrice (dependent column in the dataset) when C is x1 will be avg1, and respectively for each of these unique values in the dataset, there will be avg2, avg3, ..., avgn. Now, in order to have small values for encoding each of the averages will be divided by the sum of all averages: Avg1 = avg1 / (avg1 + 1vg2 + ..+ avgn) and so on. As it is apparent the sum of all the returned encoding values will be one: Avg1 + Avg2 + ... + Avgn = 1. Now when using this technique we can impute the NaN values within the categorical features just by replacing them with the same logic and it will not need any further imputation. Yet it is important to note that if the number of missing values (NaN) is a lot then we should not do the encoding for NaNs and impute them after they are encoded. So, if more than 10% if the data was missing then don't encode the Nan in those columns (implemented in get_encoding_dicts).\n",
    "\n",
    "2. Imputing numerical data: The numerical data will simply be imputed using the KNN imputer module of sklearn. \n",
    "\n",
    "## In-depth analysis of categorical variables:\n",
    "1. Compare the different NaNs for the same categories (and not) in the number of NaNs they have.\n",
    "2. Given that 90% data is not missing for a given feature (column) map their encoded numerical values in the dataframe, otherwise, only impute non-nan values in the feature and then impute the rest of the missing values using any other technique. Dropping the column for values with too many missing might be a general option but in order to use the data for Nerual Networks, it would make sense to just impute the missing values with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DataFrames\n",
    "cat_info, num_info = missing_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>1352</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electrical</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>730</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>1456</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>1169</td>\n",
       "      <td>1179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>1408</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test  Train\n",
       "Alley         1352   1369\n",
       "MasVnrType      16      8\n",
       "BsmtQual        44     37\n",
       "BsmtCond        45     37\n",
       "BsmtExposure    44     38\n",
       "BsmtFinType1    42     37\n",
       "BsmtFinType2    42     38\n",
       "Electrical       0      1\n",
       "FireplaceQu    730    690\n",
       "GarageType      76     81\n",
       "GarageFinish    78     81\n",
       "GarageQual      78     81\n",
       "GarageCond      78     81\n",
       "PoolQC        1456   1453\n",
       "Fence         1169   1179\n",
       "MiscFeature   1408   1406"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important to note for categorical features:\n",
    "1. Alley, PoolQc, Fence, MiscFeature are the features with an ecessive number of missing values both in training and testing.\n",
    "2. FireplaceQu does not have as many missing values as the features above but it is going to be treated the same way.\n",
    "3. Although for some these values NA means that they just don't have that feature: Alley, MiscFeature, PoolQc, we still will be imppute them as numerical variables instead of imuting with zeros.\n",
    "\n",
    "#### Note: To conclude there are 5 features that the Nan values will be not encoded in their column's category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>259.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>8.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>81.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCars</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageArea</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test  Train\n",
       "LotFrontage   259.0    227\n",
       "MasVnrArea      8.0     15\n",
       "GarageYrBlt    81.0     78\n",
       "BsmtFinSF1      0.0      1\n",
       "BsmtFinSF2      0.0      1\n",
       "BsmtUnfSF       0.0      1\n",
       "TotalBsmtSF     0.0      1\n",
       "BsmtFullBath    0.0      2\n",
       "BsmtHalfBath    0.0      2\n",
       "GarageCars      0.0      1\n",
       "GarageArea      0.0      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The missing values in the test and train dataset for numerical variables are close to each other and there is not huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data:\n",
    "Based on this dataframe, there some features missing in Training that are not missing in the test data. There is no need manually impute anything in the case of numerical values and I am just going to let KNN handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of training data to rebreak the combined data further along the way\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "# Combine the train and test:\n",
    "# Note: Pass the copies so the actual dataframes won't change and we can still use them\n",
    "feat_cols = combine_train_test(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0             60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1             20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2             60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3             70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4             60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...          ...      ...          ...      ...    ...   ...      ...   \n",
       "2914         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "2915         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "2916          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "2917          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "2918          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "1            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "2            Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "3            Lvl    AllPub    Corner  ...           0        0    NaN    NaN   \n",
       "4            Lvl    AllPub       FR2  ...           0        0    NaN    NaN   \n",
       "...          ...       ...       ...  ...         ...      ...    ...    ...   \n",
       "2914         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2915         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2916         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "2917         Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv   \n",
       "2918         Lvl    AllPub    Inside  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0            NaN       0       2    2008        WD         Normal  \n",
       "1            NaN       0       5    2007        WD         Normal  \n",
       "2            NaN       0       9    2008        WD         Normal  \n",
       "3            NaN       0       2    2006        WD        Abnorml  \n",
       "4            NaN       0      12    2008        WD         Normal  \n",
       "...          ...     ...     ...     ...       ...            ...  \n",
       "2914         NaN       0       6    2006        WD         Normal  \n",
       "2915         NaN       0       4    2006        WD        Abnorml  \n",
       "2916         NaN       0       9    2006        WD        Abnorml  \n",
       "2917        Shed     700       7    2006        WD         Normal  \n",
       "2918         NaN       0      11    2006        WD         Normal  \n",
       "\n",
       "[2919 rows x 79 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring: Alley\n",
      "Ignoring: FireplaceQu\n",
      "Ignoring: PoolQC\n",
      "Ignoring: Fence\n",
      "Ignoring: MiscFeature\n"
     ]
    }
   ],
   "source": [
    "# Get the needed dictionaries to be used for encoding categorical features\n",
    "cat_dicts = get_encoding_dicts(train, data['train_cat_list'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan: 0, 'Grvl': 0.4211, 'Pave': 0.5789}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking one of the values.\n",
    "cat_dicts['Alley']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.5818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>0.3386</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.3491</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.1726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSZoning  Street  Alley  LotShape  LandContour  Utilities  LotConfig  \\\n",
       "0       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "1       0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1837   \n",
       "2       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1826   \n",
       "3       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1875   \n",
       "4       0.2590  0.5818    0.0    0.2493       0.2376     0.5682     0.1837   \n",
       "...        ...     ...    ...       ...          ...        ...        ...   \n",
       "2914    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2915    0.1713  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2916    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2917    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "2918    0.2590  0.5818    0.0    0.1993       0.2376     0.5682     0.1826   \n",
       "\n",
       "      LandSlope  Neighborhood  Condition1  ...  GarageType  GarageFinish  \\\n",
       "0        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "1        0.3097        0.0519      0.0875  ...      0.1817        0.2939   \n",
       "2        0.3097        0.0430      0.1133  ...      0.1817        0.2939   \n",
       "3        0.3097        0.0458      0.1133  ...      0.1201        0.2067   \n",
       "4        0.3097        0.0729      0.1133  ...      0.1817        0.2939   \n",
       "...         ...           ...         ...  ...         ...           ...   \n",
       "2914     0.3097        0.0214      0.1133  ...      0.0925        0.1503   \n",
       "2915     0.3097        0.0214      0.1133  ...      0.0985        0.2067   \n",
       "2916     0.3097        0.0340      0.1133  ...      0.1201        0.2067   \n",
       "2917     0.3097        0.0340      0.1133  ...      0.0925        0.1503   \n",
       "2918     0.3386        0.0340      0.1133  ...      0.1817        0.3491   \n",
       "\n",
       "      GarageQual  GarageCond  PavedDrive  PoolQC  Fence  MiscFeature  \\\n",
       "0         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "1         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "3         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "4         0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "...          ...         ...         ...     ...    ...          ...   \n",
       "2914      0.1064      0.1263      0.4298     0.0  0.000        0.000   \n",
       "2915      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2916      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "2917      0.1064      0.1263      0.4298     0.0  0.247        0.227   \n",
       "2918      0.1930      0.2296      0.4298     0.0  0.000        0.000   \n",
       "\n",
       "      SaleType  SaleCondition  \n",
       "0       0.1035         0.1726  \n",
       "1       0.1035         0.1726  \n",
       "2       0.1035         0.1726  \n",
       "3       0.1035         0.1443  \n",
       "4       0.1035         0.1726  \n",
       "...        ...            ...  \n",
       "2914    0.1035         0.1726  \n",
       "2915    0.1035         0.1443  \n",
       "2916    0.1035         0.1443  \n",
       "2917    0.1035         0.1726  \n",
       "2918    0.1035         0.1726  \n",
       "\n",
       "[2919 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the encoding\n",
    "encoded_feat_cols = encode_categorical(feat_cols.copy(), cat_dicts)\n",
    "encoded_feat_cols[data['train_cat_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: They were some features which did not have any missing values in the training dataset however they did in test set. Hence they are going to be some missing values in the previousley categorical features and from now on they are going to be imputed the same way the numerical features will be imputed, in other words, they will be treated as numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Data with KNN:\n",
    "- Both the features of train and test are going to be implemented at the same time together using the KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the missing values with KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "missing_features = encoded_feat_cols.columns[encoded_feat_cols.isna().any()].tolist()\n",
    "\n",
    "# The number of neighbors that the function look for is the 1/3 of the whole dataframe\n",
    "num = (train_len + test_len) // 3\n",
    "\n",
    "# Instantiate the Imputer object\n",
    "imputer = KNNImputer(n_neighbors=num, weights=\"distance\")\n",
    "# Fit and transform using the imputer on the missing data and get the imputed combined data\n",
    "imputed_combined = pd.DataFrame()\n",
    "imputed_combined[encoded_feat_cols.columns.to_list()] = pd.DataFrame(imputer.fit_transform(encoded_feat_cols))\n",
    "\n",
    "# Check the imputation:\n",
    "True in imputed_combined.isna().any().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selecrion: It would be all the features for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitting and predicting datasets:\n",
    "\n",
    "# features:\n",
    "features = imputed_combined.columns.to_list()\n",
    "\n",
    "train_part = pd.DataFrame()\n",
    "train_part = imputed_combined.iloc[:train_len]\n",
    "# Add the y_feat column (for use further along the way)\n",
    "train_part.loc[:, (y_feat)] = train[y_feat]\n",
    "\n",
    "# X would be the features that will be used for both prediction and training\n",
    "X = train_part[features]\n",
    "y = train[y_feat] # y, the dependent column of the dataset\n",
    "\n",
    "# The dataset used for prediction\n",
    "X_test = imputed_combined[train_len: ].reset_index()\n",
    "X_test.drop(['index'], inplace=True, axis=1)\n",
    "\n",
    "# Normalized version of datasets\n",
    "norm_X = normalize(X.copy())\n",
    "norm_y = normalize(y.copy())\n",
    "norm_X_test = normalize(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.790982\n",
       "Neighborhood    0.738629\n",
       "GrLivArea       0.708624\n",
       "ExterQual       0.690933\n",
       "BsmtQual        0.681904\n",
       "KitchenQual     0.675721\n",
       "GarageCars      0.640409\n",
       "GarageArea      0.623431\n",
       "TotalBsmtSF     0.613581\n",
       "1stFlrSF        0.605852\n",
       "FullBath        0.560664\n",
       "GarageFinish    0.553058\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_part.corr()[y_feat].nlargest(13)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting parts\n",
    "The general idea is to come up with a number of Neural Network architectures, then fine-tune their hyperparameters and put into the models.py file. It is important to take into account that if a model had a error less than 0.14 then just save it in the models folder with the name NN-[error rate]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, losses, metrics\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2, L1L2\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer,LeakyReLU\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, InverseTimeDecay\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "# This line should be written so all the values inside of each layer is calculated as a float64\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scheduler objects to control the optimizer learning rate:\n",
    "# Note: this part should be dismissed since it is stricly written for the models.py file\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay, ExponentialDecay\n",
    "\n",
    "def TimeDecayScheduler(learning_rate=0.001, decay_steps=200, decay_rate=1.2, name=\"\"):\n",
    "    \"\"\" Returns an InverseTimeDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "def ExponentialScheduler(initial_learning_rate, decay_steps, decay_rate, name=\"\"):\n",
    "    \"\"\" Returns an ExponentialDecay object with the given properties to be used in the optimizer. \"\"\"\n",
    "    return InverseTimeDecay(\n",
    "        initial_learning_rate=initial_learning_rate, \n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "# Actual Optimizers: Adam and RMSprop are the main two optimizers that are going to be used for this project since they accept schedulers and happen to be effective.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def AdamOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "        learning_rate: the initial learning rate to be used\n",
    "        scheduler: If this is passed by the user then use it in the optimizer instead of the learning rate\n",
    "\n",
    "        # returns: an Adam optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return Adam(learning_rate)\n",
    "    else:\n",
    "        return Adam(scheduler)\n",
    "    \n",
    "\n",
    "def RMSpropOptimizer(learning_rate=0.001, scheduler=None):\n",
    "    \"\"\"\n",
    "        # params:\n",
    "            learning_rate: the initial learning rate to be used\n",
    "            scheduler: If this is passed by the user then use it in the \n",
    "            optimizer instead of the learning rate\n",
    "        \n",
    "        # returns: an RMSprop optimizer\n",
    "    \"\"\"\n",
    "    if scheduler == None:\n",
    "        return RMSprop(learning_rate)\n",
    "    else:\n",
    "        return RMSprop(scheduler)\n",
    "\n",
    "# CallBacks:\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def EarlyStopCallBack(patience=100):\n",
    "    \"\"\"\n",
    "        # params: patience of the object for the number of epochs passed with no improvement\n",
    "        # returns: a EarlyStopping callback object \n",
    "    \"\"\"\n",
    "    return EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "\n",
    "# Models: \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization  # Layers \n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2, L1L2  # Regularizer\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError # Error-metric\n",
    "import tensorflow_docs as tfdocs # For logging puposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to try out and improve the model\n",
    "1. Weight-Initializers:\n",
    "    - Use tf.keras.initializers.RandomNormal and tf.keras.initializers.RandomUniform\n",
    "    - Tweak their properties and see how they would work.\n",
    "2. Bias in Dense layers:\n",
    "    - Setup an initiallizer and regularizer for the bias of the layer\n",
    "    - Also use it those for the weights too\n",
    "    - Tweak arguments of earlt-stop call back\n",
    "3. Layers:\n",
    "    - Use LeakyRelu/TreshholdRelu/PRelu as a layer\n",
    "    - Maybe try-out tf.keras.layers.experimental.preprocessing.Normalization*\n",
    "    - Tweak BatchNormalization layer arguments\n",
    "4. Overfitting:\n",
    "    - use tf.keras.layers.GaussianDropout and tf.keras.layers.GaussianNoise ( which could be viewed as a Data augmentation method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A samll neural net to test how does the keras-tuner work\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# This line should be written so all the values inside of each layer is calculated as a float64\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "def test_model():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "         Dense(64, activation='elu', \n",
    "              kernel_regularizer=l1(0.001),\n",
    "              bias_regularizer=l2(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=25)\n",
    "        ),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Try to implement the hp in here\n",
    "    time_lr = InverseTimeDecay(\n",
    "      initial_learning_rate=0.0015,\n",
    "      decay_steps=5000,\n",
    "      decay_rate=0.0009\n",
    "    )\n",
    "    \n",
    "    optimizer = Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "        loss=MeanSquaredLogarithmicError(name='MSLE'), \n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=135, mode='min', restore_best_weights=True)\n",
    "\n",
    "m = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:233.5263,  val_loss:228.9488,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:71.1638,  val_loss:71.0619,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:53.3159,  val_loss:53.2319,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:38.2720,  val_loss:38.2062,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:26.4442,  val_loss:26.3950,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:17.5191,  val_loss:17.4849,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:10.9667,  val_loss:10.9437,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:6.3241,  val_loss:6.3092,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:3.1799,  val_loss:3.1711,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:1.2997,  val_loss:1.2968,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, loss:0.4283,  val_loss:0.4292,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, loss:0.1860,  val_loss:0.1890,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, loss:0.1379,  val_loss:0.1428,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, loss:0.1090,  val_loss:0.1127,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, loss:0.0958,  val_loss:0.0993,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, loss:0.0870,  val_loss:0.0910,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, loss:0.0816,  val_loss:0.0855,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, loss:0.0787,  val_loss:0.0826,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, loss:0.0756,  val_loss:0.0794,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, loss:0.0729,  val_loss:0.0764,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, loss:0.0706,  val_loss:0.0740,  \n",
      "....................................................................................................\n",
      "Epoch: 2100, loss:0.0684,  val_loss:0.0724,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, loss:0.0661,  val_loss:0.0695,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, loss:0.0641,  val_loss:0.0679,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, loss:0.0626,  val_loss:0.0658,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, loss:0.0607,  val_loss:0.0643,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, loss:0.0595,  val_loss:0.0632,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, loss:0.0583,  val_loss:0.0618,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, loss:0.0572,  val_loss:0.0613,  \n",
      "....................................................................................................\n",
      "Epoch: 2900, loss:0.0561,  val_loss:0.0596,  \n",
      "....................................................................................................\n",
      "MAEs:\n",
      "b011: 25.954\n",
      "b012: 26.319\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n",
      "###############################################\n",
      "Lograithmic Error:\n",
      "MSLE:\n",
      "b011: 0.03248260079337064\n",
      "b012: 0.03306046401433275\n",
      "-----------------------------------\n",
      "base-differences: 0.0020225966675850296\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(X, y, epochs=3000,\n",
    "          verbose=0, validation_split=0.33,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(m.predict(X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN02():\n",
    "    \"\"\"\n",
    "        With the lowest number of layers, generate the highest amount of bais and highly\n",
    "        regularite it.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        InputLayer(input_shape=[len(X.keys())]),\n",
    "        \n",
    "        Dense(64, activation='elu', \n",
    "              kernel_regularizer=l2(0.001),\n",
    "              bias_regularizer=l2(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=0.25)\n",
    "        ), BatchNormalization(),\n",
    "        Dense(256, activation='elu', \n",
    "              kernel_regularizer=l2(0.0001),\n",
    "              bias_regularizer=l2(0.001),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=0.25)\n",
    "        ), BatchNormalization(),\n",
    "        Dense(1028, activation='elu', \n",
    "              kernel_regularizer=l2(0.009),\n",
    "              bias_regularizer=l2(0.009),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=0.005),\n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=0.25)\n",
    "        ),\n",
    "        \n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.04, 0.004), \n",
    "              bias_regularizer=l2(0.1),\n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=2), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.05, 0.005), \n",
    "              bias_regularizer=l2(0.001), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=2), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        Dense(4, \n",
    "              kernel_regularizer=L1L2(0.6, 0.06), \n",
    "              bias_regularizer=l2(0.2), \n",
    "              bias_initializer=TruncatedNormal(mean=0, stddev=2), \n",
    "              kernel_initializer=TruncatedNormal(mean=0, stddev=5)\n",
    "        ),\n",
    "        \n",
    "        Dense(1)\n",
    "      ])\n",
    "    \n",
    "    time_lr = TimeDecayScheduler(learning_rate=0.018, decay_steps=20000, decay_rate=0.059, name=\"\")\n",
    "    \n",
    "    optimizer = Adam(time_lr)\n",
    "        \n",
    "    model.compile(\n",
    "        loss=MeanSquaredLogarithmicError(name='MSLE'), \n",
    "        optimizer=optimizer, \n",
    "    )\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = NN02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0015\n",
      "1 0.0015\n",
      "2 0.001499999250000375\n",
      "3 0.001499997750002625\n",
      "4 0.001499995500009375\n",
      "5 0.0014999925000243747\n",
      "6 0.0014999887500524998\n",
      "7 0.0014999842500997495\n",
      "8 0.0014999790001732488\n",
      "9 0.0014999730002812478\n",
      "10 0.001499966250433121\n",
      "11 0.0014999587506393677\n",
      "12 0.0014999505009116126\n",
      "13 0.001499941501262605\n",
      "14 0.001499931751706219\n",
      "15 0.001499921252257453\n",
      "16 0.0014999100029324311\n",
      "17 0.001499898003748401\n",
      "18 0.0014998852547237359\n",
      "19 0.001499871755877933\n",
      "20 0.0014998575072316144\n",
      "21 0.0014998425088065261\n",
      "22 0.0014998267606255397\n",
      "23 0.0014998102627126499\n",
      "24 0.0014997930150929763\n",
      "25 0.001499775017792763\n",
      "26 0.0014997562708393775\n",
      "27 0.001499736774261312\n",
      "28 0.0014997165280881827\n",
      "29 0.0014996955323507298\n",
      "30 0.0014996737870808172\n",
      "31 0.0014996512923114323\n",
      "32 0.0014996280480766872\n",
      "33 0.0014996040544118165\n",
      "34 0.0014995793113531791\n",
      "35 0.0014995538189382573\n",
      "36 0.0014995275772056562\n",
      "37 0.0014995005861951046\n",
      "38 0.0014994728459474546\n",
      "39 0.001499444356504681\n",
      "40 0.0014994151179098815\n",
      "41 0.0014993851302072775\n",
      "42 0.001499354393442212\n",
      "43 0.001499322907661151\n",
      "44 0.0014992906729116835\n",
      "45 0.0014992576892425202\n",
      "46 0.0014992239567034943\n",
      "47 0.001499189475345561\n",
      "48 0.0014991542452207985\n",
      "49 0.0014991182663824053\n",
      "100 0.0014962921518427749\n",
      "200 0.0014851494975362176\n",
      "300 0.0014667384986449122\n",
      "400 0.0014413321131140347\n",
      "500 0.0014093030623604254\n",
      "600 0.0013711147351468177\n",
      "700 0.0013273100384357558\n",
      "800 0.001278498569939834\n",
      "900 0.001225342535448726\n",
      "1000 0.0011685418644897818\n",
      "1100 0.0011088189896572534\n",
      "1200 0.001046903748153406\n",
      "1300 0.0009835188397486708\n",
      "1400 0.0009193662353221208\n",
      "1500 0.0008551148768977526\n",
      "1600 0.0007913899466645731\n",
      "1700 0.0007287639121997867\n",
      "1800 0.0006677494814710452\n",
      "1900 0.000608794527578088\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0015\n",
    "\n",
    "for i in range(2000):\n",
    "    if i < 50 or i % 100 == 0:\n",
    "        print(i, lr)\n",
    "    lr = lr / (1 + 0.0005 * i / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=135, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, loss:0.6540,  val_loss:0.6901,  \n",
      "....................................................................................................\n",
      "Epoch: 100, loss:0.6180,  val_loss:6.0551,  \n",
      "....................................................................................................\n",
      "Epoch: 200, loss:0.6059,  val_loss:0.6355,  \n",
      "....................................................................................................\n",
      "Epoch: 300, loss:0.6090,  val_loss:0.6387,  \n",
      "....................................................................................................\n",
      "Epoch: 400, loss:0.5975,  val_loss:0.6749,  \n",
      "....................................................................................................\n",
      "Epoch: 500, loss:0.5861,  val_loss:0.6163,  \n",
      "....................................................................................................\n",
      "Epoch: 600, loss:0.5862,  val_loss:0.5794,  \n",
      "....................................................................................................\n",
      "Epoch: 700, loss:0.5680,  val_loss:0.6219,  \n",
      "....................................................................................................\n",
      "Epoch: 800, loss:0.5637,  val_loss:0.6050,  \n",
      "....................................................................................................\n",
      "Epoch: 900, loss:0.5608,  val_loss:0.5926,  \n",
      "..........\n",
      "MAEs:\n",
      "b011: 21.417\n",
      "b012: 21.664\n",
      "-----------------------------------\n",
      "base-differences: 5.76\n",
      "###############################################\n",
      "Lograithmic Error:\n",
      "MSLE:\n",
      "b011: 0.020908094657777574\n",
      "b012: 0.021136750479458913\n",
      "-----------------------------------\n",
      "base-differences: 0.0020225966675850296\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(norm_X, y, epochs=EPOCHS,\n",
    "          verbose=0, validation_split=0.33,\n",
    "          callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "print()\n",
    "validate(quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different hyperparameters\n",
    "- Run number of different fitting and see their results.\n",
    "- Normalized X works way betetr!! The training time is way faster and the loss and val_loss decrease close to each other.\n",
    "- Using BatchNormalizations before each layer would be very helpful.\n",
    "\n",
    "Note: A lower loss value would not necessary mean that the model's prediction would improve\n",
    "##### Important Note: when we increase the amount of regularazation, the loss will be increase naturally. Hence, if one were to compare different model epochs, this should be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                      'SalePrice': quantize(pd.DataFrame(model.predict(norm_X_test, verbose=0))[0])})\n",
    "output.to_csv('submissions/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
